@article{Hebart:2020uz,
    author = {Hebart, Martin N and Zheng, Charles Y and Pereira, Francisco and Baker, Chris I},
    title = {{Revealing the multidimensional mental representations of natural objects underlying human similarity judgements}},
    journal = {Nature Human Behaviour},
    year = {2020},
    volume = {4},
    number = {11},
    pages = {1173--1185}
}
@article{nguyen_dataset_2021,
	title = {Dataset {Meta}-{Learning} from {Kernel} {Ridge}-{Regression}},
	url = {http://arxiv.org/abs/2011.00050},
	abstract = {One of the most fundamental aspects of any machine learning algorithm is the training data used by the algorithm. We introduce the novel concept of \${\textbackslash}epsilon\$-approximation of datasets, obtaining datasets which are much smaller than or are significant corruptions of the original training data while maintaining similar model performance. We introduce a meta-learning algorithm called Kernel Inducing Points (KIP) for obtaining such remarkable datasets, inspired by the recent developments in the correspondence between infinitely-wide neural networks and kernel ridge-regression (KRR). For KRR tasks, we demonstrate that KIP can compress datasets by one or two orders of magnitude, significantly improving previous dataset distillation and subset selection methods while obtaining state of the art results for MNIST and CIFAR-10 classification. Furthermore, our KIP-learned datasets are transferable to the training of finite-width neural networks even beyond the lazy-training regime, which leads to state of the art results for neural network dataset distillation with potential applications to privacy-preservation.},
	urldate = {2021-12-25},
	journal = {arXiv:2011.00050 [cs, stat]},
	author = {Nguyen, Timothy and Chen, Zhourong and Lee, Jaehoon},
	month = mar,
	year = {2021},
	note = {arXiv: 2011.00050},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Accepted to ICLR 2021. Open source implementation: https://colab.sandbox.google.com/github/google-research/google-research/blob/master/kip/KIP.ipynb},
	file = {arXiv Fulltext PDF:/Users/derekfeng/Zotero/storage/MWR7PSEE/Nguyen et al. - 2021 - Dataset Meta-Learning from Kernel Ridge-Regression.pdf:application/pdf;arXiv.org Snapshot:/Users/derekfeng/Zotero/storage/TRBQJEG9/2011.html:text/html},
}
@inproceedings{Zhang:2016ve,
author = {Zhang, Chiyuan and Recht, Benjamin and Bengio, Samy and Hardt, Moritz and Vinyals, Oriol},
title = {{Understanding deep learning requires rethinking generalization}},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
year = {2019},
organization = {University of California, Berkeley, Berkeley, United States},
month = jan,
annote = {Published in ICLR 2017}
}
@article{Bartlett:2020ck,
author = {Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
title = {{Benign overfitting in linear regression.}},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
year = {2020},
volume = {vol. 80},
pages = {201907378},
month = apr
}
@article{BakColeman:2021wt,
author = {Bak-Coleman, Joseph B and Alfano, Mark and Barfuss, Wolfram and Bergstrom, Carl T and Centeno, Miguel A and Couzin, Iain D and Donges, Jonathan F and Galesic, Mirta and Gersick, Andrew S and Jacquet, Jennifer and Kao, Albert B and Moran, Rachel E and Romanczuk, Pawel and Rubenstein, Daniel I and Tombak, Kaia J and Van Bavel, Jay J and Weber, Elke U},
title = {{Stewardship of global collective behavior}},
journal = {Proc Natl Acad Sci USA},
year = {2021},
volume = {118},
number = {27},
pages = {e2025764118},
month = jul
}
@inproceedings{Arora:2018vn,
author = {Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
title = {{On the optimization of deep networks: Implicit acceleration by overparameterization}},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
year = {2018},
pages = {372--389},
organization = {Institute for Advanced Studies, Princeton, United States},
month = jan,
annote = {Published at the International Conference on Machine Learning (ICML) 2018}
}
@article{Burke:2010ij,
author = {Burke, Brian L and Martens, Andy and Faucher, Erik H},
title = {{Two decades of terror management theory: a meta-analysis of mortality salience research.}},
journal = {Personality and social psychology review : an official journal of the Society for Personality and Social Psychology, Inc},
year = {2010},
volume = {14},
number = {2},
pages = {155--195},
month = may
}
@article{Battaglia:2018vi,
author = {Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
title = {{Relational inductive biases, deep learning, and graph networks}},
journal = {arXiv.org},
year = {2018},
eprint = {1806.01261v3},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = jun
}
@article{Li:2019tn,
author = {Li, Zhiyuan and Arora, Sanjeev},
title = {{An Exponential Learning Rate Schedule for Deep Learning}},
journal = {arXiv.org},
year = {2019},
eprint = {1910.07454v3},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = oct
}
@article{Candes:2009kj,
author = {Candes, Emmanuel J and Recht, Benjamin},
title = {{Exact matrix completion via convex optimization}},
journal = {Foundations of Computational Mathematics},
year = {2009},
volume = {9},
number = {6},
pages = {717--772},
month = dec
}
@article{Breiman:2001wl,
author = {Leo Breiman},
title = {{Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)}},
volume = {16},
journal = {Statistical Science},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {199 -- 231},
year = {2001},
doi = {10.1214/ss/1009213726},
URL = {https://doi.org/10.1214/ss/1009213726}
}
@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}
@article{Razin:2020up,
author = {Razin, Noam and Cohen, Nadav},
title = {{Implicit Regularization in Deep Learning May Not Be Explainable by Norms}},
journal = {arXiv.org},
year = {2020},
eprint = {2005.06398v1},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = may
}
@article{Arora:2019ug,
author = {Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
title = {{Implicit Regularization in Deep Matrix Factorization}},
journal = {arXiv.org},
year = {2019},
eprint = {1905.13655v3},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = may,
annote = {Published at the conference on Neural Information Processing Systems (NeurIPS) 2019}
}
@inproceedings{Kingma:2015us,
author = {Kingma, Diederik P and Ba, Jimmy Lei},
title = {{Adam: A method for stochastic optimization}},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
year = {2015},
organization = {University of Toronto, Toronto, Canada},
month = jan
}
@misc{dosovitskiy2021image,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{jaegle2021perceiver,
      title={Perceiver: General Perception with Iterative Attention}, 
      author={Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira},
      year={2021},
      eprint={2103.03206},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{sitzmann2020implicit,
  title={Implicit neural representations with periodic activation functions},
  author={Sitzmann, Vincent and Martel, Julien and Bergman, Alexander and Lindell, David and Wetzstein, Gordon},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7462--7473},
  year={2020}
}
@misc{chen2021did,
      title={Did the Model Change? Efficiently Assessing Machine Learning API Shifts}, 
      author={Lingjiao Chen and Tracy Cai and Matei Zaharia and James Zou},
      year={2021},
      eprint={2107.14203},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@article{bardes2021vicreg,
  title={Vicreg: Variance-invariance-covariance regularization for self-supervised learning},
  author={Bardes, Adrien and Ponce, Jean and LeCun, Yann},
  journal={arXiv preprint arXiv:2105.04906},
  year={2021}
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10012--10022},
  year={2021}
}
@article{schwarzschild2021can,
  title={Can You Learn an Algorithm? Generalizing from Easy to Hard Problems with Recurrent Networks},
  author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{Bubeck:2021vv,
author = {Bubeck, S{\'e}bastien and Sellke, Mark},
title = {{A Universal Law of Robustness via Isoperimetry}},
journal = {arXiv.org},
year = {2021},
eprint = {2105.12806v1},
eprinttype = {arxiv},
eprintclass = {cs.LG},
month = may
}
@article{Conneau:2017wg,
author = {Conneau, Alexis and Lample, Guillaume and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
title = {{Word Translation Without Parallel Data}},
journal = {arXiv.org},
year = {2017},
eprint = {1710.04087v3},
eprinttype = {arxiv},
eprintclass = {cs.CL},
month = oct,
annote = {ICLR 2018}
}
@article{Artetxe:2017ta,
author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko and Cho, Kyunghyun},
title = {{Unsupervised Neural Machine Translation}},
journal = {arXiv.org},
year = {2017},
eprint = {1710.11041v2},
eprinttype = {arxiv},
eprintclass = {cs.CL},
month = oct,
annote = {Published as a conference paper at ICLR 2018}
}
@inproceedings{sennrich-etal-2016-improving,
author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
title = {{Improving Neural Machine Translation Models with Monolingual Data}},
booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
year = {2016},
pages = {86--96},
publisher = {Association for Computational Linguistics},
address = {Berlin, Germany},
month = aug
}
@article{Mikolov:2013tp,
author = {Mikolov, Tomas and Le, Quoc V and Sutskever, Ilya},
title = {{Exploiting Similarities among Languages for Machine Translation}},
journal = {arXiv.org},
year = {2013},
eprint = {1309.4168v1},
eprinttype = {arxiv},
eprintclass = {cs.CL},
month = sep
}
@inproceedings{artetxe-etal-2017-learning,
author = {Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
title = {{Learning bilingual word embeddings with (almost) no bilingual data}},
booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
year = {2017},
pages = {451--462},
publisher = {Association for Computational Linguistics},
address = {Vancouver, Canada},
month = jul
}
@article{Roads:2020ty,
author = {Roads, Brett D and Love, Bradley C},
title = {{Learning as the unsupervised alignment of conceptual systems}},
journal = {Nature Machine Intelligence},
year = {2020},
volume = {2},
number = {1},
pages = {76--82}
}
@article{Tommasi:2015tn,
author = {Tommasi, Tatiana and Patricia, Novi and Caputo, Barbara and Tuytelaars, Tinne},
title = {{A Deeper Look at Dataset Bias}},
journal = {arXiv.org},
year = {2015},
eprint = {1505.01257v1},
eprinttype = {arxiv},
eprintclass = {cs.CV},
month = may
}
@article{Chang:2018uw,
author = {Chang, Jinyuan and Kolaczyk, Eric D and Yao, Qiwei},
title = {{Estimation of subgraph density in noisy networks}},
journal = {arXiv.org},
year = {2018},
eprint = {1803.02488v3},
eprinttype = {arxiv},
eprintclass = {stat.ME},
month = mar
}
@article{Li:2020uy,
author = {Li, Wenrui and Sussman, Daniel L and Kolaczyk, Eric D},
title = {{Estimation of the Epidemic Branching Factor in Noisy Contact Networks}},
year = {2020},
eprint = {2002.05763},
eprinttype = {arxiv},
month = feb
}
@inproceedings{desai2021virtex,
  title={Virtex: Learning visual representations from textual annotations},
  author={Desai, Karan and Johnson, Justin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11162--11173},
  year={2021}
}
@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
}

@InProceedings{pmlr-v48-hardt16,
  title = 	 {Train faster, generalize better: Stability of stochastic gradient descent},
  author = 	 {Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1225--1234},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/hardt16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/hardt16.html},
  abstract = 	 {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions. Applying our results to the convex case, we provide new insights for why multiple epochs of stochastic gradient methods generalize well in practice. In the non-convex case, we give a new interpretation of common practices in neural networks, and formally show that popular techniques for training large deep models are indeed stability-promoting. Our findings conceptually underscore the importance of reducing training time beyond its obvious benefit.}
}
