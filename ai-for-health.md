---
tags:
 - xAI
 - medicine
 - artificial_intelligence
---

# AI for Health

src: [WHO](https://www.who.int/publications/i/item/9789240029200) guidance

## Summary

Six key (ethical) principles:

1. *Human autonomy*: think of the humans! (also right to privacy)
2. *Safety/well-being/public interest*: this is like the Asimov robot laws
3. *Transparency/explainability/intelligibility*: self-explanatory
4. *Responsibility/accountability*: I think this follows naturally from the previous point, but means there's "points of human supervision" (human-in-the-loop?)
5. *Inclusiveness/equity*: fairness in a nutshell
6. *Responsive/sustainable*: on-line, adaptive learning + sustainability w.r.t. the environment

To someone versed in the societal import of ML, I don't think there's too much in the way of surprises in this guidance document, though it does highlight a few things (worth repeating):

 - the differentiation between high and low-income countries, and the potentially widening gap in healthcare outcomes brought about by AI. while there's nothing inherently problematic about that, it does bring up the potential problem of a mismatch in the focus of problems (i.e. cardiovascular diseases and other lifestyle-based, chronic illnesses for high-income versus the more straightforward, brutal problems faced by low-income).
 - biased learning from data collected in the west is a key problem: we know very well that racial groups often have very different health outcomes for the same treatment
 - healthcare in other parts of the world are oftentimes much more holistic (i.e. Chinese Medicine?). how do we reconcile such traditions?
 - AI requires big data, which runs counter to fundamental privacy rights. this is where privacy-preserving measures will be key. on the other hand, the acquisition of such data in less scrupulous countries might be disastrous.