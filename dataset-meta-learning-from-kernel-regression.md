---
tags:
  - paper
  - machine_learning
  - neural_tangent_kernel
---

# Dataset Meta-Learning From Kernel Ridge-Regression

 - src: [@nguyen_dataset_2021]

You have [[knowledge-distillation]], the goal of which is to compress models down while retaining the same performance. Less common is what can only be described as [[data-distillation]], the idea (?) of which is to compress datasets down in such a way that a particular algorithm trained on this compressed dataset would reach comparable test performance. ^[I feel like there's something interesting here for the psychologists. They're always looking to find the *archetypal* image for a class.]