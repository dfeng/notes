{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/vision-transformers/",
    "result": {"data":{"mdx":{"id":"a9e97a08-fc17-502b-a82b-f1aaefbf66c2","tableOfContents":{"items":[{"url":"#vision-transformers","title":"Vision Transformers","items":[{"url":"#an-image-is-worth-16x16-words","title":"An Image Is Worth 16x16 Words"}]}]},"fields":{"title":"Vision Transformers","slug":"/vision-transformers/","url":"https://deepmind.vercel.app/vision-transformers/","editUrl":"https://github.com/dfeng/notes/tree/main/vision-transformers.md","lastUpdatedAt":"2022-01-29T14:49:40.000Z","lastUpdated":"1/29/2022","gitCreatedAt":"2022-01-16T10:24:03.000Z","shouldShowTitle":false},"frontmatter":{"title":"","description":null,"imageAlt":null,"tags":["from_paper","machine_learning","computer_vision","attention"],"date":null,"dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"from_paper\", \"machine_learning\", \"computer_vision\", \"attention\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"vision-transformers\"\n  }, \"Vision Transformers\"), mdx(\"p\", null, \"Thanks to their computational efficiency (and hence scalability), self-attention based (\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/transformers/\",\n    \"title\": \"Transformers\"\n  }, \"transformers\"), \") architectures have become the model of choice in NLP. Their success has led people to wonder if such architectures are universal\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-1\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-1\",\n    \"className\": \"footnote-ref\"\n  }, \"1\")), \", or, at the very least, if they can be applied to computer vision tasks.\"), mdx(\"p\", null, \"Self-attention gives a good tradeoff between local context awareness and computational efficiency: rnn/lstm's allow for long-range dependencies, but the sequential nature of training really hampers model size.\"), mdx(\"p\", null, \"Compare this convolution layers (which are highly specialized for \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/computer-vision-tasks/\",\n    \"title\": \"Computer Vision Tasks\"\n  }, \"computer-vision-tasks\"), \")\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/stand-alone-self-attention-in-vision-models/\",\n    \"title\": \"Stand-Alone Self-Attention in Vision Models\"\n  }, \"stand-alone-self-attention-in-vision-models\"), \"\"), mdx(\"h2\", {\n    \"id\": \"an-image-is-worth-16x16-words\"\n  }, \"An Image Is Worth 16x16 Words\"), mdx(\"p\", null, \"src: \", \"(Dosovitskiy et al., 2021)\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-dosovitskiy2021image\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-dosovitskiy2021image\",\n    \"className\": \"footnote-ref\"\n  }, \"dosovitskiy2021image\"))), mdx(\"h3\", {\n    \"id\": \"summary\"\n  }, \"Summary\"), mdx(\"div\", {\n    \"className\": \"footnotes\"\n  }, mdx(\"hr\", {\n    parentName: \"div\"\n  }), mdx(\"ol\", {\n    parentName: \"div\"\n  }, mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-1\"\n  }, \"For instance, it's been speculated that the self-attention mechanism is comparable to the way attention manifests in our brains \\u2013 such a thesis would imply the universality of this module.\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-1\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")), mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-dosovitskiy2021image\"\n  }, \"Dosovitskiy, A. et al., 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-dosovitskiy2021image\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")))));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\ntags:\n - from_paper\n - machine_learning\n - computer_vision\n - attention\n---\n\n# Vision Transformers\n\nThanks to their computational efficiency (and hence scalability), self-attention based ([[transformers]]) architectures have become the model of choice in NLP. Their success has led people to wonder if such architectures are universal^[For instance, it's been speculated that the self-attention mechanism is comparable to the way attention manifests in our brains – such a thesis would imply the universality of this module.], or, at the very least, if they can be applied to computer vision tasks.\n\nSelf-attention gives a good tradeoff between local context awareness and computational efficiency: rnn/lstm's allow for long-range dependencies, but the sequential nature of training really hampers model size.\n\nCompare this convolution layers (which are highly specialized for [[computer-vision-tasks]])\n\n[[stand-alone-self-attention-in-vision-models]]\n\n\n## An Image Is Worth 16x16 Words\n\nsrc: [@dosovitskiy2021image]\n\n### Summary\n\n\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[transformers]: transformers.md \"Transformers\"\n[computer-vision-tasks]: computer-vision-tasks.md \"Computer Vision Tasks\"\n[stand-alone-self-attention-in-vision-models]: stand-alone-self-attention-in-vision-models.md \"Stand-Alone Self-Attention in Vision Models\"\n[//end]: # \"Autogenerated link references\"","excerpt":"Vision Transformers Thanks to their computational efficiency (and hence scalability), self-attention based ([ transformers ]) architectures…","outboundReferences":[{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"computer_vision\", \"biologically_inspired\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"computer-vision-tasks\"\n  }, \"Computer Vision Tasks\"), mdx(\"p\", null, \"Question: what are the key components of a computer vision task?\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-1\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-1\",\n    \"className\": \"footnote-ref\"\n  }, \"1\"))), mdx(\"p\", null, \"This question is inextricably linked to how our vision system works (evolved).\"), mdx(\"div\", {\n    \"className\": \"footnotes\"\n  }, mdx(\"hr\", {\n    parentName: \"div\"\n  }), mdx(\"ol\", {\n    parentName: \"div\"\n  }, mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-1\"\n  }, \"From thinking about convolutional layers and what makes them \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"specialized\"), \".\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-1\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")))));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/computer-vision-tasks/","title":"Computer Vision Tasks","shouldShowTitle":false}},{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"from_paper\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"stand-alone-self-attention-in-vision-models\"\n  }, \"Stand-Alone Self-Attention in Vision Models\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1906.05909\"\n  }, \"arXiv\"))));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/stand-alone-self-attention-in-vision-models/","title":"Stand-Alone Self-Attention in Vision Models","shouldShowTitle":false}},{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"machine_learning\", \"neural_networks\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"transformers\"\n  }, \"Transformers\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"references:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\"\n  }, \"lil-log\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://distill.pub/2016/augmented-rnns/\"\n  }, \"distill\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://d2l.ai/chapter_attention-mechanisms/transformer.html\"\n  }, \"ebook-chapter\"), \" on transformers (actually this ebook isn't great, as it ends up being more about the implementation)\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"it turns out that #attention is an earlier concept, which itself is motivated by the encoder-decoder sequence-to-sequence architecture\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"img\", {\n    parentName: \"li\",\n    \"src\": \"https://karpathy.github.io/assets/rnn/diags.jpeg\",\n    \"alt\": null\n  })), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"(oops) I didn't really understand this diagram:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"sequence to sequence models are essentially the third diagram, where basically the input sequence and output sequence are asynchronous\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"versus the normal rnn, which takes the output as the input\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"I really like the example of 1-to-many, image-to-caption, so your input is not a sequence, but your output is\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"seq2seq example is the translation program, where importantly the input and output sequence don't have to be the same length (due to the way languages differ in their realisations of the same meaning)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"the encoder is the rnn on the input sequence, and this culminates into the last hidden layer\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"this is essentially the \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"context\"), \" vector: the idea here is that this (fixed-length) vector captures all the information about the sentence\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"key: this acts like a sort of \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"informational bottleneck\"), \", and actually is the impetus for the attention mechanic\"))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"key point (via this \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/pdf/1703.01619.pdf\"\n  }, \"tutorial\"), \"):\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"instead of having everything represented as the last hidden layer (fixed-length), why not just look at all the hidden layers (vectors representing each word)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"but, that would be \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"variable\"), \" length, so instead just look at a linear combination of those hidden layers. this linear combination is learned, and is basically \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"attention\")))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Transformers #todo\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"prior art\\\"\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"CNN: easy to parallelise, but aren't recurrent (can't capture sequential dependency)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"RNN: reverse\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"goal of transformers/attention is achieve \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"parallelization\"), \" and \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"recurrence\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"by appealing to \\\"attention\\\" to get the recurrence (?)\")))))), mdx(\"h2\", {\n    \"id\": \"transformers-1\"\n  }, \"Transformers\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"key is \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"multi-head self-attention\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"encoded representation of input: key-value pairs (\", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"K\"), mdx(\"mo\", {\n    parentName: \"mrow\",\n    \"separator\": \"true\"\n  }, \",\"), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"V\"), mdx(\"mo\", {\n    parentName: \"mrow\"\n  }, \"\\u2208\"), mdx(\"msup\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\",\n    \"mathvariant\": \"double-struck\"\n  }, \"R\"), mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"n\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"K,V \\\\in \\\\mathbb{R}^{n}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.8777699999999999em\",\n      \"verticalAlign\": \"-0.19444em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07153em\"\n    }\n  }, \"K\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mpunct\"\n  }, \",\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.16666666666666666em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.22222em\"\n    }\n  }, \"V\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mrel\"\n  }, \"\\u2208\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68889em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathbb\"\n  }, \"R\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.664392em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.063em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"n\"))))))))))))), \")\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"corresponding to hidden states\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"previous output is compressed into query \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"Q\"), mdx(\"mo\", {\n    parentName: \"mrow\"\n  }, \"\\u2208\"), mdx(\"msup\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\",\n    \"mathvariant\": \"double-struck\"\n  }, \"R\"), mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"m\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"Q \\\\in \\\\mathbb{R}^{m}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.8777699999999999em\",\n      \"verticalAlign\": \"-0.19444em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"Q\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mrel\"\n  }, \"\\u2208\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68889em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathbb\"\n  }, \"R\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.664392em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.063em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"m\"))))))))))))), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"output of the transformer is a weighted sum of the values (\", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"V\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"V\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.22222em\"\n    }\n  }, \"V\"))))), \").\")))), mdx(\"h2\", {\n    \"id\": \"todo\"\n  }, \"Todo\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Visualising and Measuring the Geometry of BERT \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/pdf/1906.02715.pdf\"\n  }, \"arXiv\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"http://www.peterbloem.nl/blog/transformers\"\n  }, \"random blog\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"pretty intuitive description of transformers on \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://nostalgebraist.tumblr.com/post/185326092369/the-transformer-explained\"\n  }, \"tumblr\"), \", via the LessWrong community\")));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/transformers/","title":"Transformers","shouldShowTitle":false}}],"inboundReferences":[]},"tagsOutbound":{"nodes":[{"frontmatter":{"title":"","tags":["from_paper","lit_review","interpolation"]},"fields":{"slug":"/benign-overfitting-in-linear-regression/","title":"Benign Overfitting in Linear Regression","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/can-you-learn-an-algorithm/","title":"Can You Learn an Algorithm","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2021-12-23T11:27:19.000Z"}},{"frontmatter":{"title":"","tags":["interpolation","statistics","machine_learning"]},"fields":{"slug":"/classification-vs-regression/","title":"Classification vs Regression","lastUpdated":"2/12/2022","lastUpdatedAt":"2022-02-12T11:03:50.000Z","gitCreatedAt":"2022-02-12T11:03:50.000Z"}},{"frontmatter":{"title":"","tags":["computer_vision","biologically_inspired"]},"fields":{"slug":"/computer-vision-tasks/","title":"Computer Vision Tasks","lastUpdated":"1/22/2022","lastUpdatedAt":"2022-01-22T10:54:29.000Z","gitCreatedAt":"2022-01-22T10:54:29.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","machine_learning","neural_tangent_kernel"]},"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Distillation","lastUpdated":"2/26/2022","lastUpdatedAt":"2022-02-26T13:00:00.000Z","gitCreatedAt":"2021-12-25T21:42:34.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/embeddings-cant-possibly-be-right/","title":"Embeddings Can't Possibly Be Right","lastUpdated":"2/13/2022","lastUpdatedAt":"2022-02-13T17:46:16.000Z","gitCreatedAt":"2022-02-13T10:27:23.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","lit_review","gradient_descent","optimisation","regularization"]},"fields":{"slug":"/exponential-learning-rates/","title":"Exponential Learning Rates","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}},{"frontmatter":{"title":"","tags":["neural_tangent_kernel","machine_learning"]},"fields":{"slug":"/from-nerf-to-kernel-regression/","title":"From NERF to Kernel Regression","lastUpdated":"2/27/2022","lastUpdatedAt":"2022-02-27T08:49:29.000Z","gitCreatedAt":"2022-02-27T08:49:29.000Z"}},{"frontmatter":{"title":"","tags":["neural_networks","machine_learning"]},"fields":{"slug":"/generalised-neural-networks/","title":"Generalized Neural Networks","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T13:24:59.000Z","gitCreatedAt":"2022-01-22T10:54:29.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning"]},"fields":{"slug":"/invariant-risk-minimisation/","title":"Invariant Risk Minimisation","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning"]},"fields":{"slug":"/knowledge-distillation/","title":"Knowledge Distillation","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T12:51:34.000Z","gitCreatedAt":"2021-12-23T12:17:23.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","causal_inference"]},"fields":{"slug":"/learning-dags/","title":"Learning DAGs","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning","meta_learning"]},"fields":{"slug":"/meta-learning/","title":"Meta Learning","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T21:27:53.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","psychology"]},"fields":{"slug":"/multidimensional-mental-representations-of-natural-objects/","title":"Multidimensional Mental Representations of Natural Objects","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2021-12-24T11:18:15.000Z"}},{"frontmatter":{"title":"","tags":["neural_networks","from_paper"]},"fields":{"slug":"/neural-representations/","title":"Neural Representations","lastUpdated":"2/18/2022","lastUpdatedAt":"2022-02-18T14:45:19.000Z","gitCreatedAt":"2022-02-11T17:01:17.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning"]},"fields":{"slug":"/no-free-lunch/","title":"No Free Lunch","lastUpdated":"2/23/2022","lastUpdatedAt":"2022-02-23T16:00:05.000Z","gitCreatedAt":"2022-02-23T13:27:28.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/non-deep-networks/","title":"Non-deep Networks","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2021-12-23T11:27:19.000Z"}},{"frontmatter":{"title":"","tags":["gui","machine_learning"]},"fields":{"slug":"/objection-detection-for-guis/","title":"Object Detection for GUI","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T12:51:34.000Z","gitCreatedAt":"2021-12-24T10:27:07.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","overparameterization"]},"fields":{"slug":"/overparameterized-regression/","title":"Overparameterized Regression","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/stand-alone-self-attention-in-vision-models/","title":"Stand-Alone Self-Attention in Vision Models","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2021-12-23T11:27:19.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","society","research"]},"fields":{"slug":"/stewardship-of-global-collective-beahvior/","title":"Stewardship of Global Collective Behaviour","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["artificial_intelligence","machine_learning","psychology"]},"fields":{"slug":"/system-1-and-2/","title":"System 1 and 2","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning","neural_networks"]},"fields":{"slug":"/transformers/","title":"Transformers","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T21:27:53.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","statistics","research","classic_papers"]},"fields":{"slug":"/two-cultures/","title":"Two Cultures","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","machine_learning","computer_vision","attention"]},"fields":{"slug":"/vision-transformers/","title":"Vision Transformers","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-16T10:24:03.000Z"}}]}},"pageContext":{"tags":["from_paper","machine_learning","computer_vision","attention"],"slug":"/vision-transformers/","sidebarItems":[{"title":"","items":[{"title":"Recently Updated","url":"/latest/","collapse":true,"indent":false,"items":[{"title":"02-27: From NERF to Kernel Regression","url":"/from-nerf-to-kernel-regression/"},{"title":"02-26: Ideas Around Interpolation","url":"/ideas-around-interpolation/"},{"title":"02-26: Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"02-23: No Free Lunch","url":"/no-free-lunch/"},{"title":"02-21: Slack Bot","url":"/slack-bot/"},{"title":"02-18: Neural Representations","url":"/neural-representations/"},{"title":"02-13: Embeddings Can't Possibly Be Right","url":"/embeddings-cant-possibly-be-right/"},{"title":"02-13: Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"02-12: Classification vs Regression","url":"/classification-vs-regression/"},{"title":"02-04: Fast Weights","url":"/fast-weights/"}]}]},{"title":"Tags","items":[{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"Artificial Generalised Intelligence","url":"/artificial-generalised-intelligence/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"attention","type":"tag","url":"/tags/attention/","items":[{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"biologically_inspired","type":"tag","url":"/tags/biologically-inspired/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"},{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"causal_inference","type":"tag","url":"/tags/causal-inference/","items":[{"title":"Learning DAGs","url":"/learning-dags/"}]},{"title":"classic_papers","type":"tag","url":"/tags/classic-papers/","items":[{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"commentary","type":"tag","url":"/tags/commentary/","items":[{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"computer_vision","type":"tag","url":"/tags/computer-vision/","items":[{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"deep_learning","type":"tag","url":"/tags/deep-learning/","items":[{"title":"GPT-3","url":"/gpt3/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Bitter Lesson","url":"/bitter-lesson/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Next Steps for Deep Learning","url":"/next-steps-for-deep-learning/"}]},{"title":"from_paper","type":"tag","url":"/tags/from-paper/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Embeddings Can't Possibly Be Right","url":"/embeddings-cant-possibly-be-right/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Learning DAGs","url":"/learning-dags/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Neural Representations","url":"/neural-representations/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"Slack Bot","url":"/slack-bot/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"Ideas Around Interpolation","url":"/ideas-around-interpolation/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"implicit_regularisation","type":"tag","url":"/tags/implicit-regularisation/","items":[{"title":"Implicit-Regularisation","url":"/implicit-regularisation/"}]},{"title":"interpolation","type":"tag","url":"/tags/interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Ideas Around Interpolation","url":"/ideas-around-interpolation/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"From NERF to Kernel Regression","url":"/from-nerf-to-kernel-regression/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Invariant Risk Minimisation","url":"/invariant-risk-minimisation/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"No Free Lunch","url":"/no-free-lunch/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"System 1 and 2","url":"/system-1-and-2/"},{"title":"Transformers","url":"/transformers/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"matrix_completion","type":"tag","url":"/tags/matrix-completion/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Fast Weights","url":"/fast-weights/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Neural Representations","url":"/neural-representations/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"From NERF to Kernel Regression","url":"/from-nerf-to-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"neuroscience","type":"tag","url":"/tags/neuroscience/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"}]},{"title":"nlp","type":"tag","url":"/tags/nlp/","items":[{"title":"Debiasing Word Embeddings","url":"/debiasing-word-embeddings/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"GPT-3","url":"/gpt3/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"optimisation","type":"tag","url":"/tags/optimisation/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"regularization","type":"tag","url":"/tags/regularization/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Alone","url":"/alone/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"theoretical_statistics","type":"tag","url":"/tags/theoretical-statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]}]}],"tagsGroups":[{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"Artificial Generalised Intelligence","url":"/artificial-generalised-intelligence/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"attention","type":"tag","url":"/tags/attention/","items":[{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"biologically_inspired","type":"tag","url":"/tags/biologically-inspired/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"},{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"causal_inference","type":"tag","url":"/tags/causal-inference/","items":[{"title":"Learning DAGs","url":"/learning-dags/"}]},{"title":"classic_papers","type":"tag","url":"/tags/classic-papers/","items":[{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"commentary","type":"tag","url":"/tags/commentary/","items":[{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"computer_vision","type":"tag","url":"/tags/computer-vision/","items":[{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"deep_learning","type":"tag","url":"/tags/deep-learning/","items":[{"title":"GPT-3","url":"/gpt3/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Bitter Lesson","url":"/bitter-lesson/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Next Steps for Deep Learning","url":"/next-steps-for-deep-learning/"}]},{"title":"from_paper","type":"tag","url":"/tags/from-paper/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Embeddings Can't Possibly Be Right","url":"/embeddings-cant-possibly-be-right/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Learning DAGs","url":"/learning-dags/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Neural Representations","url":"/neural-representations/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"Slack Bot","url":"/slack-bot/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"Ideas Around Interpolation","url":"/ideas-around-interpolation/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"implicit_regularisation","type":"tag","url":"/tags/implicit-regularisation/","items":[{"title":"Implicit-Regularisation","url":"/implicit-regularisation/"}]},{"title":"interpolation","type":"tag","url":"/tags/interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Ideas Around Interpolation","url":"/ideas-around-interpolation/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"From NERF to Kernel Regression","url":"/from-nerf-to-kernel-regression/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Invariant Risk Minimisation","url":"/invariant-risk-minimisation/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"No Free Lunch","url":"/no-free-lunch/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"System 1 and 2","url":"/system-1-and-2/"},{"title":"Transformers","url":"/transformers/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"matrix_completion","type":"tag","url":"/tags/matrix-completion/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Fast Weights","url":"/fast-weights/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Neural Representations","url":"/neural-representations/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"From NERF to Kernel Regression","url":"/from-nerf-to-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"neuroscience","type":"tag","url":"/tags/neuroscience/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"}]},{"title":"nlp","type":"tag","url":"/tags/nlp/","items":[{"title":"Debiasing Word Embeddings","url":"/debiasing-word-embeddings/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"GPT-3","url":"/gpt3/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"optimisation","type":"tag","url":"/tags/optimisation/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"regularization","type":"tag","url":"/tags/regularization/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Alone","url":"/alone/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"theoretical_statistics","type":"tag","url":"/tags/theoretical-statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]}],"latestPosts":[{"fields":{"slug":"/from-nerf-to-kernel-regression/","title":"From NERF to Kernel Regression","lastUpdatedAt":"2022-02-27T08:49:29.000Z","lastUpdated":"2/27/2022"},"frontmatter":{"draft":false,"tags":["neural_tangent_kernel","machine_learning"]}},{"fields":{"slug":"/ideas-around-interpolation/","title":"Ideas Around Interpolation","lastUpdatedAt":"2022-02-26T16:35:48.000Z","lastUpdated":"2/26/2022"},"frontmatter":{"draft":false,"tags":["interpolation","idea"]}},{"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Distillation","lastUpdatedAt":"2022-02-26T13:00:00.000Z","lastUpdated":"2/26/2022"},"frontmatter":{"draft":false,"tags":["from_paper","machine_learning","neural_tangent_kernel"]}},{"fields":{"slug":"/no-free-lunch/","title":"No Free Lunch","lastUpdatedAt":"2022-02-23T16:00:05.000Z","lastUpdated":"2/23/2022"},"frontmatter":{"draft":false,"tags":["machine_learning"]}},{"fields":{"slug":"/slack-bot/","title":"Slack Bot","lastUpdatedAt":"2022-02-21T09:20:17.000Z","lastUpdated":"2/21/2022"},"frontmatter":{"draft":false,"tags":["gui"]}},{"fields":{"slug":"/neural-representations/","title":"Neural Representations","lastUpdatedAt":"2022-02-18T14:45:19.000Z","lastUpdated":"2/18/2022"},"frontmatter":{"draft":false,"tags":["neural_networks","from_paper"]}},{"fields":{"slug":"/embeddings-cant-possibly-be-right/","title":"Embeddings Can't Possibly Be Right","lastUpdatedAt":"2022-02-13T17:46:16.000Z","lastUpdated":"2/13/2022"},"frontmatter":{"draft":false,"tags":["from_paper"]}},{"fields":{"slug":"/explaining-word-embeddings/","title":"Explaining Word Embeddings","lastUpdatedAt":"2022-02-13T17:46:16.000Z","lastUpdated":"2/13/2022"},"frontmatter":{"draft":false,"tags":["from_article","nlp"]}},{"fields":{"slug":"/classification-vs-regression/","title":"Classification vs Regression","lastUpdatedAt":"2022-02-12T11:03:50.000Z","lastUpdated":"2/12/2022"},"frontmatter":{"draft":false,"tags":["interpolation","statistics","machine_learning"]}},{"fields":{"slug":"/fast-weights/","title":"Fast Weights","lastUpdatedAt":"2022-02-04T15:31:52.000Z","lastUpdated":"2/4/2022"},"frontmatter":{"draft":false,"tags":["neural_networks","biologically_inspired"]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}