{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/kahneman-neurips-perception/",
    "result": {"data":{"mdx":{"id":"035ad238-523a-5125-addc-84f30c68fcbe","tableOfContents":{"items":[{"url":"#perception","title":"Perception"}]},"fields":{"title":"Perception","slug":"/kahneman-neurips-perception/","url":"https://deepmind.vercel.app/kahneman-neurips-perception/","editUrl":"https://github.com/dfeng/notes/tree/main/kahneman-neurips-perception.md","lastUpdatedAt":"2021-12-24T10:27:07.000Z","lastUpdated":"12/24/2021","gitCreatedAt":"2021-12-24T10:27:07.000Z","shouldShowTitle":false},"frontmatter":{"title":"","description":null,"imageAlt":null,"tags":["neurips","psychology"],"date":null,"dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"neurips\", \"psychology\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"perception\"\n  }, \"Perception\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"top down vs bottom up (optical illusions)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"same data, different representations given context\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"AlphaGo's structure has parallels with system-1,2 thinking\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"the Monte Carlo tree search is system-2: has all the logic, slow, deliberate\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"however, you can't just exhaustively search the tree, so you need a neural network to have gain some intuition/heuristics about where to go in the tree, and that's system-1\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"GPT-3 \\u2260 system-1: heuristics are borne out of forming representations/models that are internally coherent\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"i.e. they find patterns, but they don't actually have \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"em\", {\n    parentName: \"li\"\n  }, \"absurd mistakes\"), \": 2-dimensional data, object-permanence, basically \"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"system-1,2:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"framed mainly for the benefit of the populace, as people understand \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"agents\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"in fact, it's more categorizations of mental processes, and some are slower and more deliberate than others\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"the example of simple shapes moving around: clearly we're proscribing agency and come with all this model baggage even for such simple data\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"system-1 is what happens 95% of the time, until you hit something surprising, not coherent, at which point it triggers system-2.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"system-2 can be thought of as an editor (filter) for system-1.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"distinction between \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"doubt\"), \" and \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"surprise\"), \":\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"instead of continuously predicting what is happening next, you see what happens and then make sense of it\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"the idea here is that you have your system-1 that just keeps running and continuously checking the data in a very straightforward manner. but then once something weird happens, your attention is drawn.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"much more economical (?)\")))));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\ntags:\n  - neurips\n  - psychology\n---\n\n# Perception\n\n - top down vs bottom up (optical illusions)\n - same data, different representations given context\n - AlphaGo's structure has parallels with system-1,2 thinking\n   - the Monte Carlo tree search is system-2: has all the logic, slow, deliberate\n   - however, you can't just exhaustively search the tree, so you need a neural network to have gain some intuition/heuristics about where to go in the tree, and that's system-1\n - \n - GPT-3 ≠ system-1: heuristics are borne out of forming representations/models that are internally coherent\n   - i.e. they find patterns, but they don't actually have \n   - *absurd mistakes*: 2-dimensional data, object-permanence, basically \n - system-1,2:\n   - framed mainly for the benefit of the populace, as people understand *agents*.\n   - in fact, it's more categorizations of mental processes, and some are slower and more deliberate than others\n - the example of simple shapes moving around: clearly we're proscribing agency and come with all this model baggage even for such simple data\n - system-1 is what happens 95% of the time, until you hit something surprising, not coherent, at which point it triggers system-2.\n - system-2 can be thought of as an editor (filter) for system-1.\n - distinction between *doubt* and *surprise*:\n   - instead of continuously predicting what is happening next, you see what happens and then make sense of it\n   - the idea here is that you have your system-1 that just keeps running and continuously checking the data in a very straightforward manner. but then once something weird happens, your attention is drawn.\n   - much more economical (?)","excerpt":"Perception top down vs bottom up (optical illusions) same data, different representations given context AlphaGo's structure has parallels w…","outboundReferences":[],"inboundReferences":[{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"artificial_intelligence\", \"machine_learning\", \"psychology\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"system-1-and-2\"\n  }, \"System 1 and 2\"), mdx(\"p\", null, \"Update: Kahneman actually talks about this in his NeurIPS 2021 invited talk: \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/kahneman-neurips-perception/\",\n    \"title\": \"Perception\"\n  }, \"kahneman-neurips-perception\"), \".\"), mdx(\"p\", null, \"I forget where this was mentioned (either in one of the AI podcast episodes, or this numenta \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.youtube.com/watch?v=0ZVOmBp29E0\"\n  }, \"video\"), \"), but basically we can think of GPT-3 as being the first almost perfect copy of system-1 human thinking, which is how Kahneman chose to dichotomise how our brains work---essentially, system-1 is the fast, intuitive thinking, while system-2 is the deliberate, rational, logical thinking.\"), mdx(\"p\", null, \"Pattern recognition is basically system-1, and it's where all the problems of correlation \\u2260 causation occur, since it's just focused on predicting things by association. And that's basically what GPT-3 is capable of doing.\"), mdx(\"p\", null, \"The question is then how do we get to system-2 thinking, which is pretty much our competitive edge---deliberate thought. Here's a random #idea that I had on my run, and I suspect someone has already thought about: what if system-2 = system-1 + simulation? It seems to me that the crucial piece of the puzzle is basically being able to \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"simulate\"), \" the world, or at least some very crude model of it. Once you have the capacity to simulate the world, then you can run your system-1 inferences, and see how things compare to the truths of your simulation, while also making sure to update your model of the world against reality.\"));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/system-1-and-2/","title":"System 1 and 2"}}]},"tagsOutbound":{"nodes":[{"frontmatter":{"title":"","tags":["neurips","psychology"]},"fields":{"slug":"/kahneman-neurips-perception/","title":"Perception","lastUpdated":"12/24/2021","lastUpdatedAt":"2021-12-24T10:27:07.000Z","gitCreatedAt":"2021-12-24T10:27:07.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","psychology"]},"fields":{"slug":"/multidimensional-mental-representations-of-natural-objects/","title":"Multidimensional Mental Representations of Natural Objects","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2021-12-24T11:18:15.000Z"}},{"frontmatter":{"title":"","tags":["artificial_intelligence","machine_learning","psychology"]},"fields":{"slug":"/system-1-and-2/","title":"System 1 and 2","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}}]}},"pageContext":{"tags":["neurips","psychology"],"slug":"/kahneman-neurips-perception/","sidebarItems":[{"title":"","items":[{"title":"Recently Updated","url":"/latest/","collapse":true,"indent":false,"items":[{"title":"01-30: On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"},{"title":"01-30: Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"},{"title":"01-30: The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"},{"title":"01-29: Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"01-29: Calculus For Brain Computation","url":"/calculus-for-brain-computation/"},{"title":"01-29: Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"01-29: Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"01-29: Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"01-29: Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"01-29: Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"}]}]},{"title":"Tags","items":[{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"Artificial Generalised Intelligence","url":"/artificial-generalised-intelligence/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"attention","type":"tag","url":"/tags/attention/","items":[{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"biologically_inspired","type":"tag","url":"/tags/biologically-inspired/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"},{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"causal_inference","type":"tag","url":"/tags/causal-inference/","items":[{"title":"Learning DAGs","url":"/learning-dags/"}]},{"title":"classic_papers","type":"tag","url":"/tags/classic-papers/","items":[{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"commentary","type":"tag","url":"/tags/commentary/","items":[{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"computer_vision","type":"tag","url":"/tags/computer-vision/","items":[{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"deep","type":"tag","url":"/tags/deep/","items":[{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"deep_learning","type":"tag","url":"/tags/deep-learning/","items":[{"title":"GPT-3","url":"/gpt3/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Bitter Lesson","url":"/bitter-lesson/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Next Steps for Deep Learning","url":"/next-steps-for-deep-learning/"}]},{"title":"from_paper","type":"tag","url":"/tags/from-paper/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Learning DAGs","url":"/learning-dags/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"implicit_regularisation","type":"tag","url":"/tags/implicit-regularisation/","items":[{"title":"Implicit-Regularisation","url":"/implicit-regularisation/"}]},{"title":"interpolation","type":"tag","url":"/tags/interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Invariant Risk Minimisation","url":"/invariant-risk-minimisation/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"System 1 and 2","url":"/system-1-and-2/"},{"title":"Transformers","url":"/transformers/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"matrix_completion","type":"tag","url":"/tags/matrix-completion/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"neuroscience","type":"tag","url":"/tags/neuroscience/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"}]},{"title":"nlp","type":"tag","url":"/tags/nlp/","items":[{"title":"Debiasing Word Embeddings","url":"/debiasing-word-embeddings/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"GPT-3","url":"/gpt3/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"optimisation","type":"tag","url":"/tags/optimisation/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"regularization","type":"tag","url":"/tags/regularization/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Alone","url":"/alone/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"theoretical_statistics","type":"tag","url":"/tags/theoretical-statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]}]}],"tagsGroups":[{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"Artificial Generalised Intelligence","url":"/artificial-generalised-intelligence/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"attention","type":"tag","url":"/tags/attention/","items":[{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"biologically_inspired","type":"tag","url":"/tags/biologically-inspired/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"},{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"causal_inference","type":"tag","url":"/tags/causal-inference/","items":[{"title":"Learning DAGs","url":"/learning-dags/"}]},{"title":"classic_papers","type":"tag","url":"/tags/classic-papers/","items":[{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"commentary","type":"tag","url":"/tags/commentary/","items":[{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"computer_vision","type":"tag","url":"/tags/computer-vision/","items":[{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"deep","type":"tag","url":"/tags/deep/","items":[{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"deep_learning","type":"tag","url":"/tags/deep-learning/","items":[{"title":"GPT-3","url":"/gpt3/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Bitter Lesson","url":"/bitter-lesson/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Next Steps for Deep Learning","url":"/next-steps-for-deep-learning/"}]},{"title":"from_paper","type":"tag","url":"/tags/from-paper/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Learning DAGs","url":"/learning-dags/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"implicit_regularisation","type":"tag","url":"/tags/implicit-regularisation/","items":[{"title":"Implicit-Regularisation","url":"/implicit-regularisation/"}]},{"title":"interpolation","type":"tag","url":"/tags/interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Invariant Risk Minimisation","url":"/invariant-risk-minimisation/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"System 1 and 2","url":"/system-1-and-2/"},{"title":"Transformers","url":"/transformers/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"matrix_completion","type":"tag","url":"/tags/matrix-completion/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"neuroscience","type":"tag","url":"/tags/neuroscience/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"}]},{"title":"nlp","type":"tag","url":"/tags/nlp/","items":[{"title":"Debiasing Word Embeddings","url":"/debiasing-word-embeddings/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"GPT-3","url":"/gpt3/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"optimisation","type":"tag","url":"/tags/optimisation/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"regularization","type":"tag","url":"/tags/regularization/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Alone","url":"/alone/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"theoretical_statistics","type":"tag","url":"/tags/theoretical-statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]}],"latestPosts":[{"fields":{"slug":"/on-optimisation-in-matrix-completion/","title":"On Optimisation in Matrix Completion","lastUpdatedAt":"2022-01-30T11:34:04.000Z","lastUpdated":"1/30/2022"},"frontmatter":{"draft":false,"tags":["matrix_completion","optimisation"]}},{"fields":{"slug":"/effectiveness-of-normalised-quantities/","title":"Effectiveness of Normalised Quantities","lastUpdatedAt":"2022-01-30T11:00:29.000Z","lastUpdated":"1/30/2022"},"frontmatter":{"draft":false,"tags":["matrix_completion"]}},{"fields":{"slug":"/the-unreasonable-effectiveness-of-adam/","title":"The Unreasonable Effectiveness of Adam","lastUpdatedAt":"2022-01-30T08:58:42.000Z","lastUpdated":"1/30/2022"},"frontmatter":{"draft":false,"tags":["optimisation","gradient_descent"]}},{"fields":{"slug":"/benign-overfitting-in-linear-regression/","title":"Benign Overfitting in Linear Regression","lastUpdatedAt":"2022-01-29T14:49:40.000Z","lastUpdated":"1/29/2022"},"frontmatter":{"draft":false,"tags":["from_paper","lit_review","interpolation"]}},{"fields":{"slug":"/calculus-for-brain-computation/","title":"Calculus For Brain Computation","lastUpdatedAt":"2022-01-29T14:49:40.000Z","lastUpdated":"1/29/2022"},"frontmatter":{"draft":false,"tags":["neuroscience","biologically_inspired"]}},{"fields":{"slug":"/can-you-learn-an-algorithm/","title":"Can You Learn an Algorithm","lastUpdatedAt":"2022-01-29T14:49:40.000Z","lastUpdated":"1/29/2022"},"frontmatter":{"draft":false,"tags":["from_paper"]}},{"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Meta-Learning From Kernel Ridge-Regression","lastUpdatedAt":"2022-01-29T14:49:40.000Z","lastUpdated":"1/29/2022"},"frontmatter":{"draft":false,"tags":["from_paper","machine_learning","neural_tangent_kernel"]}},{"fields":{"slug":"/michael-jordan-plenary-talk/","title":"Michael Jordan Plenary Talk","lastUpdatedAt":"2022-01-29T14:49:40.000Z","lastUpdated":"1/29/2022"},"frontmatter":{"draft":false,"tags":["from_talk","statistics","economics","data_science"]}},{"fields":{"slug":"/monopoly-in-tech/","title":"Monopoly in Tech","lastUpdatedAt":"2022-01-29T14:49:40.000Z","lastUpdated":"1/29/2022"},"frontmatter":{"draft":false,"tags":["money_stuff","finance"]}},{"fields":{"slug":"/multidimensional-mental-representations-of-natural-objects/","title":"Multidimensional Mental Representations of Natural Objects","lastUpdatedAt":"2022-01-29T14:49:40.000Z","lastUpdated":"1/29/2022"},"frontmatter":{"draft":false,"tags":["from_paper","psychology"]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}