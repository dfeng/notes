{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/knowledge-distillation/",
    "result": {"data":{"mdx":{"id":"ffebf10e-c385-5fe4-95de-9acb36b05428","tableOfContents":{"items":[{"url":"#knowledge-distillation","title":"Knowledge Distillation"}]},"fields":{"title":"Knowledge Distillation","slug":"/knowledge-distillation/","url":"https://dfeng.github.io/notes/notes/knowledge-distillation/","editUrl":"https://github.com/dfeng/notes/tree/main/knowledge-distillation.md","lastUpdatedAt":"2022-01-04T12:51:34.000Z","lastUpdated":"1/4/2022","gitCreatedAt":"2021-12-23T12:17:23.000Z","shouldShowTitle":false},"frontmatter":{"title":"","description":null,"imageAlt":null,"tags":["machine_learning"],"date":null,"dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"machine_learning\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"knowledge-distillation\"\n  }, \"Knowledge Distillation\"), mdx(\"p\", null, \"As a constructive example, think of two similar classifications (two types of breeds of dogs). With one-hot encoding, these are orthogonal, and each class is essentially equivalent/exchangeable. However there's actually more structure to these classifications (you can imagine them being in some embedding space, and the dog classes should be closer together). I guess the phrase is \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"label similarity\"), \".\"), mdx(\"p\", null, \"The softmax probabilities is (hopefully) a proxy for this. So, in part, the student gets the shortcut of learning these class representations, but also conceivably it does not have the capacity to \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"learn\"), \" these subtleties, though it is able to mimic it.\"), mdx(\"p\", null, \"Does Knowledge Distillation really work? \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/pdf/2106.05945.pdf\"\n  }, \"arXiv\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  })), mdx(\"p\", null, \"KD as Semi-parametric Inference \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www.youtube.com/watch?v=dEE3-g_8dWo\"\n  }, \"Youtube\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  })), mdx(\"p\", null, \"Many weird and curious results in this field:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"It turns out that if you do self-distillation (i.e. the student is the teacher, there's a Buddhist joke here somewhere), then the student will oftentimes outperform the teacher. \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/1805.04770\"\n  }, \"arXiv\"), \".\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\ntags:\n  - machine_learning\n---\n\n\n# Knowledge Distillation\n\nAs a constructive example, think of two similar classifications (two types of breeds of dogs). With one-hot encoding, these are orthogonal, and each class is essentially equivalent/exchangeable. However there's actually more structure to these classifications (you can imagine them being in some embedding space, and the dog classes should be closer together). I guess the phrase is *label similarity*.\n\nThe softmax probabilities is (hopefully) a proxy for this. So, in part, the student gets the shortcut of learning these class representations, but also conceivably it does not have the capacity to *learn* these subtleties, though it is able to mimic it.\n\nDoes Knowledge Distillation really work? [arXiv](https://arxiv.org/pdf/2106.05945.pdf)\n\n - \n\nKD as Semi-parametric Inference [Youtube](https://www.youtube.com/watch?v=dEE3-g_8dWo)\n\n - \n\nMany weird and curious results in this field:\n\n1. It turns out that if you do self-distillation (i.e. the student is the teacher, there's a Buddhist joke here somewhere), then the student will oftentimes outperform the teacher. [arXiv](https://arxiv.org/abs/1805.04770).","excerpt":"Knowledge Distillation As a constructive example, think of two similar classifications (two types of breeds of dogs). With one-hot encodingâ€¦","outboundReferences":[],"inboundReferences":[{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"paper\", \"machine_learning\", \"neural_tangent_kernel\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"dataset-meta-learning-from-kernel-ridge-regression\"\n  }, \"Dataset Meta-Learning From Kernel Ridge-Regression\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"src: \", \"(Nguyen et al., 2021)\", mdx(\"sup\", {\n    parentName: \"li\",\n    \"id\": \"fnref-nguyen_dataset_2021\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-nguyen_dataset_2021\",\n    \"className\": \"footnote-ref\"\n  }, \"nguyen_dataset_2021\")))), mdx(\"p\", null, \"You have \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/knowledge-distillation/\",\n    \"title\": \"Knowledge Distillation\"\n  }, \"knowledge-distillation\"), \", the goal of which is to compress models down while retaining the same performance. Less common is what can only be described as [\", \"[data-distillation]\", \"], the idea (?) of which is to compress datasets down in such a way that a particular algorithm trained on this compressed dataset would reach comparable test performance. \", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-1\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-1\",\n    \"className\": \"footnote-ref\"\n  }, \"1\"))), mdx(\"div\", {\n    \"className\": \"footnotes\"\n  }, mdx(\"hr\", {\n    parentName: \"div\"\n  }), mdx(\"ol\", {\n    parentName: \"div\"\n  }, mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-nguyen_dataset_2021\"\n  }, \"Nguyen, T., Chen, Z. & Lee, J., 2021. Dataset Meta-Learning from Kernel Ridge-Regression. arXiv:2011.00050 [cs, stat]. Available at: http://arxiv.org/abs/2011.00050 [Accessed December 25, 2021].\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-nguyen_dataset_2021\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")), mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-1\"\n  }, \"I feel like there's something interesting here for the psychologists. They're always looking to find the \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"archetypal\"), \" image for a class.\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-1\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")))));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Meta-Learning From Kernel Ridge-Regression"}}]},"tagsOutbound":{"nodes":[{"frontmatter":{"title":"","tags":["paper","machine_learning","neural_tangent_kernel"]},"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Meta-Learning From Kernel Ridge-Regression","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T12:51:34.000Z","gitCreatedAt":"2021-12-25T21:42:34.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning"]},"fields":{"slug":"/knowledge-distillation/","title":"Knowledge Distillation","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T12:51:34.000Z","gitCreatedAt":"2021-12-23T12:17:23.000Z"}},{"frontmatter":{"title":"","tags":["gui","machine_learning"]},"fields":{"slug":"/objection-detection-for-guis/","title":"Object Detection for GUI","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T12:51:34.000Z","gitCreatedAt":"2021-12-24T10:27:07.000Z"}}]}},"pageContext":{"tags":["machine_learning"],"slug":"/knowledge-distillation/","sidebarItems":[{"title":"","items":[{"title":"Recently Updated","url":"/latest/","collapse":true,"indent":false,"items":[{"title":"01-04: Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"01-04: Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"01-04: Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"01-04: Non-deep Networks","url":"/non-deep-networks/"},{"title":"01-04: Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"12-24: Triplet Loss","url":"/triplet-loss/"},{"title":"12-24: Perception","url":"/kahneman-neurips-perception/"},{"title":"12-24: 2021-12-24","url":"/journal/2021-12-24/"},{"title":"12-24: Artifactual Neural Network","url":"/"},{"title":"12-23: Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"}]}]},{"title":"Tags","items":[{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"paper","type":"tag","url":"/tags/paper/","items":[{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"}]}]}],"tagsGroups":[{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"paper","type":"tag","url":"/tags/paper/","items":[{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"}]}],"latestPosts":[{"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Meta-Learning From Kernel Ridge-Regression","lastUpdatedAt":"2022-01-04T12:51:34.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["paper","machine_learning","neural_tangent_kernel"]}},{"fields":{"slug":"/knowledge-distillation/","title":"Knowledge Distillation","lastUpdatedAt":"2022-01-04T12:51:34.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["machine_learning"]}},{"fields":{"slug":"/multidimensional-mental-representations-of-natural-objects/","title":"Multidimensional Mental Representations of Natural Objects","lastUpdatedAt":"2022-01-04T12:51:34.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["paper","psychology"]}},{"fields":{"slug":"/non-deep-networks/","title":"Non-deep Networks","lastUpdatedAt":"2022-01-04T12:51:34.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["paper"]}},{"fields":{"slug":"/objection-detection-for-guis/","title":"Object Detection for GUI","lastUpdatedAt":"2022-01-04T12:51:34.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["gui","machine_learning"]}},{"fields":{"slug":"/triplet-loss/","title":"Triplet Loss","lastUpdatedAt":"2021-12-24T18:09:30.000Z","lastUpdated":"12/24/2021"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/kahneman-neurips-perception/","title":"Perception","lastUpdatedAt":"2021-12-24T10:27:07.000Z","lastUpdated":"12/24/2021"},"frontmatter":{"draft":false,"tags":["neurips","psychology"]}},{"fields":{"slug":"/journal/2021-12-24/","title":"2021-12-24","lastUpdatedAt":"2021-12-24T10:27:07.000Z","lastUpdated":"12/24/2021"},"frontmatter":{"draft":false,"tags":["journal"]}},{"fields":{"slug":"/","title":"Artifactual Neural Network","lastUpdatedAt":"2021-12-24T08:23:18.000Z","lastUpdated":"12/24/2021"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/can-you-learn-an-algorithm/","title":"Can You Learn an Algorithm","lastUpdatedAt":"2021-12-23T11:27:19.000Z","lastUpdated":"12/23/2021"},"frontmatter":{"draft":false,"tags":["paper"]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}