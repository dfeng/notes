{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/meta-learning/",
    "result": {"data":{"mdx":{"id":"a7ac7671-a29f-5c97-8fc5-c129e3117d50","tableOfContents":{"items":[{"url":"#meta-learning","title":"Meta Learning"}]},"fields":{"title":"Meta Learning","slug":"/meta-learning/","url":"https://dfeng.github.io/notes/notes/meta-learning/","editUrl":"https://github.com/dfeng/notes/tree/main/meta-learning.md","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022","gitCreatedAt":"2022-01-04T21:27:53.000Z","shouldShowTitle":false},"frontmatter":{"title":"","description":null,"imageAlt":null,"tags":["machine_learning","meta_learning"],"date":null,"dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"machine_learning\", \"meta_learning\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"meta-learning\"\n  }, \"Meta Learning\"), mdx(\"p\", null, \"src: \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-105.pdf\"\n  }, \"thesis\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"relates to [\", \"[transfer-learning]\", \"], except you're trying to do an even better kind of pre-training, that generalises across all tasks\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intuition:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"you want to train a NN so that it does well on a whole class of tasks, and not specific to one dataset\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"you want to \\\"pre-train\\\" a NN such that, regardless of the task you eventually train it on, it'll be very quick to \\\"train\\\"\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"speed of training is a proxy for \\\"close-ness\\\" of the solution\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"if you're learning a useful representation, then this pre-trained model should speed up training\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"mark\", null, \"it seems to me that this doesn't necessarily hold: if you think of training a NN as \\\"learning\\\", then this is like starting with knowledge of \\\"fundamentals\\\". but I don't think the analogy works very well, since it's really just the \\\"initialisation\\\", and it quickly gets \\\"replaced\\\".\"), \"- I guess what I'd like is some kind of **augmentation** - a little more like how LSTMs have a memory slot\"))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"then they're going to use gradient updates, somehow\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"my initial guess:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"suppose you have a set of datasets. in some sense, what you want is to find the mid-point of all the trained models. you could just take one step per dataset (or sampled)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"but will this \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"converge\"), \"?\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"it might be the case that the steps you take are in the opposite direction?\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"though, if that's the case then that just means you've reached an \\\"impasse\\\" (or that there's nothing shared between them)\"))))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"em\", {\n    parentName: \"li\"\n  }, \"The process of training a model\\u2019s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"em\", {\n    parentName: \"li\"\n  }, \"From a dynamical systems standpoint, our learning process can be viewed as maximising the sensitivity of the loss functions of new tasks with respect to the parameters\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"they have some theory that shows that a single gradient-step can get you to any function (??) \\u2013 this seems impossible\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\ntags:\n - machine_learning\n - meta_learning\n---\n\n# Meta Learning\n\nsrc: [thesis](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-105.pdf)\n\n- relates to [[transfer-learning]], except you're trying to do an even better kind of pre-training, that generalises across all tasks\n- intuition:\n    - you want to train a NN so that it does well on a whole class of tasks, and not specific to one dataset\n    - you want to \"pre-train\" a NN such that, regardless of the task you eventually train it on, it'll be very quick to \"train\"\n        - speed of training is a proxy for \"close-ness\" of the solution\n        - if you're learning a useful representation, then this pre-trained model should speed up training\n        - <mark>it seems to me that this doesn't necessarily hold: if you think of training a NN as \"learning\", then this is like starting with knowledge of \"fundamentals\". but I don't think the analogy works very well, since it's really just the \"initialisation\", and it quickly gets \"replaced\".</mark>\n            - I guess what I'd like is some kind of **augmentation**\n            - a little more like how LSTMs have a memory slot\n- then they're going to use gradient updates, somehow\n- my initial guess:\n    - suppose you have a set of datasets. in some sense, what you want is to find the mid-point of all the trained models. you could just take one step per dataset (or sampled)\n    - but will this *converge*?\n        - it might be the case that the steps you take are in the opposite direction?\n            - though, if that's the case then that just means you've reached an \"impasse\" (or that there's nothing shared between them)\n- *The process of training a model’s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks.*\n- *From a dynamical systems standpoint, our learning process can be viewed as maximising the sensitivity of the loss functions of new tasks with respect to the parameters*\n- they have some theory that shows that a single gradient-step can get you to any function (??) – this seems impossible","excerpt":"Meta Learning src:  thesis relates to [ transfer-learning ], except you're trying to do an even better kind of pre-training, that generalis…","outboundReferences":[],"inboundReferences":[]},"tagsOutbound":{"nodes":[{"frontmatter":{"title":"","tags":["paper","machine_learning","neural_tangent_kernel"]},"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Meta-Learning From Kernel Ridge-Regression","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T12:51:34.000Z","gitCreatedAt":"2021-12-25T21:42:34.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning"]},"fields":{"slug":"/knowledge-distillation/","title":"Knowledge Distillation","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T12:51:34.000Z","gitCreatedAt":"2021-12-23T12:17:23.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning","meta_learning"]},"fields":{"slug":"/meta-learning/","title":"Meta Learning","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T21:27:53.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["gui","machine_learning"]},"fields":{"slug":"/objection-detection-for-guis/","title":"Object Detection for GUI","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T12:51:34.000Z","gitCreatedAt":"2021-12-24T10:27:07.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning","neural_networks"]},"fields":{"slug":"/transformers/","title":"Transformers","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T21:27:53.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}}]}},"pageContext":{"tags":["machine_learning","meta_learning"],"slug":"/meta-learning/","sidebarItems":[{"title":"","items":[{"title":"Recently Updated","url":"/latest/","collapse":true,"indent":false,"items":[{"title":"01-04: AI for Health","url":"/ai-for-health/"},{"title":"01-04: Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"01-04: Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"01-04: Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"01-04: Market for Lemons","url":"/market-for-lemons/"},{"title":"01-04: Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"01-04: Meta Learning","url":"/meta-learning/"},{"title":"01-04: Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"01-04: Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"01-04: One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]}]},{"title":"Tags","items":[{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"optimization","type":"tag","url":"/tags/optimization/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"paper","type":"tag","url":"/tags/paper/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"proj_interpolation","type":"tag","url":"/tags/proj-interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]}]}],"tagsGroups":[{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"optimization","type":"tag","url":"/tags/optimization/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"paper","type":"tag","url":"/tags/paper/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"proj_interpolation","type":"tag","url":"/tags/proj-interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]}],"latestPosts":[{"fields":{"slug":"/ai-for-health/","title":"AI for Health","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["xAI","medicine","artificial_intelligence"]}},{"fields":{"slug":"/benign-overfitting-in-linear-regression/","title":"Benign Overfitting in Linear Regression","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["paper","lit_review","proj_interpolation"]}},{"fields":{"slug":"/discretization-of-gradient-flow/","title":"Discretization of Gradient Flow","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["neural_networks","optimization"]}},{"fields":{"slug":"/graph-network-as-arbitrary-inductive-bias/","title":"Graph Network as Arbitrary Inductive Bias","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["from_article","neural_networks","graph_neural_networks"]}},{"fields":{"slug":"/market-for-lemons/","title":"Market for Lemons","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["economics","from_article"]}},{"fields":{"slug":"/meta-analysis-vs-preregistration/","title":"Meta Analysis vs Preregistration","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["from_podcast","meta_analysis","statistics"]}},{"fields":{"slug":"/meta-learning/","title":"Meta Learning","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["machine_learning","meta_learning"]}},{"fields":{"slug":"/michael-jordan-plenary-talk/","title":"Michael Jordan Plenary Talk","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["from_talk","statistics","data_science"]}},{"fields":{"slug":"/monopoly-in-tech/","title":"Monopoly in Tech","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["money_stuff","finance"]}},{"fields":{"slug":"/one-stock-to-rule-them-all/","title":"One Stock to Rule Them All","lastUpdatedAt":"2022-01-04T21:27:53.000Z","lastUpdated":"1/4/2022"},"frontmatter":{"draft":false,"tags":["finance","idea"]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}