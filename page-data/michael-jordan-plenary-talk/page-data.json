{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/michael-jordan-plenary-talk/",
    "result": {"data":{"mdx":{"id":"efdbcf9c-7c53-551c-83bb-bf0d3c43647f","tableOfContents":{"items":[{"url":"#michael-jordan-plenary-talk","title":"Michael Jordan Plenary Talk","items":[{"url":"#key-takeaways","title":"Key Takeaways"}]}]},"fields":{"title":"Michael Jordan Plenary Talk","slug":"/michael-jordan-plenary-talk/","url":"https://dfeng.github.io/notes/notes/michael-jordan-plenary-talk/","editUrl":"https://github.com/dfeng/notes/tree/main/michael-jordan-plenary-talk.md","lastUpdatedAt":"2022-01-12T12:14:44.000Z","lastUpdated":"1/12/2022","gitCreatedAt":"2022-01-04T21:27:53.000Z","shouldShowTitle":false},"frontmatter":{"title":"","description":null,"imageAlt":null,"tags":["from_talk","statistics","data_science"],"date":null,"dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"from_talk\", \"statistics\", \"data_science\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"michael-jordan-plenary-talk\"\n  }, \"Michael Jordan Plenary Talk\"), mdx(\"p\", null, \"Our MJ gave a plenary talk at the SIAM conference on the mathematics of DS (\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=69238\"\n  }, \"link\"), \").\\nI've seen bits and pieces of this talk already.\"), mdx(\"h2\", {\n    \"id\": \"key-takeaways\"\n  }, \"Key Takeaways\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"He thinks the next set of questions to ask of ML will be decision-based\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Example: recommendation systems. What if you recommend everyone the same restaurant? \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://hdsr.mitpress.mit.edu/pub/2imtstfu/release/6\"\n  }, \"article\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Clearly this isn't going to work out, so you need to do some type of \", mdx(\"em\", {\n    parentName: \"li\"\n  }, \"load-balancing\"), \". Or, better, just create a market.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Hence he thinks we'll be merging with microeconomics.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The essential issue here is scarcity, which therefore requires a market.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The question is then how do algorithms fit into this.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Example: you have the matching algorithm (i.e. Match day for medical students). But what if you don't know preferences, so you have to infer them in the way that you would do with bandit problems\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Of course, questions of [\", \"[causal-inference]\", \"] are also related.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This take is somewhat similar to what I'm going for with my [\", \"[fairness-project]\", \"], except that I don't think of markets as the solution. We're both thinking that these algorithms are affecting scarcity, but I'm tackling this slightly differently. It's all about the feedback mechanisms, which is also something that he hinted at.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/discretization-of-gradient-flow/\",\n    \"title\": \"Discretization of Gradient Flow\"\n  }, \"discretization-of-gradient-flow\"), \"\")));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\ntags:\n - from_talk\n - statistics\n - data_science\n---\n\n# Michael Jordan Plenary Talk\n\nOur MJ gave a plenary talk at the SIAM conference on the mathematics of DS ([link](https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=69238)).\nI've seen bits and pieces of this talk already.\n\n## Key Takeaways\n\n - He thinks the next set of questions to ask of ML will be decision-based\n   + Example: recommendation systems. What if you recommend everyone the same restaurant? [article](https://hdsr.mitpress.mit.edu/pub/2imtstfu/release/6)\n   + Clearly this isn't going to work out, so you need to do some type of *load-balancing*. Or, better, just create a market.\n   + Hence he thinks we'll be merging with microeconomics.\n   + The essential issue here is scarcity, which therefore requires a market.\n   + The question is then how do algorithms fit into this.\n   + Example: you have the matching algorithm (i.e. Match day for medical students). But what if you don't know preferences, so you have to infer them in the way that you would do with bandit problems\n   + Of course, questions of [[causal-inference]] are also related.\n   + This take is somewhat similar to what I'm going for with my [[fairness-project]], except that I don't think of markets as the solution. We're both thinking that these algorithms are affecting scarcity, but I'm tackling this slightly differently. It's all about the feedback mechanisms, which is also something that he hinted at.\n - [[discretization-of-gradient-flow]]\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[discretization-of-gradient-flow]: discretization-of-gradient-flow.md \"Discretization of Gradient Flow\"\n[//end]: # \"Autogenerated link references\"","excerpt":"Michael Jordan Plenary Talk Our MJ gave a plenary talk at the SIAM conference on the mathematics of DS ( link ).\nI've seen bits and pieces â€¦","outboundReferences":[{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"neural_networks\", \"optimisation\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"discretization-of-gradient-flow\"\n  }, \"Discretization of Gradient Flow\"), mdx(\"p\", null, \"Via \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/michael-jordan-plenary-talk/\",\n    \"title\": \"Michael Jordan Plenary Talk\"\n  }, \"michael-jordan-plenary-talk\"), \", to this paper on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/pdf/1905.07436.pdf\"\n  }, \"arXiv\"), \".\"), mdx(\"p\", null, \"You have gradient descent, which you can show under convex problems to have a convergence rate of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mfrac\", {\n    parentName: \"mrow\"\n  }, mdx(\"mn\", {\n    parentName: \"mfrac\"\n  }, \"1\"), mdx(\"mi\", {\n    parentName: \"mfrac\"\n  }, \"t\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\frac{1}{t}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1.190108em\",\n      \"verticalAlign\": \"-0.345em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mopen nulldelimiter\"\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mfrac\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.845108em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.6550000000000002em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"t\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.23em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"frac-line\",\n    \"style\": {\n      \"borderBottomWidth\": \"0.04em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.394em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"1\"))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.345em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  }))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mclose nulldelimiter\"\n  })))))), \", whereas if you use Nesterov's accelerated gradient method gets \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mfrac\", {\n    parentName: \"mrow\"\n  }, mdx(\"mn\", {\n    parentName: \"mfrac\"\n  }, \"1\"), mdx(\"msup\", {\n    parentName: \"mfrac\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"t\"), mdx(\"mn\", {\n    parentName: \"msup\"\n  }, \"2\")))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\frac{1}{t^2}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1.190108em\",\n      \"verticalAlign\": \"-0.345em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mopen nulldelimiter\"\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mfrac\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.845108em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.6550000000000002em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"t\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.7463142857142857em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.786em\",\n      \"marginRight\": \"0.07142857142857144em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.5em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size3 size1 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"2\"))))))))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.23em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"frac-line\",\n    \"style\": {\n      \"borderBottomWidth\": \"0.04em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.394em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"1\"))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.345em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  }))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mclose nulldelimiter\"\n  })))))), \".\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-1\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-1\",\n    \"className\": \"footnote-ref\"\n  }, \"1\"))), mdx(\"p\", null, \"If you take the limit of the step sizes to zero, then you're going to get some kind of differential equation. This is \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"gradient flow\"), \". It turns out you can basically construct a class of \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1603.04245\"\n  }, \"Bregman Lagrangians\"), \", which essentially encapsulate all the gradient methods.\"), mdx(\"p\", null, \"You can solve the Lagrangians for a particular rate, and then out pops an differential equation that obtains that rate in continuous time. What's curious is that the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"path\"), \" is identical across all these ODEs. Essentially you're getting path-independence from the rate, which suggests that this method has found an \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"optimal\"), \" path, and you can essentially tweak how fast you want to go along that path.\"), mdx(\"p\", null, \"This would suggest that you could then get arbritrary rates for your gradient method. But it turns out that the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"discretization\"), \" step is where things break. In fact, Nesterov already has a lower bound on his rate of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mfrac\", {\n    parentName: \"mrow\"\n  }, mdx(\"mn\", {\n    parentName: \"mfrac\"\n  }, \"1\"), mdx(\"msup\", {\n    parentName: \"mfrac\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"t\"), mdx(\"mn\", {\n    parentName: \"msup\"\n  }, \"2\")))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\frac{1}{t^2}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1.190108em\",\n      \"verticalAlign\": \"-0.345em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mopen nulldelimiter\"\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mfrac\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.845108em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.6550000000000002em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"t\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.7463142857142857em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.786em\",\n      \"marginRight\": \"0.07142857142857144em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.5em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size3 size1 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"2\"))))))))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.23em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"frac-line\",\n    \"style\": {\n      \"borderBottomWidth\": \"0.04em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.394em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"1\"))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.345em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  }))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mclose nulldelimiter\"\n  })))))), \",\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-2\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-2\",\n    \"className\": \"footnote-ref\"\n  }, \"2\")), \" so we know it can't do arbitrarily well. And it turns out that it does match the lower bound. The intuition is that the discretization suffers with curvature. If you go too quickly, then you're not going to be able to read the curvature well enough.\"), mdx(\"p\", null, \"In other words, discretization is non-trivially different to continuous time. Which sort of makes sense, since in continuous time you have basically all the information.\"), mdx(\"p\", null, \"Finally, in relation to the [\", \"[penalty-project]\", \"], this doesn't actually work for things like Adam, which we know to be amazing in practice. So, it seems like there's still work left to understand why on earth the adaptive learning rates work so well.\"), mdx(\"div\", {\n    \"className\": \"footnotes\"\n  }, mdx(\"hr\", {\n    parentName: \"div\"\n  }), mdx(\"ol\", {\n    parentName: \"div\"\n  }, mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-1\"\n  }, \"And this rate is entirely independent of the ambient dimension of the space.\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-1\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")), mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-2\"\n  }, \"The class of gradient methods are those that have access to all past gradients, I think.\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-2\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")))));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/discretization-of-gradient-flow/","title":"Discretization of Gradient Flow","shouldShowTitle":false}}],"inboundReferences":[{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"neural_networks\", \"optimisation\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"discretization-of-gradient-flow\"\n  }, \"Discretization of Gradient Flow\"), mdx(\"p\", null, \"Via \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/michael-jordan-plenary-talk/\",\n    \"title\": \"Michael Jordan Plenary Talk\"\n  }, \"michael-jordan-plenary-talk\"), \", to this paper on \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/pdf/1905.07436.pdf\"\n  }, \"arXiv\"), \".\"), mdx(\"p\", null, \"You have gradient descent, which you can show under convex problems to have a convergence rate of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mfrac\", {\n    parentName: \"mrow\"\n  }, mdx(\"mn\", {\n    parentName: \"mfrac\"\n  }, \"1\"), mdx(\"mi\", {\n    parentName: \"mfrac\"\n  }, \"t\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\frac{1}{t}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1.190108em\",\n      \"verticalAlign\": \"-0.345em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mopen nulldelimiter\"\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mfrac\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.845108em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.6550000000000002em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"t\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.23em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"frac-line\",\n    \"style\": {\n      \"borderBottomWidth\": \"0.04em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.394em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"1\"))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.345em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  }))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mclose nulldelimiter\"\n  })))))), \", whereas if you use Nesterov's accelerated gradient method gets \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mfrac\", {\n    parentName: \"mrow\"\n  }, mdx(\"mn\", {\n    parentName: \"mfrac\"\n  }, \"1\"), mdx(\"msup\", {\n    parentName: \"mfrac\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"t\"), mdx(\"mn\", {\n    parentName: \"msup\"\n  }, \"2\")))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\frac{1}{t^2}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1.190108em\",\n      \"verticalAlign\": \"-0.345em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mopen nulldelimiter\"\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mfrac\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.845108em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.6550000000000002em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"t\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.7463142857142857em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.786em\",\n      \"marginRight\": \"0.07142857142857144em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.5em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size3 size1 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"2\"))))))))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.23em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"frac-line\",\n    \"style\": {\n      \"borderBottomWidth\": \"0.04em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.394em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"1\"))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.345em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  }))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mclose nulldelimiter\"\n  })))))), \".\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-1\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-1\",\n    \"className\": \"footnote-ref\"\n  }, \"1\"))), mdx(\"p\", null, \"If you take the limit of the step sizes to zero, then you're going to get some kind of differential equation. This is \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"gradient flow\"), \". It turns out you can basically construct a class of \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/1603.04245\"\n  }, \"Bregman Lagrangians\"), \", which essentially encapsulate all the gradient methods.\"), mdx(\"p\", null, \"You can solve the Lagrangians for a particular rate, and then out pops an differential equation that obtains that rate in continuous time. What's curious is that the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"path\"), \" is identical across all these ODEs. Essentially you're getting path-independence from the rate, which suggests that this method has found an \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"optimal\"), \" path, and you can essentially tweak how fast you want to go along that path.\"), mdx(\"p\", null, \"This would suggest that you could then get arbritrary rates for your gradient method. But it turns out that the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"discretization\"), \" step is where things break. In fact, Nesterov already has a lower bound on his rate of \", mdx(\"span\", {\n    parentName: \"p\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mfrac\", {\n    parentName: \"mrow\"\n  }, mdx(\"mn\", {\n    parentName: \"mfrac\"\n  }, \"1\"), mdx(\"msup\", {\n    parentName: \"mfrac\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"t\"), mdx(\"mn\", {\n    parentName: \"msup\"\n  }, \"2\")))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\frac{1}{t^2}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1.190108em\",\n      \"verticalAlign\": \"-0.345em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mopen nulldelimiter\"\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mfrac\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.845108em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.6550000000000002em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"t\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.7463142857142857em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.786em\",\n      \"marginRight\": \"0.07142857142857144em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.5em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size3 size1 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"2\"))))))))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.23em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"frac-line\",\n    \"style\": {\n      \"borderBottomWidth\": \"0.04em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.394em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"1\"))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.345em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  }))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mclose nulldelimiter\"\n  })))))), \",\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-2\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-2\",\n    \"className\": \"footnote-ref\"\n  }, \"2\")), \" so we know it can't do arbitrarily well. And it turns out that it does match the lower bound. The intuition is that the discretization suffers with curvature. If you go too quickly, then you're not going to be able to read the curvature well enough.\"), mdx(\"p\", null, \"In other words, discretization is non-trivially different to continuous time. Which sort of makes sense, since in continuous time you have basically all the information.\"), mdx(\"p\", null, \"Finally, in relation to the [\", \"[penalty-project]\", \"], this doesn't actually work for things like Adam, which we know to be amazing in practice. So, it seems like there's still work left to understand why on earth the adaptive learning rates work so well.\"), mdx(\"div\", {\n    \"className\": \"footnotes\"\n  }, mdx(\"hr\", {\n    parentName: \"div\"\n  }), mdx(\"ol\", {\n    parentName: \"div\"\n  }, mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-1\"\n  }, \"And this rate is entirely independent of the ambient dimension of the space.\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-1\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")), mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-2\"\n  }, \"The class of gradient methods are those that have access to all past gradients, I think.\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-2\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")))));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/discretization-of-gradient-flow/","title":"Discretization of Gradient Flow"}}]},"tagsOutbound":{"nodes":[{"frontmatter":{"title":"","tags":["statistics","theoretical_statistics"]},"fields":{"slug":"/estimating-the-mean/","title":"Estimating the Mean","lastUpdated":"1/16/2022","lastUpdatedAt":"2022-01-16T10:24:03.000Z","gitCreatedAt":"2022-01-16T10:24:03.000Z"}},{"frontmatter":{"title":"","tags":["from_podcast","meta_analysis","statistics"]},"fields":{"slug":"/meta-analysis-vs-preregistration/","title":"Meta Analysis vs Preregistration","lastUpdated":"1/4/2022","lastUpdatedAt":"2022-01-04T21:27:53.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["from_talk","statistics","data_science"]},"fields":{"slug":"/michael-jordan-plenary-talk/","title":"Michael Jordan Plenary Talk","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["statistics","theoretical_statistics","commentary"]},"fields":{"slug":"/theoretical-statistics-beauty-or-banality/","title":"Theoretical Statistics: Beauty or Banality","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","statistics","research","classic_papers"]},"fields":{"slug":"/two-cultures/","title":"Two Cultures","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}}]}},"pageContext":{"tags":["from_talk","statistics","data_science"],"slug":"/michael-jordan-plenary-talk/","sidebarItems":[{"title":"","items":[{"title":"Recently Updated","url":"/latest/","collapse":true,"indent":false,"items":[{"title":"01-22: Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"01-22: Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"01-22: Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"01-22: System 1 and 2","url":"/system-1-and-2/"},{"title":"01-22: Vision Transformers","url":"/vision-transformers/"},{"title":"01-16: Estimating the Mean","url":"/estimating-the-mean/"},{"title":"01-12: Alone","url":"/alone/"},{"title":"01-12: Bitter Lesson","url":"/bitter-lesson/"},{"title":"01-12: Calculus-for-Brain-Computation","url":"/calculus-for-brain-computation/"},{"title":"01-12: Debiasing Word Embeddings","url":"/debiasing-word-embeddings/"}]}]},{"title":"Tags","items":[{"title":"GPT","type":"tag","url":"/tags/gpt/","items":[{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"attention","type":"tag","url":"/tags/attention/","items":[{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"biologically_inspired","type":"tag","url":"/tags/biologically-inspired/","items":[{"title":"Calculus-for-Brain-Computation","url":"/calculus-for-brain-computation/"},{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"causal_inference","type":"tag","url":"/tags/causal-inference/","items":[{"title":"Learning DAGs","url":"/learning-dags/"}]},{"title":"classic_papers","type":"tag","url":"/tags/classic-papers/","items":[{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"commentary","type":"tag","url":"/tags/commentary/","items":[{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"computer_vision","type":"tag","url":"/tags/computer-vision/","items":[{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"deep","type":"tag","url":"/tags/deep/","items":[{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"deep_learning","type":"tag","url":"/tags/deep-learning/","items":[{"title":"GPT-3","url":"/gpt3/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Bitter Lesson","url":"/bitter-lesson/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Next Steps for Deep Learning","url":"/next-steps-for-deep-learning/"}]},{"title":"from_paper","type":"tag","url":"/tags/from-paper/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Learning DAGs","url":"/learning-dags/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"implicit_regularisation","type":"tag","url":"/tags/implicit-regularisation/","items":[{"title":"Implicit-Regularisation","url":"/implicit-regularisation/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Invariant Risk Minimisation","url":"/invariant-risk-minimisation/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"System 1 and 2","url":"/system-1-and-2/"},{"title":"Transformers","url":"/transformers/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"matrix_completion","type":"tag","url":"/tags/matrix-completion/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"neuroscience","type":"tag","url":"/tags/neuroscience/","items":[{"title":"Calculus-for-Brain-Computation","url":"/calculus-for-brain-computation/"}]},{"title":"nlp","type":"tag","url":"/tags/nlp/","items":[{"title":"Debiasing Word Embeddings","url":"/debiasing-word-embeddings/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"GPT-3","url":"/gpt3/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"optimisation","type":"tag","url":"/tags/optimisation/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"paper","type":"tag","url":"/tags/paper/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"penalty","type":"tag","url":"/tags/penalty/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"}]},{"title":"proj_interpolation","type":"tag","url":"/tags/proj-interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"regularization","type":"tag","url":"/tags/regularization/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Alone","url":"/alone/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"theoretical_statistics","type":"tag","url":"/tags/theoretical-statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]}]}],"tagsGroups":[{"title":"GPT","type":"tag","url":"/tags/gpt/","items":[{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"attention","type":"tag","url":"/tags/attention/","items":[{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"biologically_inspired","type":"tag","url":"/tags/biologically-inspired/","items":[{"title":"Calculus-for-Brain-Computation","url":"/calculus-for-brain-computation/"},{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"causal_inference","type":"tag","url":"/tags/causal-inference/","items":[{"title":"Learning DAGs","url":"/learning-dags/"}]},{"title":"classic_papers","type":"tag","url":"/tags/classic-papers/","items":[{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"commentary","type":"tag","url":"/tags/commentary/","items":[{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"computer_vision","type":"tag","url":"/tags/computer-vision/","items":[{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"deep","type":"tag","url":"/tags/deep/","items":[{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"deep_learning","type":"tag","url":"/tags/deep-learning/","items":[{"title":"GPT-3","url":"/gpt3/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Bitter Lesson","url":"/bitter-lesson/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Next Steps for Deep Learning","url":"/next-steps-for-deep-learning/"}]},{"title":"from_paper","type":"tag","url":"/tags/from-paper/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Learning DAGs","url":"/learning-dags/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"implicit_regularisation","type":"tag","url":"/tags/implicit-regularisation/","items":[{"title":"Implicit-Regularisation","url":"/implicit-regularisation/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"journal","type":"tag","url":"/tags/journal/","items":[{"title":"2021-12-24","url":"/journal/2021-12-24/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Invariant Risk Minimisation","url":"/invariant-risk-minimisation/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"System 1 and 2","url":"/system-1-and-2/"},{"title":"Transformers","url":"/transformers/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"matrix_completion","type":"tag","url":"/tags/matrix-completion/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"neuroscience","type":"tag","url":"/tags/neuroscience/","items":[{"title":"Calculus-for-Brain-Computation","url":"/calculus-for-brain-computation/"}]},{"title":"nlp","type":"tag","url":"/tags/nlp/","items":[{"title":"Debiasing Word Embeddings","url":"/debiasing-word-embeddings/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"GPT-3","url":"/gpt3/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"optimisation","type":"tag","url":"/tags/optimisation/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"paper","type":"tag","url":"/tags/paper/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Meta-Learning From Kernel Ridge-Regression","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"penalty","type":"tag","url":"/tags/penalty/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"}]},{"title":"proj_interpolation","type":"tag","url":"/tags/proj-interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"regularization","type":"tag","url":"/tags/regularization/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Alone","url":"/alone/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"theoretical_statistics","type":"tag","url":"/tags/theoretical-statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]}],"latestPosts":[{"fields":{"slug":"/computer-vision-tasks/","title":"Computer Vision Tasks","lastUpdatedAt":"2022-01-22T10:54:29.000Z","lastUpdated":"1/22/2022"},"frontmatter":{"draft":false,"tags":["computer_vision","biologically_inspired"]}},{"fields":{"slug":"/discretization-of-gradient-flow/","title":"Discretization of Gradient Flow","lastUpdatedAt":"2022-01-22T10:54:29.000Z","lastUpdated":"1/22/2022"},"frontmatter":{"draft":false,"tags":["neural_networks","optimisation"]}},{"fields":{"slug":"/generalised-neural-networks/","title":"Generalized Neural Networks","lastUpdatedAt":"2022-01-22T10:54:29.000Z","lastUpdated":"1/22/2022"},"frontmatter":{"draft":false,"tags":["neural_networks","machine_learning"]}},{"fields":{"slug":"/system-1-and-2/","title":"System 1 and 2","lastUpdatedAt":"2022-01-22T10:54:29.000Z","lastUpdated":"1/22/2022"},"frontmatter":{"draft":false,"tags":["artificial_intelligence","GPT","machine_learning","psychology"]}},{"fields":{"slug":"/vision-transformers/","title":"Vision Transformers","lastUpdatedAt":"2022-01-22T10:54:29.000Z","lastUpdated":"1/22/2022"},"frontmatter":{"draft":false,"tags":["paper","machine_learning","computer_vision","attention"]}},{"fields":{"slug":"/estimating-the-mean/","title":"Estimating the Mean","lastUpdatedAt":"2022-01-16T10:24:03.000Z","lastUpdated":"1/16/2022"},"frontmatter":{"draft":false,"tags":["statistics","theoretical_statistics"]}},{"fields":{"slug":"/alone/","title":"Alone","lastUpdatedAt":"2022-01-12T12:26:42.000Z","lastUpdated":"1/12/2022"},"frontmatter":{"draft":false,"tags":["society"]}},{"fields":{"slug":"/bitter-lesson/","title":"Bitter Lesson","lastUpdatedAt":"2022-01-12T12:14:44.000Z","lastUpdated":"1/12/2022"},"frontmatter":{"draft":false,"tags":["from_article"]}},{"fields":{"slug":"/calculus-for-brain-computation/","title":"Calculus-for-Brain-Computation","lastUpdatedAt":"2022-01-12T12:14:44.000Z","lastUpdated":"1/12/2022"},"frontmatter":{"draft":false,"tags":["neuroscience","biologically_inspired"]}},{"fields":{"slug":"/debiasing-word-embeddings/","title":"Debiasing Word Embeddings","lastUpdatedAt":"2022-01-12T12:14:44.000Z","lastUpdated":"1/12/2022"},"frontmatter":{"draft":false,"tags":["nlp"]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}