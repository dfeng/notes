{
    "componentChunkName": "component---node-modules-gatsby-theme-primer-wiki-src-templates-post-query-js",
    "path": "/benign-overfitting-in-linear-regression/",
    "result": {"data":{"mdx":{"id":"e691db06-6c3c-50d5-8b65-54b3a6d5be8d","tableOfContents":{"items":[{"url":"#benign-overfitting-in-linear-regression","title":"Benign Overfitting in Linear Regression"}]},"fields":{"title":"Benign Overfitting in Linear Regression","slug":"/benign-overfitting-in-linear-regression/","url":"https://deepmind.vercel.app/benign-overfitting-in-linear-regression/","editUrl":"https://github.com/dfeng/notes/tree/main/benign-overfitting-in-linear-regression.md","lastUpdatedAt":"2022-01-29T14:49:40.000Z","lastUpdated":"1/29/2022","gitCreatedAt":"2022-01-04T21:27:53.000Z","shouldShowTitle":false},"frontmatter":{"title":"","description":null,"imageAlt":null,"tags":["from_paper","lit_review","interpolation"],"date":null,"dateModified":null,"language":null,"seoTitle":null,"image":null},"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"from_paper\", \"lit_review\", \"interpolation\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"benign-overfitting-in-linear-regression\"\n  }, \"Benign Overfitting in Linear Regression\"), mdx(\"p\", null, \"src: \", \"(Bartlett et al., 2020)\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-Bartlett:2020ck\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-Bartlett:2020ck\",\n    \"className\": \"footnote-ref\"\n  }, \"Bartlett:2020ck\")), \".\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"references:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.stat.berkeley.edu/~bartlett/talks/201908Microsoft.pdf\"\n  }, \"slides\"), \", better exposition\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"new paper that generalises this: \\\"Benign overfitting in the large deviation regime\\\", via \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/abs/2003.05838.pdf\"\n  }, \"arXiv\")))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"setting:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"linear regression (quadratic loss), linear prediction rule\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"mark\", null, \"important\"), \": we are actually thinking about the ML regime, whereby there isn't just a *true* parameter, but that there is a *best* one by considering the expectation (which, under general settings, corresponds to what would be the \\\"truth\\\").\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"mark\", null, \"important\"), \": they are considering random covariates in their analysis! I guess this is pretty standard practice, when you're calculating things like *risk*\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"that is, you can think of linear regression as a linear approximation to \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"double-struck\"\n  }, \"E\"), mdx(\"mo\", {\n    parentName: \"mrow\",\n    \"stretchy\": \"false\"\n  }, \"(\"), mdx(\"msub\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msub\"\n  }, \"Y\"), mdx(\"mi\", {\n    parentName: \"msub\"\n  }, \"i\")), mdx(\"mtext\", {\n    parentName: \"mrow\"\n  }, \"\\u2009\"), mdx(\"mo\", {\n    parentName: \"mrow\"\n  }, \"\\u2223\"), mdx(\"mtext\", {\n    parentName: \"mrow\"\n  }, \"\\u2009\"), mdx(\"msub\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msub\"\n  }, \"X\"), mdx(\"mi\", {\n    parentName: \"msub\"\n  }, \"i\")), mdx(\"mo\", {\n    parentName: \"mrow\",\n    \"stretchy\": \"false\"\n  }, \")\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\mathbb{E} ( Y_i \\\\,\\\\mid\\\\, X_i)\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1em\",\n      \"verticalAlign\": \"-0.25em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathbb\"\n  }, \"E\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mopen\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.22222em\"\n    }\n  }, \"Y\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.31166399999999994em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.5500000000000003em\",\n      \"marginLeft\": \"-0.22222em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"i\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.15em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  })))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.16666666666666666em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mrel\"\n  }, \"\\u2223\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.16666666666666666em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1em\",\n      \"verticalAlign\": \"-0.25em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07847em\"\n    }\n  }, \"X\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.31166399999999994em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.5500000000000003em\",\n      \"marginLeft\": \"-0.07847em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal mtight\"\n  }, \"i\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.15em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  })))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mclose\"\n  }, \")\")))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.timlrx.com/2018/02/26/notes-on-regression-approximation-of-the-conditional-expectation-function/\"\n  }, \"blog\"), \" with a good explanation: very standard classical exposition, showing that the conditional expectation is the optimal choice for \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"msub\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msub\"\n  }, \"L\"), mdx(\"mn\", {\n    parentName: \"msub\"\n  }, \"2\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"L_2\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.83333em\",\n      \"verticalAlign\": \"-0.15em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"L\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.30110799999999993em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.5500000000000003em\",\n      \"marginLeft\": \"0em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"2\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.15em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  })))))))))), \" loss, and the linear regression solutions is the best linear approximation to the conditional expectation (something like that)\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"infinite dimensional data: \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"p\"), mdx(\"mo\", {\n    parentName: \"mrow\"\n  }, \"=\"), mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u221E\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"p = \\\\infty\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.625em\",\n      \"verticalAlign\": \"-0.19444em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"p\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mrel\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.43056em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u221E\"))))), \" (separable Hilbert space)\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"though results apply to finite-dimensional\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"thus, there exists \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"msup\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"\\u03B8\"), mdx(\"mo\", {\n    parentName: \"msup\"\n  }, \"\\u2217\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\theta^*\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.69444em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.02778em\"\n    }\n  }, \"\\u03B8\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.688696em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.063em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mbin mtight\"\n  }, \"\\u2217\")))))))))))), \" that minimises the \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"expected\"), \" quadratic loss\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"in order to be able to calculate expected value, they must be assuming some model (normal error)\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"but they want to deal with the interpolating regime, so at the very least \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"p\"), mdx(\"mo\", {\n    parentName: \"mrow\"\n  }, \">\"), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"n\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"p > n\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.7335400000000001em\",\n      \"verticalAlign\": \"-0.19444em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"p\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mrel\"\n  }, \">\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.43056em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"n\"))))), \", and probably >>\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"We ask when it is possible to fit the data exactly and still compete with the prediction accuracy of \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"msup\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"\\u03B8\"), mdx(\"mo\", {\n    parentName: \"msup\"\n  }, \"\\u2217\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\theta^*\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.69444em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.02778em\"\n    }\n  }, \"\\u03B8\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.688696em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.063em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mbin mtight\"\n  }, \"\\u2217\")))))))))))), \".\\\"\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"del\", {\n    parentName: \"li\"\n  }, \"Unless I'm being stupid, this line doesn't make any sense. If you fit the data exactly, then by definition you're an optimal solution\")))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"the solution that minimises is underdetermined, so there isn't a unique solution to the convex program\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"okay, so I think what they're trying to say is that, if your choice of \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mover\", {\n    parentName: \"mrow\",\n    \"accent\": \"true\"\n  }, mdx(\"mi\", {\n    parentName: \"mover\"\n  }, \"\\u03B8\"), mdx(\"mo\", {\n    parentName: \"mover\"\n  }, \"^\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\hat{\\\\theta}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.9578799999999998em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord accent\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.9578799999999998em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.02778em\"\n    }\n  }, \"\\u03B8\"))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.26344em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"accent-body\",\n    \"style\": {\n      \"left\": \"-0.16666em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"^\"))))))))))), \" is the one that minimises the quadratic loss (= 0), and then pick the one with minimum norm (what norm), then let's see when do you get good generalizability\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"We ask when it is possible to overfit in this way \\u2013 and embed all of the noise of the labels into the parameter estimate \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mover\", {\n    parentName: \"mrow\",\n    \"accent\": \"true\"\n  }, mdx(\"mi\", {\n    parentName: \"mover\"\n  }, \"\\u03B8\"), mdx(\"mo\", {\n    parentName: \"mover\"\n  }, \"^\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\hat{\\\\theta}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.9578799999999998em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord accent\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.9578799999999998em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.02778em\"\n    }\n  }, \"\\u03B8\"))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.26344em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"accent-body\",\n    \"style\": {\n      \"left\": \"-0.16666em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"^\"))))))))))), \" \\u2013 without harming prediction accuracy\\\"\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"results:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"covariance matrix \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u03A3\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\Sigma\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u03A3\"))))), \" plays a crucial role\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"prediction error has following decomposition (two terms):\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"provided nuclear norm of \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u03A3\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\Sigma\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u03A3\"))))), \" is small compared to \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"n\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"n\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.43056em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"n\"))))), \", then \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mover\", {\n    parentName: \"mrow\",\n    \"accent\": \"true\"\n  }, mdx(\"mi\", {\n    parentName: \"mover\"\n  }, \"\\u03B8\"), mdx(\"mo\", {\n    parentName: \"mover\"\n  }, \"^\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\hat{\\\\theta}\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.9578799999999998em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord accent\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.9578799999999998em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.02778em\"\n    }\n  }, \"\\u03B8\"))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.26344em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"accent-body\",\n    \"style\": {\n      \"left\": \"-0.16666em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"^\"))))))))))), \" can find \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"msup\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"\\u03B8\"), mdx(\"mo\", {\n    parentName: \"msup\"\n  }, \"\\u2217\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\theta^*\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.69444em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.02778em\"\n    }\n  }, \"\\u03B8\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.688696em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.063em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mbin mtight\"\n  }, \"\\u2217\"))))))))))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"impact of noise in the labels on prediction accuracy \", mdx(\"mark\", null, \"(important)\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"this is small iff the effective rank of \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u03A3\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\Sigma\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u03A3\"))))), \" in the subspace corresponding to the low variance directions is large compared to \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"n\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"n\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.43056em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"n\"))))), \"\\\"\"))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"fitting the training data exactly but with near-optimal prediction accuracy occurs if and only if there are many low variance (and hence unimportant) directions in parameter space where the label noise can be hidden\\\"\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"intuition: I don't think they've provided much in the way of any intuition about what is going on, so let's see if we can come up with some\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"this is weird because we're thinking of linear regression, and so basically it's a linear combination of the \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"X\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"X\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07847em\"\n    }\n  }, \"X\"))))), \"'s to get \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"Y\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"Y\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.22222em\"\n    }\n  }, \"Y\"))))), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"if we were to think about this geometrically, is that we have an infinite dimensional \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"C\"), mdx(\"mo\", {\n    parentName: \"mrow\",\n    \"stretchy\": \"false\"\n  }, \"(\"), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"X\"), mdx(\"mo\", {\n    parentName: \"mrow\",\n    \"stretchy\": \"false\"\n  }, \")\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"C(X)\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1em\",\n      \"verticalAlign\": \"-0.25em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07153em\"\n    }\n  }, \"C\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mopen\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07847em\"\n    }\n  }, \"X\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mclose\"\n  }, \")\"))))), \", with sufficient \\\"range\\\"/span that our \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"Y\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"Y\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.22222em\"\n    }\n  }, \"Y\"))))), \" lands in \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"C\"), mdx(\"mo\", {\n    parentName: \"mrow\",\n    \"stretchy\": \"false\"\n  }, \"(\"), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"X\"), mdx(\"mo\", {\n    parentName: \"mrow\",\n    \"stretchy\": \"false\"\n  }, \")\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"C(X)\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1em\",\n      \"verticalAlign\": \"-0.25em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07153em\"\n    }\n  }, \"C\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mopen\"\n  }, \"(\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07847em\"\n    }\n  }, \"X\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mclose\"\n  }, \")\"))))), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"mark\", null, \"I feel like what they're trying to do is basically have the main true linear model, and then what you have are these small vectors in all sorts of directions, so that you can just add those to your solution, in order to interpolate (??)\"), \"- and since we're dealing with a model that is linear in truth, then - I'm confused, because since both $x,y$ are random, then calculating the risk is over both of these things.\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"takeaway:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"basically, the eigenvalues of the covariance matrix should satisfy:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"many non-zero entries, large compared to \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"n\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"n\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.43056em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"n\")))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"small sum compared to \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"n\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"n\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.43056em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"n\"))))), \" (smallest eigenvalues decays slowly)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"i.e. lots of small ones, probably some big ones (?)\"))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"summary: this is what I think is going on\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"let's just start with \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"X\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"X\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07847em\"\n    }\n  }, \"X\"))))), \" and it's covariance matrix \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u03A3\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\Sigma\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u03A3\"))))), \": what we have here is something, at the population level, that can be described by a few large eigenvalues, and then many, many small eigenvalues. The idea is that the \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u03A3\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\Sigma\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.68333em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u03A3\"))))), \" describes the distribution of the sampled \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"x\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"x\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.43056em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"x\"))))), \" vectors, and so the eigenvalues determine the variability in the direction of the respective eigenvector. However, there is no guarantee that there needs to be a rotation, so it is entirely possible that the variability in the \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"x\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"x\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.43056em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"x\"))))), \"'s are concentrated on individual coordinates.\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"del\", {\n    parentName: \"li\"\n  }, \"I incorrectly thought that what we have are random vectors, and so this spans all the coordinates, which makes it easier to extract the coefficients\")))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"So what we need are very many small directions of \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"x\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"x\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.43056em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"x\"))))), \" (again, you can just align them to a coordinate)...\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The optimal thing would be this nice sort of decomposition, where the first few elements of \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"\\u03B2\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\beta\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.8888799999999999em\",\n      \"verticalAlign\": \"-0.19444em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.05278em\"\n    }\n  }, \"\\u03B2\"))))), \" correspond to the true coefficients. Then, the point is that the error can be entirely captured by a linear combination of the rest of the covariates.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"589px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/d4f690a7133a2898d5a746466a86061e/b1665/benign_1.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"64%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsTAAALEwEAmpwYAAABnklEQVQ4y51TTUvDQBDN/7958mLPXjwUBEGPHhQVoaIWKmKMtW3SNh9Nmq9tkidv0qmh9CAOTGd2d/bNe9ON1aA1xrppoOu/WPeOusWfZL3eHVR1jaIsxfOigNls4M3nCKMIxhjZo5fG7O4wGlNKtHho2zayPJeuqzjG0vexTlPESSLNuPaDQNa6xwZplsqdPBrDHZyhMJsWkMUKyGKy2zcyJQM11iTrRPLUt7F4OEGep7B4MJlOhRFlMCq4SmLTNMtkHArKvSiKUDEPx3DveygU8HU4FFksnS8WAtoFnM5mIpGAypGAYRiiBuDP3jG8OoIpc1iu58FxHGGgkjUnIEFobLKpqh0gVajk1WwE766HsizbGZKdylTp+8Z9nWOzN8PAfcHT7TEKU8HSZ5FtWVHaYrkUlmRL5z/veR6CMES0WklNHMf4nkzQ1MCX/4n+4ymqpm7foQ6bOdkRhMzJmlFl6+PF9g7raJ/LD1w+n0tudV9691kc+iK63q0cuUNcDPotYLf40MVDZ5pTIs0JbNy8Xf8C/td34GjE+S3/AMs682YNpvDcAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"benign 1\",\n    \"title\": \"benign 1\",\n    \"src\": \"/static/d4f690a7133a2898d5a746466a86061e/b1665/benign_1.png\",\n    \"srcSet\": [\"/static/d4f690a7133a2898d5a746466a86061e/8a4e8/benign_1.png 150w\", \"/static/d4f690a7133a2898d5a746466a86061e/5a46d/benign_1.png 300w\", \"/static/d4f690a7133a2898d5a746466a86061e/b1665/benign_1.png 589w\"],\n    \"sizes\": \"(max-width: 589px) 100vw, 589px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"This is a more explicit decomposition of the two terms\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"600px\"\n    }\n  }, \"\\n      \", mdx(\"a\", {\n    parentName: \"span\",\n    \"className\": \"gatsby-resp-image-link\",\n    \"href\": \"/static/4cee7764e38a7d06c164e8c7110c3979/aec65/benign_2.png\",\n    \"style\": {\n      \"display\": \"block\"\n    },\n    \"target\": \"_blank\",\n    \"rel\": \"noopener\"\n  }, \"\\n    \", mdx(\"span\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"50.66666666666667%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAABcUlEQVQoz3VSy2rDMBD0//9JoRRCS+ih0B4KvaWFkBz6CElTyMt27FhePSxpysqW65JWRuyiHc3OypM47+EBxIguH57H3P9b87DehTyJh7yU1pBKIS7vfZ8PcRy1MQNcFwEksZOoa2R5Hjbn5ekEUhpU7lAXm/5yYy2ORYHlcomiLFETwcFjna2gjEbCinhz0ToXLvAmqkEGyN4esJvfBjJjDCohAp4xgmpUlUDjLO7mY+zLXauQOzJhHIeXqEUYYT27x2oyCjkRBfVMFho0DUjKoGz0dIm0SpHEAo/4i1BUcNzsc4J0dg3bjXyqqp6Q31FKhVoRxs8jKKtahQw4pGmIPDaT5sccqgHo8Ip8egFjXU8YR2Z1/ARkJG5eriAb2RIySSxuttvwY/LjEYIURLHB4f0RtlOutQ44xuz2exBJ6MZg9jWFcU1rmx9zAB+LRfhz6H0HWI8zr/JbplnW485sE8HDsaNp+fvL6EN/us7Y3zS7Cgm5QRHyAAAAAElFTkSuQmCC')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  }), \"\\n  \", mdx(\"img\", {\n    parentName: \"a\",\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"benign 2\",\n    \"title\": \"benign 2\",\n    \"src\": \"/static/4cee7764e38a7d06c164e8c7110c3979/0a47e/benign_2.png\",\n    \"srcSet\": [\"/static/4cee7764e38a7d06c164e8c7110c3979/8a4e8/benign_2.png 150w\", \"/static/4cee7764e38a7d06c164e8c7110c3979/5a46d/benign_2.png 300w\", \"/static/4cee7764e38a7d06c164e8c7110c3979/0a47e/benign_2.png 600w\", \"/static/4cee7764e38a7d06c164e8c7110c3979/aec65/benign_2.png 687w\"],\n    \"sizes\": \"(max-width: 600px) 100vw, 600px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\",\n    \"decoding\": \"async\"\n  }), \"\\n  \"), \"\\n    \")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The idea here is that, in some simplified sense, the error is just a random Gaussian vector, and we should be able to represent it exactly as a linear combination of a lot of small gaussian vectors.\")))), mdx(\"div\", {\n    \"className\": \"footnotes\"\n  }, mdx(\"hr\", {\n    parentName: \"div\"\n  }), mdx(\"ol\", {\n    parentName: \"div\"\n  }, mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-Bartlett:2020ck\"\n  }, \"Bartlett, P.L. et al., 2020. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences of the United States of America, 80, p.201907378.\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-Bartlett:2020ck\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")))));\n}\n;\nMDXContent.isMDXComponent = true;","rawBody":"---\ntags:\n - from_paper\n - lit_review\n - interpolation\n---\n\n# Benign Overfitting in Linear Regression\n\nsrc: [@Bartlett:2020ck].\n\n- references:\n    - [slides](https://www.stat.berkeley.edu/~bartlett/talks/201908Microsoft.pdf), better exposition\n    - new paper that generalises this: \"Benign overfitting in the large deviation regime\", via [arXiv](https://arxiv.org/abs/2003.05838.pdf)\n- setting:\n    - linear regression (quadratic loss), linear prediction rule\n        - <mark>important</mark>: we are actually thinking about the ML regime, whereby there isn't just a *true* parameter, but that there is a *best* one by considering the expectation (which, under general settings, corresponds to what would be the \"truth\").\n        - <mark>important</mark>: they are considering random covariates in their analysis! I guess this is pretty standard practice, when you're calculating things like *risk*\n        - that is, you can think of linear regression as a linear approximation to $\\mathbb{E} ( Y_i \\,\\mid\\, X_i)$\n        - [blog](https://www.timlrx.com/2018/02/26/notes-on-regression-approximation-of-the-conditional-expectation-function/) with a good explanation: very standard classical exposition, showing that the conditional expectation is the optimal choice for $L_2$ loss, and the linear regression solutions is the best linear approximation to the conditional expectation (something like that)\n    - infinite dimensional data: $p = \\infty$ (separable Hilbert space)\n        - though results apply to finite-dimensional\n    - thus, there exists $\\theta^*$ that minimises the **expected** quadratic loss\n        - in order to be able to calculate expected value, they must be assuming some model (normal error)\n    - but they want to deal with the interpolating regime, so at the very least $p > n$, and probably >>\n    - \"We ask when it is possible to fit the data exactly and still compete with the prediction accuracy of $\\theta^*$.\"\n        - ~~Unless I'm being stupid, this line doesn't make any sense. If you fit the data exactly, then by definition you're an optimal solution~~\n    - the solution that minimises is underdetermined, so there isn't a unique solution to the convex program\n    - okay, so I think what they're trying to say is that, if your choice of $\\hat{\\theta}$ is the one that minimises the quadratic loss (= 0), and then pick the one with minimum norm (what norm), then let's see when do you get good generalizability\n    - \"We ask when it is possible to overfit in this way – and embed all of the noise of the labels into the parameter estimate $\\hat{\\theta}$ – without harming prediction accuracy\"\n- results:\n    - covariance matrix $\\Sigma$ plays a crucial role\n    - prediction error has following decomposition (two terms):\n        - provided nuclear norm of $\\Sigma$ is small compared to $n$, then $\\hat{\\theta}$ can find $\\theta^*$\n        - impact of noise in the labels on prediction accuracy <mark>(important)</mark>\n            - \"this is small iff the effective rank of $\\Sigma$ in the subspace corresponding to the low variance directions is large compared to $n$\"\n    - \"fitting the training data exactly but with near-optimal prediction accuracy occurs if and only if there are many low variance (and hence unimportant) directions in parameter space where the label noise can be hidden\"\n- intuition: I don't think they've provided much in the way of any intuition about what is going on, so let's see if we can come up with some\n    - this is weird because we're thinking of linear regression, and so basically it's a linear combination of the $X$'s to get $Y$.\n    - if we were to think about this geometrically, is that we have an infinite dimensional $C(X)$, with sufficient \"range\"/span that our $Y$ lands in $C(X)$.\n    - <mark>I feel like what they're trying to do is basically have the main true linear model, and then what you have are these small vectors in all sorts of directions, so that you can just add those to your solution, in order to interpolate (??)</mark>\n        - and since we're dealing with a model that is linear in truth, then\n        - I'm confused, because since both $x,y$ are random, then calculating the risk is over both of these things.\n- takeaway:\n    - basically, the eigenvalues of the covariance matrix should satisfy:\n        - many non-zero entries, large compared to $n$\n        - small sum compared to $n$ (smallest eigenvalues decays slowly)\n        - i.e. lots of small ones, probably some big ones (?)\n- summary: this is what I think is going on\n    - let's just start with $X$ and it's covariance matrix $\\Sigma$: what we have here is something, at the population level, that can be described by a few large eigenvalues, and then many, many small eigenvalues. The idea is that the $\\Sigma$ describes the distribution of the sampled $x$ vectors, and so the eigenvalues determine the variability in the direction of the respective eigenvector. However, there is no guarantee that there needs to be a rotation, so it is entirely possible that the variability in the $x$'s are concentrated on individual coordinates.\n        - ~~I incorrectly thought that what we have are random vectors, and so this spans all the coordinates, which makes it easier to extract the coefficients~~\n    - So what we need are very many small directions of $x$ (again, you can just align them to a coordinate)...\n    - The optimal thing would be this nice sort of decomposition, where the first few elements of $\\beta$ correspond to the true coefficients. Then, the point is that the error can be entirely captured by a linear combination of the rest of the covariates.\n    - ![](img/benign_1.png)\n    - This is a more explicit decomposition of the two terms\n    - ![](img/benign_2.png)\n    - The idea here is that, in some simplified sense, the error is just a random Gaussian vector, and we should be able to represent it exactly as a linear combination of a lot of small gaussian vectors.","excerpt":"Benign Overfitting in Linear Regression src:  @Bartlett:2020ck . references: slides , better exposition new paper that generalises this: \"B…","outboundReferences":[],"inboundReferences":[{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"tags\": [\"mathematics\", \"gradient_descent\"]\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"pseudo-inverses-and-sgd\"\n  }, \"Pseudo-inverses and SGD\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"The Moore-Penrose pseudo inverse (which is often just called the pseudoinverse) is just a choice of multiple solutions that leads to a minimum norm solution\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mover\", {\n    parentName: \"mrow\",\n    \"accent\": \"true\"\n  }, mdx(\"mi\", {\n    parentName: \"mover\"\n  }, \"\\u03B2\"), mdx(\"mo\", {\n    parentName: \"mover\"\n  }, \"^\")), mdx(\"mo\", {\n    parentName: \"mrow\"\n  }, \"=\"), mdx(\"msup\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msup\"\n  }, \"X\"), mdx(\"mo\", {\n    parentName: \"msup\"\n  }, \"+\")), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"Y\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\hat{\\\\beta} = X^+ Y\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1.1523199999999998em\",\n      \"verticalAlign\": \"-0.19444em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord accent\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.9578799999999998em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.05278em\"\n    }\n  }, \"\\u03B2\"))), mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.26344em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"3em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"accent-body\",\n    \"style\": {\n      \"left\": \"-0.16666em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"^\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.19444em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  }))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mrel\"\n  }, \"=\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.771331em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07847em\"\n    }\n  }, \"X\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.771331em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-3.063em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mbin mtight\"\n  }, \"+\")))))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.22222em\"\n    }\n  }, \"Y\")))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"An obvious consequence, is that ridge regression is not the same as the pseudoinverse solution, even though they're obviously very similar\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"my feeling is that, for a particular choice of penalty, the solution should be same (since you should be able to reframe the optimization into\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"min\"), mdx(\"mo\", {\n    parentName: \"mrow\"\n  }, \"\\u2061\"), mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u2223\"), mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u2223\"), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"\\u03B2\"), mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u2223\"), mdx(\"msub\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msub\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u2223\"), mdx(\"mn\", {\n    parentName: \"msub\"\n  }, \"2\")), mdx(\"mtext\", {\n    parentName: \"mrow\"\n  }, \"\\xA0s.t.\\xA0\"), mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u2223\"), mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u2223\"), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"Y\"), mdx(\"mo\", {\n    parentName: \"mrow\"\n  }, \"\\u2212\"), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"X\"), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"\\u03B2\"), mdx(\"mi\", {\n    parentName: \"mrow\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u2223\"), mdx(\"msub\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msub\",\n    \"mathvariant\": \"normal\"\n  }, \"\\u2223\"), mdx(\"mn\", {\n    parentName: \"msub\"\n  }, \"2\")), mdx(\"mo\", {\n    parentName: \"mrow\"\n  }, \"\\u2264\"), mdx(\"mi\", {\n    parentName: \"mrow\"\n  }, \"t\")), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"\\\\min ||\\\\beta||_2 \\\\text{ s.t. } ||Y - X \\\\beta ||_2 \\\\leq t\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1em\",\n      \"verticalAlign\": \"-0.25em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mop\"\n  }, \"min\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.16666666666666666em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u2223\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u2223\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.05278em\"\n    }\n  }, \"\\u03B2\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u2223\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u2223\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.30110799999999993em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.5500000000000003em\",\n      \"marginLeft\": \"0em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"2\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.15em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  })))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord text\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\xA0s.t.\\xA0\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u2223\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u2223\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.22222em\"\n    }\n  }, \"Y\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2222222222222222em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mbin\"\n  }, \"\\u2212\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2222222222222222em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"1em\",\n      \"verticalAlign\": \"-0.25em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.07847em\"\n    }\n  }, \"X\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.05278em\"\n    }\n  }, \"\\u03B2\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u2223\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, \"\\u2223\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.30110799999999993em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.5500000000000003em\",\n      \"marginLeft\": \"0em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"2\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.15em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  })))))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mrel\"\n  }, \"\\u2264\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mspace\",\n    \"style\": {\n      \"marginRight\": \"0.2777777777777778em\"\n    }\n  })), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.61508em\",\n      \"verticalAlign\": \"0em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\"\n  }, \"t\")))))))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"part of me is a little confused: if you can just define this pseudoinverse, then how come we don't use this estimator more often? clearly it must not have good properties?\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"thanks to this \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://twitter.com/beenwrekt/status/1041724607911227392\"\n  }, \"tweet\"), \", realize there's a section in \", \"(Zhang et al., 2019)\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-Zhang:2016ve\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-Zhang:2016ve\",\n    \"className\": \"footnote-ref\"\n  }, \"Zhang:2016ve\")), \" that relates to \", \"(Bartlett et al., 2020)\", mdx(\"sup\", {\n    parentName: \"p\",\n    \"id\": \"fnref-Bartlett:2020ck\"\n  }, mdx(\"a\", {\n    parentName: \"sup\",\n    \"href\": \"#fn-Bartlett:2020ck\",\n    \"className\": \"footnote-ref\"\n  }, \"Bartlett:2020ck\")), \"[benign-overfitting-in-linear-regression]\", \"], in that they show that the SGD solution is the same as the minimum-norm (pseudo-inverse)\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"the curvature of the minimum (LS) solutions don't actually tell you anything (they're the same)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\\\"Unfortunately, this notion of minimum norm is not predictive of generalization performance. For example, returning to the MNIST example, the \", mdx(\"span\", {\n    parentName: \"li\",\n    \"className\": \"math math-inline\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-mathml\"\n  }, mdx(\"math\", {\n    parentName: \"span\",\n    \"xmlns\": \"http://www.w3.org/1998/Math/MathML\"\n  }, mdx(\"semantics\", {\n    parentName: \"math\"\n  }, mdx(\"mrow\", {\n    parentName: \"semantics\"\n  }, mdx(\"msub\", {\n    parentName: \"mrow\"\n  }, mdx(\"mi\", {\n    parentName: \"msub\"\n  }, \"l\"), mdx(\"mn\", {\n    parentName: \"msub\"\n  }, \"2\"))), mdx(\"annotation\", {\n    parentName: \"semantics\",\n    \"encoding\": \"application/x-tex\"\n  }, \"l_2\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"katex-html\",\n    \"aria-hidden\": \"true\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"base\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"strut\",\n    \"style\": {\n      \"height\": \"0.84444em\",\n      \"verticalAlign\": \"-0.15em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mathnormal\",\n    \"style\": {\n      \"marginRight\": \"0.01968em\"\n    }\n  }, \"l\"), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"msupsub\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-t vlist-t2\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.30110799999999993em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"style\": {\n      \"top\": \"-2.5500000000000003em\",\n      \"marginLeft\": \"-0.01968em\",\n      \"marginRight\": \"0.05em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"pstrut\",\n    \"style\": {\n      \"height\": \"2.7em\"\n    }\n  }), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"sizing reset-size6 size3 mtight\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"mord mtight\"\n  }, \"2\")))), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-s\"\n  }, \"\\u200B\")), mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist-r\"\n  }, mdx(\"span\", {\n    parentName: \"span\",\n    \"className\": \"vlist\",\n    \"style\": {\n      \"height\": \"0.15em\"\n    }\n  }, mdx(\"span\", {\n    parentName: \"span\"\n  })))))))))), \"-norm of the minimum norm solution with no preprocessing is approximately 220. With wavelet preprocessing, the norm jumps to 390. Yet the test error drops by a factor of 2. So while this minimum-norm intuition may provide some guidance to new algorithm design, it is only a very small piece of the generalization story.\\\"\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"but this is changing the data, so I don't think this comparison really matters \\u2013 it's not saying across all kinds of models, we should be minimizing the norm. it's just saying that we prefer models with minimum norm\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"interesting that this works well for MNIST/CIFAR10\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"there must be something in all of this: on page 29 of \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.stat.berkeley.edu/~bartlett/talks/201908Microsoft.pdf\"\n  }, \"slides\"), \", they show that SGD converges to min-norm interpolating solution with respect to a certain kernel (so the norm is on the coefficients for each kernel)\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"as pointed out, this is very different to the benign paper, as this result is data-independent (it's just a feature of SGD)\")))))), mdx(\"div\", {\n    \"className\": \"footnotes\"\n  }, mdx(\"hr\", {\n    parentName: \"div\"\n  }), mdx(\"ol\", {\n    parentName: \"div\"\n  }, mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-Zhang:2016ve\"\n  }, \"Zhang, C. et al., 2019. Understanding deep learning requires rethinking generalization. In 5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings. University of California, Berkeley, Berkeley, United States.\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-Zhang:2016ve\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")), mdx(\"li\", {\n    parentName: \"ol\",\n    \"id\": \"fn-Bartlett:2020ck\"\n  }, \"Bartlett, P.L. et al., 2020. Benign overfitting in linear regression. Proceedings of the National Academy of Sciences of the United States of America, 80, p.201907378.\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"#fnref-Bartlett:2020ck\",\n    \"className\": \"footnote-backref\"\n  }, \"\\u21A9\")))));\n}\n;\nMDXContent.isMDXComponent = true;","fields":{"slug":"/pseudo-inverses-and-sgd/","title":"Pseudo-inverses and SGD"}}]},"tagsOutbound":{"nodes":[{"frontmatter":{"title":"","tags":["self_supervised_learning","machine_learning","from_paper"]},"fields":{"slug":"/MoCo/","title":"Momentum Contrast for Unsupervised Visual Representation Learning","lastUpdated":"4/9/2022","lastUpdatedAt":"2022-04-09T10:57:18.000Z","gitCreatedAt":"2022-04-08T15:16:16.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/a-universal-law-of-robustness-via-isoperimetry/","title":"A Universal Law of Robustness via Isoperimetry","lastUpdated":"4/16/2022","lastUpdatedAt":"2022-04-16T07:10:21.000Z","gitCreatedAt":"2022-04-15T21:18:03.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","lit_review","interpolation"]},"fields":{"slug":"/benign-overfitting-in-linear-regression/","title":"Benign Overfitting in Linear Regression","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/can-you-learn-an-algorithm/","title":"Can You Learn an Algorithm","lastUpdated":"3/24/2022","lastUpdatedAt":"2022-03-24T17:13:26.000Z","gitCreatedAt":"2021-12-23T11:27:19.000Z"}},{"frontmatter":{"title":"","tags":["interpolation","statistics","machine_learning"]},"fields":{"slug":"/classification-vs-regression/","title":"Classification vs Regression","lastUpdated":"4/6/2022","lastUpdatedAt":"2022-04-06T00:52:19.000Z","gitCreatedAt":"2022-02-12T11:03:50.000Z"}},{"frontmatter":{"title":"","tags":["machine_learning","from_paper"]},"fields":{"slug":"/dataset-bias/","title":"Dataset Bias","lastUpdated":"4/16/2022","lastUpdatedAt":"2022-04-16T07:10:21.000Z","gitCreatedAt":"2022-04-15T21:18:03.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","machine_learning","neural_tangent_kernel"]},"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Distillation","lastUpdated":"2/26/2022","lastUpdatedAt":"2022-02-26T13:00:00.000Z","gitCreatedAt":"2021-12-25T21:42:34.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/embeddings-cant-possibly-be-right/","title":"Embeddings Can't Possibly Be Right","lastUpdated":"2/13/2022","lastUpdatedAt":"2022-02-13T17:46:16.000Z","gitCreatedAt":"2022-02-13T10:27:23.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","lit_review","gradient_descent","optimisation","regularization"]},"fields":{"slug":"/exponential-learning-rates/","title":"Exponential Learning Rates","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","computer_vision","attention","machine_learning"]},"fields":{"slug":"/how-do-vision-transformers-work/","title":"How Do Vision Transformers Work","lastUpdated":"3/11/2022","lastUpdatedAt":"2022-03-11T09:42:14.000Z","gitCreatedAt":"2022-03-11T09:08:13.000Z"}},{"frontmatter":{"title":"","tags":["interpolation","idea"]},"fields":{"slug":"/ideas-around-interpolation/","title":"Ideas Around Interpolation","lastUpdated":"2/26/2022","lastUpdatedAt":"2022-02-26T16:35:48.000Z","gitCreatedAt":"2022-02-12T07:57:19.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","nlp","lit_review","neuroscience"]},"fields":{"slug":"/learning-as-the-unsupervised-alignment-of-conceptual-systems/","title":"Learning as the Unsupervised Alignment of Conceptual Systems","lastUpdated":"4/16/2022","lastUpdatedAt":"2022-04-16T07:10:21.000Z","gitCreatedAt":"2022-04-15T21:18:03.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","causal_inference"]},"fields":{"slug":"/learning-dags/","title":"Learning DAGs","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}},{"frontmatter":{"title":"","tags":["self_supervised_learning","machine_learning","from_paper"]},"fields":{"slug":"/multiMAE/","title":"Multi MAE","lastUpdated":"4/8/2022","lastUpdatedAt":"2022-04-08T15:16:16.000Z","gitCreatedAt":"2022-04-08T15:16:16.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","psychology"]},"fields":{"slug":"/multidimensional-mental-representations-of-natural-objects/","title":"Multidimensional Mental Representations of Natural Objects","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2021-12-24T11:18:15.000Z"}},{"frontmatter":{"title":"","tags":["neural_networks","from_paper"]},"fields":{"slug":"/neural-representations/","title":"Neural Representations","lastUpdated":"4/6/2022","lastUpdatedAt":"2022-04-06T00:52:19.000Z","gitCreatedAt":"2022-02-11T17:01:17.000Z"}},{"frontmatter":{"title":"","tags":["lit_review","networks"]},"fields":{"slug":"/noisy-networks/","title":"Noisy Networks","lastUpdated":"4/16/2022","lastUpdatedAt":"2022-04-16T07:10:21.000Z","gitCreatedAt":"2022-04-15T21:18:03.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/non-deep-networks/","title":"Non-deep Networks","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2021-12-23T11:27:19.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","overparameterization"]},"fields":{"slug":"/overparameterized-regression/","title":"Overparameterized Regression","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/spectral-bias/","title":"On the Spectral Bias of Neural Networks","lastUpdated":"4/17/2022","lastUpdatedAt":"2022-04-17T20:39:26.000Z","gitCreatedAt":"2022-04-15T08:46:07.000Z"}},{"frontmatter":{"title":"","tags":["from_paper"]},"fields":{"slug":"/stand-alone-self-attention-in-vision-models/","title":"Stand-Alone Self-Attention in Vision Models","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2021-12-23T11:27:19.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","society","research"]},"fields":{"slug":"/stewardship-of-global-collective-beahvior/","title":"Stewardship of Global Collective Behaviour","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-04T21:27:53.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","statistics","research","classic_papers"]},"fields":{"slug":"/two-cultures/","title":"Two Cultures","lastUpdated":"1/12/2022","lastUpdatedAt":"2022-01-12T12:14:44.000Z","gitCreatedAt":"2022-01-12T12:14:44.000Z"}},{"frontmatter":{"title":"","tags":["research","nlp","from_paper"]},"fields":{"slug":"/unsupervised-language-translation/","title":"Unsupervised Language Translation","lastUpdated":"4/16/2022","lastUpdatedAt":"2022-04-16T07:10:21.000Z","gitCreatedAt":"2022-04-15T20:31:09.000Z"}},{"frontmatter":{"title":"","tags":["from_paper","machine_learning","computer_vision","attention"]},"fields":{"slug":"/vision-transformers/","title":"Vision Transformers","lastUpdated":"1/29/2022","lastUpdatedAt":"2022-01-29T14:49:40.000Z","gitCreatedAt":"2022-01-16T10:24:03.000Z"}}]}},"pageContext":{"tags":["from_paper","lit_review","interpolation"],"slug":"/benign-overfitting-in-linear-regression/","sidebarItems":[{"title":"","items":[{"title":"Recently Updated","url":"/latest/","collapse":true,"indent":false,"items":[{"title":"07-07: Text Recognition Literature Review","url":"/text-recognition-review/"},{"title":"07-06: Text Recognition","url":"/text-recognition/"},{"title":"07-04: Causality in ML","url":"/causality/"},{"title":"07-04: CNNs","url":"/convolutional-neural-networks/"},{"title":"06-27: Generalisation Literature Review","url":"/generalisation-literature-review/"},{"title":"05-20: Multi-Modal Learning","url":"/multi-modal learning/"},{"title":"05-20: Self-supervised Learning","url":"/self-supervised-learning/"},{"title":"05-10: Generalisation","url":"/generalisation/"},{"title":"05-06: Generalisation from Batch","url":"/generalisation-from-batch/"},{"title":"05-04: Fourier Features: A Curious Lens into Deep Learning","url":"/neural-paper/"}]}]},{"title":"Tags","items":[{"title":"MLOps","type":"tag","url":"/tags/ml-ops/","items":[{"title":"Machine Learning APIs","url":"/machine-learning-apis/"}]},{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"Artificial Generalised Intelligence","url":"/artificial-generalised-intelligence/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"attention","type":"tag","url":"/tags/attention/","items":[{"title":"How Do Vision Transformers Work","url":"/how-do-vision-transformers-work/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"biologically_inspired","type":"tag","url":"/tags/biologically-inspired/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"},{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"causal_inference","type":"tag","url":"/tags/causal-inference/","items":[{"title":"Learning DAGs","url":"/learning-dags/"}]},{"title":"classic_papers","type":"tag","url":"/tags/classic-papers/","items":[{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"commentary","type":"tag","url":"/tags/commentary/","items":[{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"computer_vision","type":"tag","url":"/tags/computer-vision/","items":[{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"How Do Vision Transformers Work","url":"/how-do-vision-transformers-work/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"deep_learning","type":"tag","url":"/tags/deep-learning/","items":[{"title":"DALL-E 2","url":"/DALL-E2/"},{"title":"GPT-3","url":"/gpt3/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Efficient Markets and Data","url":"/efficient-markets-and-data/"},{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Bitter Lesson","url":"/bitter-lesson/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Next Steps for Deep Learning","url":"/next-steps-for-deep-learning/"}]},{"title":"from_paper","type":"tag","url":"/tags/from-paper/","items":[{"title":"Momentum Contrast for Unsupervised Visual Representation Learning","url":"/MoCo/"},{"title":"A Universal Law of Robustness via Isoperimetry","url":"/a-universal-law-of-robustness-via-isoperimetry/"},{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Bias","url":"/dataset-bias/"},{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Embeddings Can't Possibly Be Right","url":"/embeddings-cant-possibly-be-right/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"How Do Vision Transformers Work","url":"/how-do-vision-transformers-work/"},{"title":"Learning as the Unsupervised Alignment of Conceptual Systems","url":"/learning-as-the-unsupervised-alignment-of-conceptual-systems/"},{"title":"Learning DAGs","url":"/learning-dags/"},{"title":"Multi MAE","url":"/multiMAE/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Neural Representations","url":"/neural-representations/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"On the Spectral Bias of Neural Networks","url":"/spectral-bias/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"},{"title":"Unsupervised Language Translation","url":"/unsupervised-language-translation/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"Slack Bot","url":"/slack-bot/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"Ideas Around Interpolation","url":"/ideas-around-interpolation/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"implicit_regularisation","type":"tag","url":"/tags/implicit-regularisation/","items":[{"title":"Implicit Regularisation","url":"/implicit-regularisation/"}]},{"title":"interpolation","type":"tag","url":"/tags/interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Ideas Around Interpolation","url":"/ideas-around-interpolation/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Learning as the Unsupervised Alignment of Conceptual Systems","url":"/learning-as-the-unsupervised-alignment-of-conceptual-systems/"},{"title":"Noisy Networks","url":"/noisy-networks/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"DALL-E 2","url":"/DALL-E2/"},{"title":"Momentum Contrast for Unsupervised Visual Representation Learning","url":"/MoCo/"},{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Dataset Bias","url":"/dataset-bias/"},{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Explainable Trees","url":"/explainable-trees/"},{"title":"From NERF to Kernel Regression","url":"/from-nerf-to-kernel-regression/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"How Do Vision Transformers Work","url":"/how-do-vision-transformers-work/"},{"title":"Invariant Risk Minimisation","url":"/invariant-risk-minimisation/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Machine Learning APIs","url":"/machine-learning-apis/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"Multi MAE","url":"/multiMAE/"},{"title":"No Free Lunch","url":"/no-free-lunch/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"Self-supervised Learning","url":"/self-supervised-learning/"},{"title":"System 1 and 2","url":"/system-1-and-2/"},{"title":"Transformers","url":"/transformers/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"matrix_completion","type":"tag","url":"/tags/matrix-completion/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Efficient Markets and Data","url":"/efficient-markets-and-data/"},{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"networks","type":"tag","url":"/tags/networks/","items":[{"title":"Dynamic Graph Models","url":"/dynamic-graph-models/"},{"title":"Noisy Networks","url":"/noisy-networks/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Fast Weights","url":"/fast-weights/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Neural Representations","url":"/neural-representations/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"From NERF to Kernel Regression","url":"/from-nerf-to-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"neuroscience","type":"tag","url":"/tags/neuroscience/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"},{"title":"Learning as the Unsupervised Alignment of Conceptual Systems","url":"/learning-as-the-unsupervised-alignment-of-conceptual-systems/"}]},{"title":"nlp","type":"tag","url":"/tags/nlp/","items":[{"title":"Debiasing Word Embeddings","url":"/debiasing-word-embeddings/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"GPT-3","url":"/gpt3/"},{"title":"Learning as the Unsupervised Alignment of Conceptual Systems","url":"/learning-as-the-unsupervised-alignment-of-conceptual-systems/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"},{"title":"Unsupervised Language Translation","url":"/unsupervised-language-translation/"}]},{"title":"optimisation","type":"tag","url":"/tags/optimisation/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"regularization","type":"tag","url":"/tags/regularization/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"reinforcement_learning","type":"tag","url":"/tags/reinforcement-learning/","items":[{"title":"World Models","url":"/world-models/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Dynamic Graph Models","url":"/dynamic-graph-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"},{"title":"Unsupervised Language Translation","url":"/unsupervised-language-translation/"}]},{"title":"self_supervised_learning","type":"tag","url":"/tags/self-supervised-learning/","items":[{"title":"Momentum Contrast for Unsupervised Visual Representation Learning","url":"/MoCo/"},{"title":"Multi MAE","url":"/multiMAE/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Alone","url":"/alone/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Dynamic Graph Models","url":"/dynamic-graph-models/"},{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"theoretical_statistics","type":"tag","url":"/tags/theoretical-statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"Explainable Trees","url":"/explainable-trees/"}]}]}],"tagsGroups":[{"title":"MLOps","type":"tag","url":"/tags/ml-ops/","items":[{"title":"Machine Learning APIs","url":"/machine-learning-apis/"}]},{"title":"artificial_intelligence","type":"tag","url":"/tags/artificial-intelligence/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"Artificial Generalised Intelligence","url":"/artificial-generalised-intelligence/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"attention","type":"tag","url":"/tags/attention/","items":[{"title":"How Do Vision Transformers Work","url":"/how-do-vision-transformers-work/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"biologically_inspired","type":"tag","url":"/tags/biologically-inspired/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"},{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"Fast Weights","url":"/fast-weights/"}]},{"title":"causal_inference","type":"tag","url":"/tags/causal-inference/","items":[{"title":"Learning DAGs","url":"/learning-dags/"}]},{"title":"classic_papers","type":"tag","url":"/tags/classic-papers/","items":[{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"commentary","type":"tag","url":"/tags/commentary/","items":[{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"computer_science","type":"tag","url":"/tags/computer-science/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"computer_vision","type":"tag","url":"/tags/computer-vision/","items":[{"title":"Computer Vision Tasks","url":"/computer-vision-tasks/"},{"title":"How Do Vision Transformers Work","url":"/how-do-vision-transformers-work/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"data_science","type":"tag","url":"/tags/data-science/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"deep_learning","type":"tag","url":"/tags/deep-learning/","items":[{"title":"DALL-E 2","url":"/DALL-E2/"},{"title":"GPT-3","url":"/gpt3/"}]},{"title":"economics","type":"tag","url":"/tags/economics/","items":[{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"finance","type":"tag","url":"/tags/finance/","items":[{"title":"Efficient Markets and Data","url":"/efficient-markets-and-data/"},{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"}]},{"title":"from_article","type":"tag","url":"/tags/from-article/","items":[{"title":"Bitter Lesson","url":"/bitter-lesson/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Market for Lemons","url":"/market-for-lemons/"},{"title":"Next Steps for Deep Learning","url":"/next-steps-for-deep-learning/"}]},{"title":"from_paper","type":"tag","url":"/tags/from-paper/","items":[{"title":"Momentum Contrast for Unsupervised Visual Representation Learning","url":"/MoCo/"},{"title":"A Universal Law of Robustness via Isoperimetry","url":"/a-universal-law-of-robustness-via-isoperimetry/"},{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Can You Learn an Algorithm","url":"/can-you-learn-an-algorithm/"},{"title":"Dataset Bias","url":"/dataset-bias/"},{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Embeddings Can't Possibly Be Right","url":"/embeddings-cant-possibly-be-right/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"How Do Vision Transformers Work","url":"/how-do-vision-transformers-work/"},{"title":"Learning as the Unsupervised Alignment of Conceptual Systems","url":"/learning-as-the-unsupervised-alignment-of-conceptual-systems/"},{"title":"Learning DAGs","url":"/learning-dags/"},{"title":"Multi MAE","url":"/multiMAE/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"Neural Representations","url":"/neural-representations/"},{"title":"Non-deep Networks","url":"/non-deep-networks/"},{"title":"Overparameterized Regression","url":"/overparameterized-regression/"},{"title":"On the Spectral Bias of Neural Networks","url":"/spectral-bias/"},{"title":"Stand-Alone Self-Attention in Vision Models","url":"/stand-alone-self-attention-in-vision-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"},{"title":"Unsupervised Language Translation","url":"/unsupervised-language-translation/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"from_podcast","type":"tag","url":"/tags/from-podcast/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"from_talk","type":"tag","url":"/tags/from-talk/","items":[{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"}]},{"title":"gradient_descent","type":"tag","url":"/tags/gradient-descent/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"graph_neural_networks","type":"tag","url":"/tags/graph-neural-networks/","items":[{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"}]},{"title":"gui","type":"tag","url":"/tags/gui/","items":[{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"Slack Bot","url":"/slack-bot/"}]},{"title":"idea","type":"tag","url":"/tags/idea/","items":[{"title":"Ideas Around Interpolation","url":"/ideas-around-interpolation/"},{"title":"One Stock to Rule Them All","url":"/one-stock-to-rule-them-all/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"}]},{"title":"implicit_regularisation","type":"tag","url":"/tags/implicit-regularisation/","items":[{"title":"Implicit Regularisation","url":"/implicit-regularisation/"}]},{"title":"interpolation","type":"tag","url":"/tags/interpolation/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Ideas Around Interpolation","url":"/ideas-around-interpolation/"}]},{"title":"interview","type":"tag","url":"/tags/interview/","items":[{"title":"Precision/Recall","url":"/precision-recall/"}]},{"title":"lit_review","type":"tag","url":"/tags/lit-review/","items":[{"title":"Benign Overfitting in Linear Regression","url":"/benign-overfitting-in-linear-regression/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"Learning as the Unsupervised Alignment of Conceptual Systems","url":"/learning-as-the-unsupervised-alignment-of-conceptual-systems/"},{"title":"Noisy Networks","url":"/noisy-networks/"}]},{"title":"machine_learning","type":"tag","url":"/tags/machine-learning/","items":[{"title":"DALL-E 2","url":"/DALL-E2/"},{"title":"Momentum Contrast for Unsupervised Visual Representation Learning","url":"/MoCo/"},{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Dataset Bias","url":"/dataset-bias/"},{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"Explainable Trees","url":"/explainable-trees/"},{"title":"From NERF to Kernel Regression","url":"/from-nerf-to-kernel-regression/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"How Do Vision Transformers Work","url":"/how-do-vision-transformers-work/"},{"title":"Invariant Risk Minimisation","url":"/invariant-risk-minimisation/"},{"title":"Knowledge Distillation","url":"/knowledge-distillation/"},{"title":"Machine Learning APIs","url":"/machine-learning-apis/"},{"title":"Meta Learning","url":"/meta-learning/"},{"title":"Multi MAE","url":"/multiMAE/"},{"title":"No Free Lunch","url":"/no-free-lunch/"},{"title":"Object Detection for GUI","url":"/objection-detection-for-guis/"},{"title":"Self-supervised Learning","url":"/self-supervised-learning/"},{"title":"System 1 and 2","url":"/system-1-and-2/"},{"title":"Transformers","url":"/transformers/"},{"title":"Vision Transformers","url":"/vision-transformers/"}]},{"title":"mathematics","type":"tag","url":"/tags/mathematics/","items":[{"title":"Pseudo-inverses and SGD","url":"/pseudo-inverses-and-sgd/"}]},{"title":"matrix_completion","type":"tag","url":"/tags/matrix-completion/","items":[{"title":"Effectiveness of Normalised Quantities","url":"/effectiveness-of-normalised-quantities/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"}]},{"title":"medicine","type":"tag","url":"/tags/medicine/","items":[{"title":"AI for Health","url":"/ai-for-health/"}]},{"title":"meta_analysis","type":"tag","url":"/tags/meta-analysis/","items":[{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"}]},{"title":"meta_learning","type":"tag","url":"/tags/meta-learning/","items":[{"title":"Meta Learning","url":"/meta-learning/"}]},{"title":"money_stuff","type":"tag","url":"/tags/money-stuff/","items":[{"title":"Efficient Markets and Data","url":"/efficient-markets-and-data/"},{"title":"Monopoly in Tech","url":"/monopoly-in-tech/"}]},{"title":"networks","type":"tag","url":"/tags/networks/","items":[{"title":"Dynamic Graph Models","url":"/dynamic-graph-models/"},{"title":"Noisy Networks","url":"/noisy-networks/"}]},{"title":"neural_networks","type":"tag","url":"/tags/neural-networks/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Fast Weights","url":"/fast-weights/"},{"title":"Generalized Neural Networks","url":"/generalised-neural-networks/"},{"title":"Graph Network as Arbitrary Inductive Bias","url":"/graph-network-as-arbitrary-inductive-bias/"},{"title":"Neural Representations","url":"/neural-representations/"},{"title":"Transformers","url":"/transformers/"}]},{"title":"neural_tangent_kernel","type":"tag","url":"/tags/neural-tangent-kernel/","items":[{"title":"Dataset Distillation","url":"/dataset-meta-learning-from-kernel-regression/"},{"title":"From NERF to Kernel Regression","url":"/from-nerf-to-kernel-regression/"}]},{"title":"neurips","type":"tag","url":"/tags/neurips/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"}]},{"title":"neuroscience","type":"tag","url":"/tags/neuroscience/","items":[{"title":"Calculus For Brain Computation","url":"/calculus-for-brain-computation/"},{"title":"Learning as the Unsupervised Alignment of Conceptual Systems","url":"/learning-as-the-unsupervised-alignment-of-conceptual-systems/"}]},{"title":"nlp","type":"tag","url":"/tags/nlp/","items":[{"title":"Debiasing Word Embeddings","url":"/debiasing-word-embeddings/"},{"title":"Explaining Word Embeddings","url":"/explaining-word-embeddings/"},{"title":"GPT-3","url":"/gpt3/"},{"title":"Learning as the Unsupervised Alignment of Conceptual Systems","url":"/learning-as-the-unsupervised-alignment-of-conceptual-systems/"},{"title":"Signed Word Embeddings","url":"/signed-word-embeddings/"},{"title":"Unsupervised Language Translation","url":"/unsupervised-language-translation/"}]},{"title":"optimisation","type":"tag","url":"/tags/optimisation/","items":[{"title":"Discretization of Gradient Flow","url":"/discretization-of-gradient-flow/"},{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"},{"title":"On Optimisation in Matrix Completion","url":"/on-optimisation-in-matrix-completion/"},{"title":"The Unreasonable Effectiveness of Adam","url":"/the-unreasonable-effectiveness-of-adam/"}]},{"title":"overparameterization","type":"tag","url":"/tags/overparameterization/","items":[{"title":"Overparameterized Regression","url":"/overparameterized-regression/"}]},{"title":"psychology","type":"tag","url":"/tags/psychology/","items":[{"title":"Perception","url":"/kahneman-neurips-perception/"},{"title":"Multidimensional Mental Representations of Natural Objects","url":"/multidimensional-mental-representations-of-natural-objects/"},{"title":"System 1 and 2","url":"/system-1-and-2/"}]},{"title":"regularization","type":"tag","url":"/tags/regularization/","items":[{"title":"Exponential Learning Rates","url":"/exponential-learning-rates/"}]},{"title":"reinforcement_learning","type":"tag","url":"/tags/reinforcement-learning/","items":[{"title":"World Models","url":"/world-models/"}]},{"title":"research","type":"tag","url":"/tags/research/","items":[{"title":"Dynamic Graph Models","url":"/dynamic-graph-models/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"},{"title":"Two Cultures","url":"/two-cultures/"},{"title":"Unsupervised Language Translation","url":"/unsupervised-language-translation/"}]},{"title":"self_supervised_learning","type":"tag","url":"/tags/self-supervised-learning/","items":[{"title":"Momentum Contrast for Unsupervised Visual Representation Learning","url":"/MoCo/"},{"title":"Multi MAE","url":"/multiMAE/"}]},{"title":"society","type":"tag","url":"/tags/society/","items":[{"title":"Alone","url":"/alone/"},{"title":"Stewardship of Global Collective Behaviour","url":"/stewardship-of-global-collective-beahvior/"}]},{"title":"statistics","type":"tag","url":"/tags/statistics/","items":[{"title":"Classification vs Regression","url":"/classification-vs-regression/"},{"title":"Dynamic Graph Models","url":"/dynamic-graph-models/"},{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Meta Analysis vs Preregistration","url":"/meta-analysis-vs-preregistration/"},{"title":"Michael Jordan Plenary Talk","url":"/michael-jordan-plenary-talk/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"},{"title":"Two Cultures","url":"/two-cultures/"}]},{"title":"theoretical_statistics","type":"tag","url":"/tags/theoretical-statistics/","items":[{"title":"Estimating the Mean","url":"/estimating-the-mean/"},{"title":"Theoretical Statistics: Beauty or Banality","url":"/theoretical-statistics-beauty-or-banality/"}]},{"title":"xAI","type":"tag","url":"/tags/x-ai/","items":[{"title":"AI for Health","url":"/ai-for-health/"},{"title":"Explainable Trees","url":"/explainable-trees/"}]}],"latestPosts":[{"fields":{"slug":"/text-recognition-review/","title":"Text Recognition Literature Review","lastUpdatedAt":"2022-07-07T10:04:19.000Z","lastUpdated":"7/7/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/text-recognition/","title":"Text Recognition","lastUpdatedAt":"2022-07-06T14:35:20.000Z","lastUpdated":"7/6/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/causality/","title":"Causality in ML","lastUpdatedAt":"2022-07-04T16:47:40.000Z","lastUpdated":"7/4/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/convolutional-neural-networks/","title":"CNNs","lastUpdatedAt":"2022-07-04T13:36:29.000Z","lastUpdated":"7/4/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/generalisation-literature-review/","title":"Generalisation Literature Review","lastUpdatedAt":"2022-06-27T15:06:07.000Z","lastUpdated":"6/27/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/multi-modal learning/","title":"Multi-Modal Learning","lastUpdatedAt":"2022-05-20T08:34:03.000Z","lastUpdated":"5/20/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/self-supervised-learning/","title":"Self-supervised Learning","lastUpdatedAt":"2022-05-20T08:21:07.000Z","lastUpdated":"5/20/2022"},"frontmatter":{"draft":false,"tags":["machine_learning"]}},{"fields":{"slug":"/generalisation/","title":"Generalisation","lastUpdatedAt":"2022-05-10T07:20:45.000Z","lastUpdated":"5/10/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/generalisation-from-batch/","title":"Generalisation from Batch","lastUpdatedAt":"2022-05-06T21:55:47.000Z","lastUpdated":"5/6/2022"},"frontmatter":{"draft":false,"tags":[]}},{"fields":{"slug":"/neural-paper/","title":"Fourier Features: A Curious Lens into Deep Learning","lastUpdatedAt":"2022-05-04T10:36:45.000Z","lastUpdated":"5/4/2022"},"frontmatter":{"draft":false,"tags":[]}}]}},
    "staticQueryHashes": ["2230547434","2320115945","3495835395","451533639"]}