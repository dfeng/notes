{"data":{"allMdx":{"nodes":[{"fields":{"slug":"/placeholder/","title":"This Is a Placeholder File for Mdx"},"frontmatter":{"draft":true},"rawBody":"---\ntitle: This Is a Placeholder File for Mdx\ndraft: true\ntags:\n  - gatsby-theme-primer-wiki-placeholder\n---\n"},{"fields":{"slug":"/ai-for-health/","title":"AI for Health"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - xAI\n - medicine\n - artificial_intelligence\n---\n\n# AI for Health\n\nsrc: [WHO](https://www.who.int/publications/i/item/9789240029200) guidance\n\n## Summary\n\nSix key (ethical) principles:\n\n1. *Human autonomy*: think of the humans! (also right to privacy)\n2. *Safety/well-being/public interest*: this is like the Asimov robot laws\n3. *Transparency/explainability/intelligibility*: self-explanatory\n4. *Responsibility/accountability*: I think this follows naturally from the previous point, but means there's \"points of human supervision\" (human-in-the-loop?)\n5. *Inclusiveness/equity*: fairness in a nutshell\n6. *Responsive/sustainable*: on-line, adaptive learning + sustainability w.r.t. the environment\n\nTo someone versed in the societal import of ML, I don't think there's too much in the way of surprises in this guidance document, though it does highlight a few things (worth repeating):\n\n - the differentiation between high and low-income countries, and the potentially widening gap in healthcare outcomes brought about by AI. while there's nothing inherently problematic about that, it does bring up the potential problem of a mismatch in the focus of problems (i.e. cardiovascular diseases and other lifestyle-based, chronic illnesses for high-income versus the more straightforward, brutal problems faced by low-income).\n - biased learning from data collected in the west is a key problem: we know very well that racial groups often have very different health outcomes for the same treatment\n - healthcare in other parts of the world are oftentimes much more holistic (i.e. Chinese Medicine?). how do we reconcile such traditions?\n - AI requires big data, which runs counter to fundamental privacy rights. this is where privacy-preserving measures will be key. on the other hand, the acquisition of such data in less scrupulous countries might be disastrous."},{"fields":{"slug":"/benign-overfitting-in-linear-regression/","title":"Benign Overfitting in Linear Regression"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - paper\n - lit_review\n - proj_interpolation\n---\n\n# Benign Overfitting in Linear Regression\n\nsrc: [@Bartlett:2020ck].\n\n- references:\n    - [slides](https://www.stat.berkeley.edu/~bartlett/talks/201908Microsoft.pdf), better exposition\n    - new paper that generalises this: \"Benign overfitting in the large deviation regime\", via [arXiv](https://arxiv.org/abs/2003.05838.pdf)\n- setting:\n    - linear regression (quadratic loss), linear prediction rule\n        - <mark>important</mark>: we are actually thinking about the ML regime, whereby there isn't just a *true* parameter, but that there is a *best* one by considering the expectation (which, under general settings, corresponds to what would be the \"truth\").\n        - <mark>important</mark>: they are considering random covariates in their analysis! I guess this is pretty standard practice, when you're calculating things like *risk*\n        - that is, you can think of linear regression as a linear approximation to $\\mathbb{E} ( Y_i \\,\\mid\\, X_i)$\n        - [blog](https://www.timlrx.com/2018/02/26/notes-on-regression-approximation-of-the-conditional-expectation-function/) with a good explanation: very standard classical exposition, showing that the conditional expectation is the optimal choice for $L_2$ loss, and the linear regression solutions is the best linear approximation to the conditional expectation (something like that)\n    - infinite dimensional data: $p = \\infty$ (separable Hilbert space)\n        - though results apply to finite-dimensional\n    - thus, there exists $\\theta^*$ that minimises the **expected** quadratic loss\n        - in order to be able to calculate expected value, they must be assuming some model (normal error)\n    - but they want to deal with the interpolating regime, so at the very least $p > n$, and probably >>\n    - \"We ask when it is possible to fit the data exactly and still compete with the prediction accuracy of $\\theta^*$.\"\n        - ~~Unless I'm being stupid, this line doesn't make any sense. If you fit the data exactly, then by definition you're an optimal solution~~\n    - the solution that minimises is underdetermined, so there isn't a unique solution to the convex program\n    - okay, so I think what they're trying to say is that, if your choice of $\\hat{\\theta}$ is the one that minimises the quadratic loss (= 0), and then pick the one with minimum norm (what norm), then let's see when do you get good generalizability\n    - \"We ask when it is possible to overfit in this way – and embed all of the noise of the labels into the parameter estimate $\\hat{\\theta}$ – without harming prediction accuracy\"\n- results:\n    - covariance matrix $\\Sigma$ plays a crucial role\n    - prediction error has following decomposition (two terms):\n        - provided nuclear norm of $\\Sigma$ is small compared to $n$, then $\\hat{\\theta}$ can find $\\theta^*$\n        - impact of noise in the labels on prediction accuracy <mark>(important)</mark>\n            - \"this is small iff the effective rank of $\\Sigma$ in the subspace corresponding to the low variance directions is large compared to $n$\"\n    - \"fitting the training data exactly but with near-optimal prediction accuracy occurs if and only if there are many low variance (and hence unimportant) directions in parameter space where the label noise can be hidden\"\n- intuition: I don't think they've provided much in the way of any intuition about what is going on, so let's see if we can come up with some\n    - this is weird because we're thinking of linear regression, and so basically it's a linear combination of the $X$'s to get $Y$.\n    - if we were to think about this geometrically, is that we have an infinite dimensional $C(X)$, with sufficient \"range\"/span that our $Y$ lands in $C(X)$.\n    - <mark>I feel like what they're trying to do is basically have the main true linear model, and then what you have are these small vectors in all sorts of directions, so that you can just add those to your solution, in order to interpolate (??)</mark>\n        - and since we're dealing with a model that is linear in truth, then\n        - I'm confused, because since both $x,y$ are random, then calculating the risk is over both of these things.\n- takeaway:\n    - basically, the eigenvalues of the covariance matrix should satisfy:\n        - many non-zero entries, large compared to $n$\n        - small sum compared to $n$ (smallest eigenvalues decays slowly)\n        - i.e. lots of small ones, probably some big ones (?)\n- summary: this is what I think is going on\n    - let's just start with $X$ and it's covariance matrix $\\Sigma$: what we have here is something, at the population level, that can be described by a few large eigenvalues, and then many, many small eigenvalues. The idea is that the $\\Sigma$ describes the distribution of the sampled $x$ vectors, and so the eigenvalues determine the variability in the direction of the respective eigenvector. However, there is no guarantee that there needs to be a rotation, so it is entirely possible that the variability in the $x$'s are concentrated on individual coordinates.\n        - ~~I incorrectly thought that what we have are random vectors, and so this spans all the coordinates, which makes it easier to extract the coefficients~~\n    - So what we need are very many small directions of $x$ (again, you can just align them to a coordinate)...\n    - The optimal thing would be this nice sort of decomposition, where the first few elements of $\\beta$ correspond to the true coefficients. Then, the point is that the error can be entirely captured by a linear combination of the rest of the covariates.\n    - ![](img/benign_1.png)\n    - This is a more explicit decomposition of the two terms\n    - ![](img/benign_2.png)\n    - The idea here is that, in some simplified sense, the error is just a random Gaussian vector, and we should be able to represent it exactly as a linear combination of a lot of small gaussian vectors."},{"fields":{"slug":"/can-you-learn-an-algorithm/","title":"Can You Learn an Algorithm"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - paper\n---\n\n# Can You Learn an Algorithm\n\n - [arXiv](https://arxiv.org/abs/2106.04537)"},{"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Meta-Learning From Kernel Ridge-Regression"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - paper\n  - machine_learning\n  - neural_tangent_kernel\n---\n\n# Dataset Meta-Learning From Kernel Ridge-Regression\n\n - src: [@nguyen_dataset_2021]\n\nYou have [[knowledge-distillation]], the goal of which is to compress models down while retaining the same performance. Less common is what can only be described as [[data-distillation]], the idea (?) of which is to compress datasets down in such a way that a particular algorithm trained on this compressed dataset would reach comparable test performance. ^[I feel like there's something interesting here for the psychologists. They're always looking to find the *archetypal* image for a class.]\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[knowledge-distillation]: knowledge-distillation.md \"Knowledge Distillation\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/discretization-of-gradient-flow/","title":"Discretization of Gradient Flow"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - neural_networks\n - optimization\n---\n\n# Discretization of Gradient Flow\n\nVia [[michael-jordan-plenary-talk]], to this paper on [arXiv](https://arxiv.org/pdf/1905.07436.pdf).\n\nYou have gradient descent, which you can show under convex problems to have a convergence rate of $\\frac{1}{t}$, whereas if you use Nesterov's accelerated gradient method gets $\\frac{1}{t^2}$.^[And this rate is entirely independent of the ambient dimension of the space.]\n\nIf you take the limit of the step sizes to zero, then you're going to get some kind of differential equation. This is *gradient flow*. It turns out you can basically construct a class of [Bregman Lagrangians](https://arxiv.org/abs/1603.04245), which essentially encapsulate all the gradient methods.\n\nYou can solve the Lagrangians for a particular rate, and then out pops an differential equation that obtains that rate in continuous time. What's curious is that the *path* is identical across all these ODEs. Essentially you're getting path-independence from the rate, which suggests that this method has found an *optimal* path, and you can essentially tweak how fast you want to go along that path.\n\nThis would suggest that you could then get arbritrary rates for your gradient method. But it turns out that the *discretization* step is where things break. In fact, Nesterov already has a lower bound on his rate of $\\frac{1}{t^2}$,^[The class of gradient methods are those that have access to all past gradients, I think.] so we know it can't do arbitrarily well. And it turns out that it does match the lower bound. The intuition is that the discretization suffers with curvature. If you go too quickly, then you're not going to be able to read the curvature well enough.\n\nIn other words, discretization is non-trivially different to continuous time. Which sort of makes sense, since in continuous time you have basically all the information.\n\nFinally, in relation to the [[penalty-project]], this doesn't actually work for things like Adam, which we know to be amazing in practice. So, it seems like there's still work left to understand why on earth the adaptive learning rates work so well.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[michael-jordan-plenary-talk]: michael-jordan-plenary-talk.md \"Michael Jordan Plenary Talk\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/graph-network-as-arbitrary-inductive-bias/","title":"Graph Network as Arbitrary Inductive Bias"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_article\n - neural_networks\n - graph_neural_networks\n---\n\n# Graph Network as Arbitrary Inductive Bias\n\n[src](https://blog.paperspace.com/introduction-to-geometric-deep-learning/).\n\nThe architecture of a neural network imposes some kind of structure that lends itself to particular types of problem (CNN, RNN). Thus, you can think of this as some form of *inductive bias*. An interesting view of [[graph-neural-networks]] is that essentially these provide arbitrary inductive bias, since the goal is to learn the architecture?\n\n| Component | Entities      | Relations  | Inductive Bias | Invariance      |\n|:----------|:--------------|:-----------|:---------------|:----------------|\n| FC        | Units         | All-to-all | Weak           | -               |\n| Conv.     | Grid elements | Local      | Locality       | Spatial transl. |\n| Recurrent | Time          | Sequential | Sequentially   | Time transl.    |\n| Graph     | Nodes         | Edges      | *Arbitrary*    | V,E permute     |\n\nFrom [@Battaglia:2018vi]\n\n(Though, is that really what GNNs really do?)"},{"fields":{"slug":"/kahneman-neurips-perception/","title":"Perception"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - neurips\n  - psychology\n---\n\n# Perception\n\n - top down vs bottom up (optical illusions)\n - same data, different representations given context\n - AlphaGo's structure has parallels with system-1,2 thinking\n   - the Monte Carlo tree search is system-2: has all the logic, slow, deliberate\n   - however, you can't just exhaustively search the tree, so you need a neural network to have gain some intuition/heuristics about where to go in the tree, and that's system-1\n - \n - GPT-3 ≠ system-1: heuristics are borne out of forming representations/models that are internally coherent\n   - i.e. they find patterns, but they don't actually have \n   - *absurd mistakes*: 2-dimensional data, object-permanence, basically \n - system-1,2:\n   - framed mainly for the benefit of the populace, as people understand *agents*.\n   - in fact, it's more categorizations of mental processes, and some are slower and more deliberate than others\n - the example of simple shapes moving around: clearly we're proscribing agency and come with all this model baggage even for such simple data\n - system-1 is what happens 95% of the time, until you hit something surprising, not coherent, at which point it triggers system-2.\n - system-2 can be thought of as an editor (filter) for system-1.\n - distinction between *doubt* and *surprise*:\n   - instead of continuously predicting what is happening next, you see what happens and then make sense of it\n   - the idea here is that you have your system-1 that just keeps running and continuously checking the data in a very straightforward manner. but then once something weird happens, your attention is drawn.\n   - much more economical (?)"},{"fields":{"slug":"/knowledge-distillation/","title":"Knowledge Distillation"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - machine_learning\n---\n\n\n# Knowledge Distillation\n\nAs a constructive example, think of two similar classifications (two types of breeds of dogs). With one-hot encoding, these are orthogonal, and each class is essentially equivalent/exchangeable. However there's actually more structure to these classifications (you can imagine them being in some embedding space, and the dog classes should be closer together). I guess the phrase is *label similarity*.\n\nThe softmax probabilities is (hopefully) a proxy for this. So, in part, the student gets the shortcut of learning these class representations, but also conceivably it does not have the capacity to *learn* these subtleties, though it is able to mimic it.\n\nDoes Knowledge Distillation really work? [arXiv](https://arxiv.org/pdf/2106.05945.pdf)\n\n - \n\nKD as Semi-parametric Inference [Youtube](https://www.youtube.com/watch?v=dEE3-g_8dWo)\n\n - \n\nMany weird and curious results in this field:\n\n1. It turns out that if you do self-distillation (i.e. the student is the teacher, there's a Buddhist joke here somewhere), then the student will oftentimes outperform the teacher. [arXiv](https://arxiv.org/abs/1805.04770)."},{"fields":{"slug":"/market-for-lemons/","title":"Market for Lemons"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - economics\n - from_article\n---\n\n# Market for Lemons \n\nvia FT's [Free Lunch](https://app.ft.com/cms/s/db917987-63af-42b9-b4a7-9f91ff648ce1.html).\n\nNobel Laureate George Arkerlof's paper on \"The Market for Lemons\"\n\n> It was a pioneering analysis of how markets fail to achieve efficient transactions when consumers have worse information about the product than the sellers. In the used-car example, because some unscrupulous used-car traders try to pass off “lemons” as “peaches”, consumers are aware that not all cars are what they seem to be, and discount the value they are willing to put even on the cars of perfectly respectable sellers. Because it is hard to distinguish lemons and peaches in the dealership yard, all used cars are tarnished by this risk.\n\nI talked about this in my Introduction to Economics class, on a simple example of inefficient markets. Interestingly, you can think of #misinformation along the same lines, as a *marketplace of ideas*, as well as recent riots:\n\n> These provocations are, therefore, of a piece with social media propaganda campaigns that spread misinformation. Again, trust is the casualty: the effect of misinformation is not so much to spread false beliefs as to eliminate confidence in truthful ones. As the title of a book by Peter Pomerantsev captures so well, the outcome is that “nothing is true and everything is possible”.\n\n> We should understand the effect of agents provocateurs in America’s equal justice protests in the same way. The effect of seeding vandalism and rioting amid the protests is to make it hard to distinguish legitimately angry but peaceful protesters from violent mobs. As a result all are tarred with the same brush."},{"fields":{"slug":"/meta-analysis-vs-preregistration/","title":"Meta Analysis vs Preregistration"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_podcast\n - meta_analysis\n - statistics\n---\n\n# Meta Analysis vs Preregistration\n\nsrc: [Expertise of Death](http://www.theblackgoatpodcast.com/posts/the-expertise-of-death/), Black Goat Podcast\n\n>  How important is expertise in conducting replications? Many Labs 4 was a large, multi-lab effort that brought together 21 labs, running 2,220 subjects, to study that question. The goal was to compare replications with and without the involvement of the original authors to see if that made a difference. But things got complicated when the effect they chose to study – the mortality salience effect that is a cornerstone of terror management theory – could not be replicated by anyone.\n\nThe somewhat crazy thing is that a comprehensive meta-analysis was conducted for terror management theory, via [@Burke:2010ij]. In particular, it was \"164 articles with 277 experiments\". That's a lot of experiments. Things like funnel plots don't help either.\n\n![](img/terror_funnel.png)\n\nThe important question is: are there any meta-analysis techniques that would be able to flag something wrong with these experiments? I'm pretty sure the answer here is no. Though, I would loved to be proved wrong. As one of the podcasters remarked, doing a meta-analysis cannot beat doing a large preregistered study.\n\nThe problem with meta-analyses is that they assume that the $p$-values are drawn from the true distribution, without any bias or problems with the experimental design. But that's an unreasonable assumption. What is the point of a meta-analysis if not to help combat this replication crisis? I thought the point was that you're able to detect cases of $p$-hacking."},{"fields":{"slug":"/meta-learning/","title":"Meta Learning"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - machine_learning\n - meta_learning\n---\n\n# Meta Learning\n\nsrc: [thesis](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-105.pdf)\n\n- relates to [[transfer-learning]], except you're trying to do an even better kind of pre-training, that generalises across all tasks\n- intuition:\n    - you want to train a NN so that it does well on a whole class of tasks, and not specific to one dataset\n    - you want to \"pre-train\" a NN such that, regardless of the task you eventually train it on, it'll be very quick to \"train\"\n        - speed of training is a proxy for \"close-ness\" of the solution\n        - if you're learning a useful representation, then this pre-trained model should speed up training\n        - <mark>it seems to me that this doesn't necessarily hold: if you think of training a NN as \"learning\", then this is like starting with knowledge of \"fundamentals\". but I don't think the analogy works very well, since it's really just the \"initialisation\", and it quickly gets \"replaced\".</mark>\n            - I guess what I'd like is some kind of **augmentation**\n            - a little more like how LSTMs have a memory slot\n- then they're going to use gradient updates, somehow\n- my initial guess:\n    - suppose you have a set of datasets. in some sense, what you want is to find the mid-point of all the trained models. you could just take one step per dataset (or sampled)\n    - but will this *converge*?\n        - it might be the case that the steps you take are in the opposite direction?\n            - though, if that's the case then that just means you've reached an \"impasse\" (or that there's nothing shared between them)\n- *The process of training a model’s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks.*\n- *From a dynamical systems standpoint, our learning process can be viewed as maximising the sensitivity of the loss functions of new tasks with respect to the parameters*\n- they have some theory that shows that a single gradient-step can get you to any function (??) – this seems impossible"},{"fields":{"slug":"/michael-jordan-plenary-talk/","title":"Michael Jordan Plenary Talk"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_talk\n - statistics\n - data_science\n---\n\n# Michael Jordan Plenary Talk\n\nOur MJ gave a plenary talk at the SIAM conference on the mathematics of DS ([link](https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=69238)).\nI've seen bits and pieces of this talk already.\n\n## Key Takeaways\n\n - He thinks the next set of questions to ask of ML will be decision-based\n   + Example: recommendation systems. What if you recommend everyone the same restaurant? [article](https://hdsr.mitpress.mit.edu/pub/2imtstfu/release/6)\n   + Clearly this isn't going to work out, so you need to do some type of *load-balancing*. Or, better, just create a #market.\n   + Hence he thinks we'll be merging with microeconomics.\n   + The essential issue here is scarcity, which therefore requires a market.\n   + The question is then how do algorithms fit into this.\n   + Example: you have the matching algorithm (i.e. Match day for medical students). But what if you don't know preferences, so you have to infer them in the way that you would do with #bandit problems\n   + Of course, questions of [[causal-inference]] are also related.\n   + This take is somewhat similar to what I'm going for with my [[fairness-project]], except that I don't think of markets as the solution. We're both thinking that these algorithms are affecting scarcity, but I'm tackling this slightly differently. It's all about the feedback mechanisms, which is also something that he hinted at.\n - [[discretization-of-gradient-flow]]\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[discretization-of-gradient-flow]: discretization-of-gradient-flow.md \"Discretization of Gradient Flow\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/monopoly-in-tech/","title":"Monopoly in Tech"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - money_stuff\n - finance\n---\n\n# Monopoly in Tech\n\nThis is quintessential Levine: taking a concept that seems pretty straightforward at first blush (the worry that tech companies are becoming monopolies -- duh, right?), and using very simple words and ideas to make you slowly realise that you haven't thought this through, clearly.\n\nI aspire to create content like this.\n\n> **Bigness**\n> \n> Some banks are very big. Some people think this is bad. There is a traditional way to say “it is bad that this company is so big,” and that way uses the word “monopoly.” “This company is so big that it is a monopoly, which is bad.” It is useful to be able to say this, because the government has a lot of power to limit monopolies, to regulate their behavior and break them up. \n> \n> It is not, however, particularly true of the big banks. A monopoly is a specific thing, a company that is so big that it dominates its market and can force out competitors and raise prices. The markets in which banks compete are, for the most part, extremely competitive.[5] If you want a mortgage, you pretty much pay the market rate for mortgages; JPMorgan Chase & Co. can’t charge you whatever it wants.\n> \n> Still, people think it is bad that the banks are so big, for other reasons. They worry about risk concentration, about banks that are “too big to fail” taking too many risks and the taxpayers bearing those risks, about too much centralization of banking making it more fragile, about banks that are “too big to manage” doing dumb things and crashing the financial system. Those worries are controversial, but never mind that. Assume for now that they are correct. What should the government do about them?\n> \n> One possibility is that the government’s antitrust regulators — at the Justice Department and the Federal Trade Commission — should go after the biggest banks for antitrust violations. The regulators could say “you are too big, you are a monopoly, we need to break you up into smaller pieces.” And then the banks would say “no,” and they would go to court, and the regulators could try to prove that the big banks are monopolists. And this would be hard to do, because they basically aren’t. It wouldn’t be impossible, though, I guess, because they are in lots of businesses and some of them are less competitive than others and there are probably some bad emails somewhere and so forth. The regulators’ odds of breaking up the big banks on antitrust grounds wouldn’t be zero. But they would be low.\n> \n> The other possibility is that other government regulators should, in setting other regulations, take bigness into account and try to regulate and discourage it. Conveniently banking is a very regulated business, and there are regulators and prudential supervisors who can do all sorts of meddling in a bank’s business. So for instance if you worried that giant banks could be “too big to fail” and pose a systemic risk to the financial system, the banks’ capital regulators could put out a rule saying “very big banks need to have more capital to offset the higher risk they pose to the financial system.” That would both reduce the risk of big-bank failure and also create an incentive for banks to stay smaller or break themselves up. And in fact there is such a rule, for exactly those sorts of reasons; it is called the “G-SIB surcharge.”\n> \n> Or if you worried that giant banks could be “too big to manage” and do dumb things, then the banks’ supervisors could tell a big bank that did a dumb thing “you can’t get any bigger until we’re satisfied you won’t do more dumb things.” They can just do that! The supervisors can just tell a bank not to get bigger, and it has to listen! They actually did it to Wells Fargo & Co., it’s kind of amazing. The theory wasn’t “Wells Fargo is a monopoly”; it was just “we don’t like what Wells Fargo has been up to so it can’t get any bigger.”\n> \n> I should emphasize that banking is a very regulated business, and the government doesn’t have quite as many levers to pull with most other businesses. Still lots of businesses are regulated in lots of ways, and the same general principles apply:\n> \n> If you think it is bad that a business is big, because it has a monopoly, sure, have the antitrust regulators go after it for antitrust violations.\n> If you think it is bad that a business is big, for other reasons, have other regulators try to limit its bigness in ways that directly address those other reasons.\n> In a pinch, if you think it is bad that a business is big, you could always have the other regulators try to limit its bigness in ways that don’t relate in any particularly logical way to those reasons. If you think that the bigness of social media companies is bad because they spread misinformation and undermine democracy, that is not really an antitrust problem,[6] and there is not exactly a Federal Truth Regulator that can promulgate misinformation rules. But maybe you can find some regulatory regime to shoehorn into that purpose. Maybe you’ve got a regulator in charge of, I don’t know, internet bandwidth or wireless spectrum or electricity usage or truth in advertising or whatever,[7] and you tell that regulator to turn up the heat on big social media companies. Not because you care about their electricity usage or whatever, but just to deter them from being big, because you think their bigness is bad.\n> If all else fails, you can have the antitrust regulators try to break up the business because it is too big, even though it isn’t a monopoly. That may not work though.\n> We talked yesterday about Facebook Inc. Lots of people think it is bad that Facebook is so big, but it is a little hard to put that in traditional monopoly terms. Is the problem of Facebook’s bigness that it can charge users monopolistically high prices for posting on Facebook? No; posting on Facebook is free. Is the problem that it can charge advertisers monopolistically high prices for advertising on Facebook? No. I don’t know if it can; I just know that I have never heard anyone make that complaint: If you don’t like Facebook, it’s not because you worry about advertisers overpaying.\n> \n> Is the problem of Facebook’s bigness some other form of anticompetitive behavior that makes consumers (Facebook users) worse off? Sure, maybe; intuitively I suspect Instagram would be a nicer place if it was still independent than it is under Facebook’s ownership. But this stuff is a little hard to articulate, which is why the FTC failed to articulate it: It sued Facebook for antitrust violations, and this week a judge dismissed that lawsuit for failing to even say why Facebook might have a monopoly. “It is almost as if the agency expects the Court to simply nod to the conventional wisdom that Facebook is a monopolist,” wrote the judge. The conventional wisdom is that it is bad that Facebook is so big! But that does not make it a monopoly in the technical, legal sense of the term.\n> \n> I suspect the main problem most people have with Facebook’s bigness is not about consumer choice but rather about Facebook’s political and social influence. You could imagine addressing that more directly than with antitrust law. And there have been suggestions for doing so — repealing Section 230, using election law to regulate Facebook, etc. — though I can’t say any of them strike me as great. Still it’s the right basic idea. Figure out what you don’t like about Facebook’s dominance and then regulate that, rather than just equating bigness with antitrust.\n> \n> Anyway here’s this:\n> \n> > The Biden administration is developing an executive order directing agencies to strengthen oversight of industries that they perceive to be dominated by a small number of companies, a wide-ranging attempt to rein in big business power across the economy, according to people familiar with the plans.\n> > \n> > The executive order, which President Biden could sign as soon as next week, would direct regulators of industries from airlines to agriculture to rethink their rule-making process to inject more competition and to give consumers, workers and suppliers more rights to challenge large producers.\n> > \n> > The goal is to broaden the way policy makers approach business concentration in the U.S., going beyond conventional antitrust enforcement focused on blocking big mergers. For example, companies in industries controlled by a small number of big firms might face new rules for disclosing fees to consumers or for their relationships with suppliers, the people familiar with the effort said.\n> \n> Seems right! Or not, I mean; I guess it depends on how you feel about big business generally. But if you feel bad about some big business specifically, addressing that in a specific way — rather than assuming that big business is exclusively an antitrust problem — seems like the way to go."},{"fields":{"slug":"/multidimensional-mental-representations-of-natural-objects/","title":"Multidimensional Mental Representations of Natural Objects"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - paper\n - psychology\n---\n\n# Multidimensional Mental Representations of Natural Objects\n\n - src: [@Hebart:2020uz]\n - [arXiv](https://psyarxiv.com/7wrgh/)\n - [ICLR](https://arxiv.org/abs/1901.02915) (earlier version)\n\n## Contemplations\n\n### Distributed Representation\n\n![General Workflow](img/41562_2020_951_Fig1_HTML.png)\n\nThe idea of \"distributed representation\" (`word2vec`) is that a) words are represented as high-dimensional vectors, and b) words that cooccur should be closer together. What this means is that you can simply ingest a large corpus of unstructured text, collecting co-occurrence data, and learning a distributed representation along the way. The beautiful thing about this approach is that the only thing it relies on is some notion of co-occurrence; in the case of language, this is simply a window function.\n\nWhat happens when you don't have such information? Well then, you have to get a little creative. For those familiar with `word2vec`, you'll know that a lesser-known but powerful trick they proposed is this notion of *negative sampling*: they found that it wasn't enough to, during the traversal of your corpus and training your model, to push co-occurring words together, but it was also important to push words apart (i.e. randomly pick a word, and then do the opposite maneuver). Note the similarity to [[triplet-loss]]. In essence, we can move from pairs to triplets. But this seems like we've made the problem even more difficult.\n\nAs it turns out, the triplet structure allows us do something interesting, which is to turn the problem into an \"odd-one-out\" task. Cute!^[Dwelling on this a little longer, the paired version would be more like, show pairs to people and ask if they should be paired, which feels like an arbitrary task. The alternative, which is to give a rating of the similarity, naturally inherits all the ailments that come with analyzing ratings.] In other words, given a corpus of images of objects, we can show a random triplet of them to humans, and ask which one is the odd one out. All that remains is to train our models with this triplet data. Here, they adapt `softmax` to this problem, applying it only to the chosen pair:\n\n$$\n\\sum_{(i,j,k)} \\log \\frac{ \\exp\\left\\{ x_i^T x_j \\right\\} }{ \\exp\\left\\{ x_i^T x_j \\right\\} + \\exp\\left\\{ x_i^T x_k \\right\\} + \\exp\\left\\{ x_j^T x_k \\right\\}} + \\lambda \\sum_{i = 1}^{m} \\|x_{i}\\|_{1},\n$$\n\nwhere the first summation is over all triplets, but, crucially, we designate $(i,j)$ to be the chosen pair (i.e. $k$ is the odd one out). The $l_1$ penalty induces sparsity on the vectors, which improves interpretability. Finally, and I'm not sure exactly how they implement this, but they also enforce the weights of the vector to be positive, to provide an additional modicum of interpretability.^[I feel like they overemphasize these two points, sparsity and positivity, as a way to differentiate themselves from other methods, when it's really quite the trivial change from a technical perspective.]\n\n![Interpretable dimensions](img/41562_2020_951_Fig3_HTML.png)\n\nWhat is the output of this model? Ostensibly, it is a \"distributed representation\" of how we as humans organize our understanding of objects, albeit also conflating the particulars of the image used to represent this object.^[Does the prompt ask the subject to consider the image as an archetype, or simply ask to compare the three images on face value, whatever they see fit? My guess is probably the latter.] Note that even though we're dealing with the visual domain, the open-endedness of the task means that humans aren't necessarily just using visual cues for comparison, but might also be utilizing higher order notions.\n\nA crude approximation is that this model outputs two types of *features*: visual properties, like size, texture, etc.; and then the more holistic, higher-order properties, such as usage, value/price, etc. The key is that the latter property should not be able to be inferred from visuals alone.^[So really what I'm saying is that there are two groups, $V$ and $V^C$. Not groundbreaking.] Thus, this model is picking something intangible that you cannot possibly learn from simply analyzing image classification models. However, this then begs the question: is it possible to learn a similar model by considering a combination of images (visual features) as well as text (holistic features)? What would be the nature of such a multi-modal learning model?\n\nI see a few immediate challenges. The first is that, while one could take the particular image chosen in this experiment to represent an object, it would be too specific. What we would want is to be able to learn a single feature across all images of a single object. I feel like this should be something people have thought about, right? The second problem is, are word embedding of the objects sufficient to get the higher-order features? \nSupposing we solve all those problems, we're still left with the question of how to combine these two things meaningfully. It feels to me like we should be training these things in unison?\n\n### Typicality\n\nThe idea here is that the magnitude of a particular dimension should import some meaning, the most likely being the *typicality* of this object to this particular feature (e.g. food-related). What they do is pick 17 broad categories (the graphs below) that have a corresponding dimension in the vector representation, then for each image/object in this category (e.g. there are 19 images in the Drink category, corresponding to 19 points on the graph), they get humans to rate the typicality of that image for that category.\n\n![Typicality scores](img/41562_2020_951_Fig7_HTML.png)\n\nThis result is somewhat surprising to me. For one, if you give me a list of images for, say, the category of animals, I would have no idea how to rate them based on *typicality*. Like, I think it would involve something cultural, regarding perhaps the stereotypes of what an animal is. I can imagine there being some very crude gradation whereby there are the clear examples of atypical, and then clear examples of typical, and then the rest is just a jumble. It doesn't really appear that way from the data – I would have to look at these images to get a better sense.^[As an aside, I wish they also included the values of all the other images too, not just those in this category. Perhaps they are all at close to zero, which would be great, but they don't mention it, so I assume the worst.]\n\nAlso, how is it able to get at typicality through this model? I think what's illuminating to note is that, out of the 27 broad categories of images in this dataset, 17 can be found in the 49-dimensional vector representation. Here's what I think is probably happening (in particular for these 17 categories):\n\n - if all three images are from different categories, then probably we're learning something about one of those other dimensions (hopefully);^[Another thing is that this step helps to situate the categories amongst each other.]\n - if two images are from the same category, and the third is different, then the same pair is most often picked (helping to form this particular dimension);\n - if all three images are picked, then I guess the odd one out is the one that's the least *typical*.\n\nHaving laid it out like so, I'm starting to get a little skeptical about the results: it almost feels like this is a little too crude, and the data is not sufficiently expressive for a model to be able to learn something that's not just trivial. Put another way, this almost feels like a glorified way of learning the categories – though, there's nothing necessarily wrong with that, since the (high-level) categories are obviously important.\n\nPerhaps it helps to consider the following generative model for images: suppose each image was represented by a sparse vector of weights, with many of the coordinates corresponding to broad categories. Set it up in a such a way that if you're in a broad category, then that dominates all other considerations (so it follows the pattern above). Then, simply run this odd-one-out task directly on these vectors, and see if you're able to recover these vectors.^[It almost feels like a weird autoencoder architecture...]\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[triplet-loss]: triplet-loss.md \"Triplet Loss\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/non-deep-networks/","title":"Non-deep Networks"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - paper\n---\n\n# Non-deep Networks\n\n - [arXiv](http://arxiv.org/abs/2110.07641)\n - [OR](https://openreview.net/forum?id=Xg47v73CDaj)\n\nOne common wisdom in the era of deep learning is that \"depth trumps width\". After all, we are not in the era of wide learning. Deeper networks seem to allow for more *expressivity* given the compositional nature of layers, holding the number of parameters fixed. From a theoretical perspective, we know that depth-2 networks are universal approximators, but the size of the network is exponential in the input dimension. I'm pretty sure that there has been work showing that dependence for deeper networks is no longer exponential in $n$, but who knows under what stringent conditions does this hold.\n\nMore recently, we've entered the era of **deep** learning, whereby the number of layers are in the hundreds, if not thousands. This feels like an altogether different regime, which I would call really-deep learning. This also gets closer to the question of scaling.\n\nAlso, note that a lot of the theoretical work talks about ReLU networks that are just MLPs. This is so far removed from what the models are like now – existing architectures are much more like compositions of *blocks* of computation, with only a handful of block types. It's very much like a circuit, except instead of electricity you have a flow of data/information, and the components are these blocks.\n\nContinuing with the circuit analogy, you can differentiate between parallel and series. However, I think this is where the usefulness of the analogy ends. It's more helpful to think about computer programs, where we have sequential and parallel computation. It's clear that almost all programming is done sequentially, as that's how you get composition and the power of compounding. Parallelisation is something that can help speed things up, but for the most part it's something that you do to take advantage of distributed resources (once you hit the limitations of the single core).\n\nWhere does this paper fit in? It takes aim at this very-deep learning regime, and asks the question: is it really necessary to have hundreds of layers? Can we achieve the same sort of performance with only a handful of layers instead?\n\n![](https://i.imgur.com/1ybpnql.png)\n\nTo be honest, I'm not *that* familiar with the ImageNet benchmark. I think what they're trying to argue is that they're replacing depth with width. But to me, seeing the performance of the single-stream already shows that you can already do quite a lot with 12 layers.\n\nActually, as one reviewer notes:\n\n> My major concern is that the authors argues contribution of parallel subnetworks, while I think the traditional RepVGG-wise blocks adopted in ParNet contributes significantly more to prediction accuracy. Even if ParNet adopts 1 branch only, ParNet's accuracy can still achieve 75% more, as \"ParNet-M-OneStream\" shown in Table 8. Comparison of Table 7 and Table 10 shows that the RepVGG blocks is significantly more important than adopted number of branches on improving accuracy.\n\n\n## Paper Specifics\n\n - A key architectural choice is that the *parallel* streams use different resolutions (I guess this is also why you get diminishing returns with more streams).\n - They argue that moving calculations to width will take advantage of multiple cores. Unconvinced by this.\n\n### Parallel Performance\n\n![](https://i.imgur.com/yhfImSa.png)\n\nTable 10: comparing \\# of branches against performance. What they find is that for a fixed number of parameters, the optimal number of branches is 3. This shows that all this \"embarrassingly parallel\" talk is only *embarrassing*. For one thing, the performance improvements are, in my eyes, incredibly marginal. \n\nSomething that they keep doing is to fix the number of parameters and then vary across various configuration parameters (depth, width, channels, streams). This is clearly the simplest way to keep the *capacity* of the model constant, but in this era of complicated architectures, it's unclear how fair this metric really is.\n\n### Ensembles\n\n![](https://i.imgur.com/UpRHFxS.png)\n\nTable 9: comparing ensembles against the same number of streams. Their conclusion for this ablation study is that their *parallel* configuration outperforms a simple ensemble. From my perspective, this makes me think that ensembles, which are the actual \"embarrassingly parallel\" operation, aren't all that bad. Granted, it's not a particularly efficient usage of parameters (and my intuition says that part of why networks do so well is that in some sense they are performing some kind of bagging or boosting).\n\nIt's not clear to me how they train the ensemble (multiple networks). I assume it's not the classic statistical way, which is to do *bagging*. Perhaps there's a canonical choice of ensemble method, but it's really annoying that this isn't specified.\n\n## Final Thoughts / Open Questions\n\n - My feeling is that moving to width is the new *feature engineering*. Yes it's probably possible to *architect* a complex neural network with only a few layers. But that's not really that interesting, because you've essentially *feature engineered* that architecture for that particular problem.\n   - What would make a compelling case would be if the application of extending width was some very simple, extensible procedure, i.e. actually \"embarrassingly parallel\".\n - Anyone thought of adding closed loops to a network? In some sense, this explosion of layers (and scaling), the point of it is to be able to let the neural network go do its thing. I guess the problem is that backprop no longer works. This feels a little like [[can-you-learn-an-algorithm]], where the idea is to the give a RNN more *runway* to be able to do more processing.\n   - My feeling is that loops might be possible as some sort of extension. In other words, you have the normal training with a huge dump of data, which requires backprop, and there are no loops there. But then you move into this later stage where it's much more single-data-at-a-time, and here it's no longer about backprop (what is the replacement then?), and you can now add some loops. This is encroaching on Hofstadter's \"I am a Strange Loop\" territory.\n   - Maybe (reflecting how we learn), what you want is something like unsupervised learning in the beginning (huge unstructured data), which allows you to train these generic processing units, to capture very high level things. And then, at the same time, you have the more systemic learning engine that contains loops and whatnot, and these use these generic units, and that is \"trained\" in a much more deliberate manner, without backprop, to allow for things like loops to work.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[can-you-learn-an-algorithm]: can-you-learn-an-algorithm.md \"Can You Learn an Algorithm\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/objection-detection-for-guis/","title":"Object Detection for GUI"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - gui\n  - machine_learning\n---\n\n# Object Detection for GUI\n\n## Problems\n\nLarge in-class variance and similarity between classes: basically, a very difficult problem!\n\n - Large in-class variance:\n   - style is at the whim of the designer/application.\n   - in general (interestingly), each data point has the same style, but across samples you have nothing\n - High cross-class similarity:\n   - key here is that sometimes the difference between two GUIs is something very small. that being said, this is more about classifying different types of GUIs, which we don't really do right now.\n - Packed scenes:\n   - close quarters, dense – in stark contrast to natural object detection problems\n\nI think it's also important to highlight the difference between GUIs and natural images, because that's exactly where we can improve (or even cross-pollinate).\n\n1. sharp, pixel based (plus some shadows, but all computer generated)\n2. close quarter, dense\n3. no common style per class\n\n![examples](https://i.imgur.com/dPDbZRP.png)\n\n\n\n![table](https://i.imgur.com/N1pFCLT.png)\n\n"},{"fields":{"slug":"/one-stock-to-rule-them-all/","title":"One Stock to Rule Them All"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - finance\n - idea\n---\n\n# One Stock to Rule Them All\n\nA small, stupid, simple idea (that's been bouncing around my head): the price of a stock is a function of all the information about that particular stock (which should dictate its movement). If you believe the efficient market hypothesis (EMH) (which, granted, most people do not), then by extension this suggests that a stock's price must also *capture* all the information in the world. Of course, most other information takes up an infinitely small part of that stock's price (the less relevant, the smaller the part), but in some sense, the whole world is captured in every single stock.\n\nNow, even if you don't believe in the EMH, such a view might be instructive when considering the real world: essentially each stock is like a *view* of the infinite dimensional representation of the world. Under the EMH, a view (a projection) does not reduce the dimensions (i.e. no zero'd coordinates), whereas in real life, you're most likely getting a projection into a pretty compact finite space.\n\nWhat's the end result of all this? It sort of feels like the word embedding stuff, but not really.^[The key difference there is that you assume words/stocks have some latent representation in some shared space, whereas here we assume all stocks derive from the same latent representation, but simply take on different views.] The conjecture that comes out of this is that, essentially, there's a very high dimensional vector that corresponds to the world, and each stock is simply one view into that world. The goal is then to learn this vector, as well as the projections associated with each stock. The question is then: is this a solvable problem?\n\nMore formally, let $Z_t$ be this latent representation of the world (at time $t$). For simplicity, let's assume that $Z_t$ is finite-dimensional, $\\in \\mathbb{R}^{n}$. Then, each stock $X^{i}_{t}$ corresponds to a view of $Z_{t}$, namely $X^{i}_{t} = P^{i}(Z_{t})$, where $P^{i}$ is some projection matrix (possibly varies with $t$, but if it does, we probably need to assume that it is slowly-varying). We are given the $\\left\\{ X^{i}_{t} \\right\\}_{i, t}$, and our goal is to estimate $\\left\\{ Z_t \\right\\}_{t}$ and $\\left\\{ P^{i} \\right\\}_{i}$.\n\nThe above formulation still subscribes to the linear world, whereas nowadays we want to express things as non-linear functions, allowing us to take full advantage of deep learning. However, it's unclear to me whether or not it's even possible to frame this in a way that's conducive for deep learning. Of course, every time there's talk of a latent representation, it feels like you should be able to solve it with neural networks."},{"fields":{"slug":"/overparameterized-regression/","title":"Overparameterized Regression"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - paper\n - overparameterization\n---\n\n# Overparameterized Regression\n\nsrc: [@Arora:2018vn], section 3 on $l_p$ regression\n\nThey show that in the simple scalar linear regression problem, i.e. loss function\n\n$$\n  \\E[(x,y)\\sim S]{\\frac{1}{p}(y - \\mathbf{x}^\\top \\mathbf{w})^p}\n$$\n\nif you overparameterize ever so slightly to\n\n$$\n  \\E[(x,y)\\sim S]{\\frac{1}{p}(y - \\mathbf{x}^\\top \\mathbf{w}_1 w_2)^p},\n$$\n\nthen gradient descent turns into an accelerated version.\n\n- I remember Arora presenting on this particular result a few years back (or something like this), and finding it very intriguing.\n- I think this goes nicely with the theme of [[statistics-vs-ml]], though it's a slightly different angle.\n  + essentially: we statisticians are **afraid** of overparameterization, because it removes specificity/leads to ambiguity\n  + but actually, when it comes to these gradient methods, it actually helps to *overparameterize*"},{"fields":{"slug":"/precision-recall/","title":"Precision/Recall"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - interview\n - computer_science\n---\n\n# Precision/Recall\n\nSomething that we statisticians don't really think about is the notion of precision/recall ([wiki](https://en.wikipedia.org/wiki/Precision_and_recall)) in classification. The definition actually comes from *information retrieval* (e.g. web searches). You have the *retrieved* documents (the pages returned by the search engine), as well as the *relevant* documents (those pages that are actually relevant to the query). Then *precision* is the fraction of those documents that were retrieved that are actually relevant. Similarly, *recall* is the fraction of relevant documents that were retrieved.\n\nTo translate this to something more relevant to classification, if you're trying to classify dogs, then precision is the fraction of those that you categorised as dogs (retrieved) that were actually dogs, while recall is the fraction of actual dogs (relevant) that were correctly categorised. So recall is more about coverage, whereas precision is more like accuracy, sort of.\n\nEven though it's an incredibly straightforward concept, it actually lends itself to non-trivial queries.\n\nA very similar in statistics is the notion of false/true positives/negatives. Though in that case what we're mostly interested in is hypothesis testing, though you can easily coerce it into the framework of classification."},{"fields":{"slug":"/pseudo-inverses-and-sgd/","title":"Pseudo-inverses and SGD"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - mathematics\n - gradient_descent\n---\n\n# Pseudo-inverses and SGD\n\n- The Moore-Penrose pseudo inverse (which is often just called the pseudoinverse) is just a choice of multiple solutions that leads to a minimum norm solution\n  + $\\hat{\\beta} = X^+ Y$\n  + An obvious consequence, is that ridge regression is not the same as the pseudoinverse solution, even though they're obviously very similar\n  + my feeling is that, for a particular choice of penalty, the solution should be same (since you should be able to reframe the optimization into\n    *  $\\min ||\\beta||_2 \\text{ s.t. } ||Y - X \\beta ||_2 \\leq t$\n  + part of me is a little confused: if you can just define this pseudoinverse, then how come we don't use this estimator more often? clearly it must not have good properties?\n\n- thanks to this [tweet](https://twitter.com/beenwrekt/status/1041724607911227392), realize there's a section in [@Zhang:2016ve] that relates to [@Bartlett:2020ck] [[benign-overfitting-in-linear-regression]], in that they show that the SGD solution is the same as the minimum-norm (pseudo-inverse)\n  + the curvature of the minimum (LS) solutions don't actually tell you anything (they're the same)\n  + \"Unfortunately, this notion of minimum norm is not predictive of generalization performance. For example, returning to the MNIST example, the $l_2$-norm of the minimum norm solution with no preprocessing is approximately 220. With wavelet preprocessing, the norm jumps to 390. Yet the test error drops by a factor of 2. So while this minimum-norm intuition may provide some guidance to new algorithm design, it is only a very small piece of the generalization story.\"\n    * but this is changing the data, so I don't think this comparison really matters – it's not saying across all kinds of models, we should be minimizing the norm. it's just saying that we prefer models with minimum norm\n    * interesting that this works well for MNIST/CIFAR10\n  + there must be something in all of this: on page 29 of [slides](https://www.stat.berkeley.edu/~bartlett/talks/201908Microsoft.pdf), they show that SGD converges to min-norm interpolating solution with respect to a certain kernel (so the norm is on the coefficients for each kernel)\n    * as pointed out, this is very different to the benign paper, as this result is data-independent (it's just a feature of SGD)"},{"fields":{"slug":"/","title":"Artifactual Neural Network"},"frontmatter":{"draft":false},"rawBody":"# Artifactual Neural Network\n\nHere be gradients."},{"fields":{"slug":"/stand-alone-self-attention-in-vision-models/","title":"Stand-Alone Self-Attention in Vision Models"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - paper\n---\n\n# Stand-Alone Self-Attention in Vision Models\n\n - [arXiv](https://arxiv.org/abs/1906.05909)\n\n"},{"fields":{"slug":"/stewardship-of-global-collective-beahvior/","title":"Stewardship of Global Collective Behaviour"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - paper\n - society\n - research\n---\n\n# Stewardship of Global Collective Behaviour\n\nsrc: [@BakColeman:2021wt]\n\nA well written call-to-arms for some sort of interdisciplinary effort to understand the possible pernicious effects of social media and more broadly the rapid pace of technological change affecting the way we communicate, form groups, digest information, and hopefully provide guidance on how to solve these problems (e.g. writing pieces specifically for regulators).\n\nThey use a term called *crisis discipline* which I like, the canonical example being climate (change) science: you have this incredibly complicated system that needs urgent research and attention (for catastrophic reasons), but you don't necessarily have the time (or it's just not possible given the complexities of the system) to be entirely systematic and sure about the conclusions. In other words, these kinds of disciplines call for a much more *agile* form of research.\n\nWhat's interesting to me is that they couch all this talk through the lens of [[complexity-theory]]. The idea is that, for instance, once you connect half the world's population together through the internet, or social media, you're going to get unaccounted-for emergent behaviour, very much like those studied in complexity science, except usually the subject is natural processes (like swarms of locusts or school of fish). The difference now is that we're dealing with humans, social interactions.\n\nA good example here is the flow of information. Usually when we think of information flow we think of communication networks, where we're sending bits of data around. However, the real information flow networks, and those that matter the most right now from a catastrophic perspective, are the information flows that we humans create when we read and share news over social media, thereby enabling the incredible propagation of fake news that we see permeate the world today. And this isn't just a simple process: once you incorporate humans (and human judgement) into this network, it becomes infinitely more complicated to model and predict.\n\nI definitely feel like this is something that I've been trying to articulate, and so I'm happy to see it laid out in this clear manner (unlike the way my brain organises its information, if it does that at all). It also has the same sort of flavor as my [[fairness-project]] work."},{"fields":{"slug":"/transformers/","title":"Transformers"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - machine_learning\n - neural_networks\n---\n\n# Transformers\n\n- references:\n  - [lil-log](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n  - [distill](https://distill.pub/2016/augmented-rnns/)\n  - [ebook-chapter](http://d2l.ai/chapter_attention-mechanisms/transformer.html) on transformers (actually this ebook isn't great, as it ends up being more about the implementation)\n- it turns out that #attention is an earlier concept, which itself is motivated by the encoder-decoder sequence-to-sequence architecture\n  - ![](https://karpathy.github.io/assets/rnn/diags.jpeg)\n  - (oops) I didn't really understand this diagram:\n    - sequence to sequence models are essentially the third diagram, where basically the input sequence and output sequence are asynchronous\n    - versus the normal rnn, which takes the output as the input\n    - I really like the example of 1-to-many, image-to-caption, so your input is not a sequence, but your output is\n  - seq2seq example is the translation program, where importantly the input and output sequence don't have to be the same length (due to the way languages differ in their realisations of the same meaning)\n  - the encoder is the rnn on the input sequence, and this culminates into the last hidden layer\n    - this is essentially the *context* vector: the idea here is that this (fixed-length) vector captures all the information about the sentence\n      - key: this acts like a sort of **informational bottleneck**, and actually is the impetus for the attention mechanic\n  - key point (via this [tutorial](https://arxiv.org/pdf/1703.01619.pdf)):\n    - instead of having everything represented as the last hidden layer (fixed-length), why not just look at all the hidden layers (vectors representing each word)\n    - but, that would be *variable* length, so instead just look at a linear combination of those hidden layers. this linear combination is learned, and is basically *attention*\n- Transformers #todo\n  - \"prior art\"\n    - CNN: easy to parallelise, but aren't recurrent (can't capture sequential dependency)\n    - RNN: reverse\n  - goal of transformers/attention is achieve **parallelization** and **recurrence**\n    - by appealing to \"attention\" to get the recurrence (?)\n\n## Transformers\n\n - key is *multi-head self-attention*\n   + encoded representation of input: key-value pairs ($K,V \\in \\mathbb{R}^{n}$)\n     * corresponding to hidden states\n   + previous output is compressed into query $Q \\in \\mathbb{R}^{m}$.\n   + output of the transformer is a weighted sum of the values ($V$).\n\n## Todo\n\n - Visualising and Measuring the Geometry of BERT [arXiv](https://arxiv.org/pdf/1906.02715.pdf)\n - [random blog](http://www.peterbloem.nl/blog/transformers)\n - pretty intuitive description of transformers on [tumblr](https://nostalgebraist.tumblr.com/post/185326092369/the-transformer-explained), via the LessWrong community"},{"fields":{"slug":"/triplet-loss/","title":"Triplet Loss"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n---\n\n# Triplet Loss\n\nsrc: [Git](https://omoindrot.github.io/triplet-loss)"},{"fields":{"slug":"/journal/2021-12-24/","title":"2021-12-24"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - journal\n---\n"}]}}}