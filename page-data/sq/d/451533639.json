{"data":{"allMdx":{"nodes":[{"fields":{"slug":"/placeholder/","title":"This Is a Placeholder File for Mdx"},"frontmatter":{"draft":true},"rawBody":"---\ntitle: This Is a Placeholder File for Mdx\ndraft: true\ntags:\n  - gatsby-theme-primer-wiki-placeholder\n---\n"},{"fields":{"slug":"/can-you-learn-an-algorithm/","title":"Can You Learn an Algorithm"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - paper\n---\n\n# Can You Learn an Algorithm\n\n - [arXiv](https://arxiv.org/abs/2106.04537)"},{"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Meta-Learning From Kernel Ridge-Regression"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - paper\n---\n\n# Dataset Meta-Learning From Kernel Ridge-Regression"},{"fields":{"slug":"/kahneman-neurips-perception/","title":"Perception"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - neurips\n  - psychology\n---\n\n# Perception\n\n - top down vs bottom up (optical illusions)\n - same data, different representations given context\n - AlphaGo's structure has parallels with system-1,2 thinking\n   - the Monte Carlo tree search is system-2: has all the logic, slow, deliberate\n   - however, you can't just exhaustively search the tree, so you need a neural network to have gain some intuition/heuristics about where to go in the tree, and that's system-1\n - \n - GPT-3 ≠ system-1: heuristics are borne out of forming representations/models that are internally coherent\n   - i.e. they find patterns, but they don't actually have \n   - *absurd mistakes*: 2-dimensional data, object-permanence, basically \n - system-1,2:\n   - framed mainly for the benefit of the populace, as people understand *agents*.\n   - in fact, it's more categorizations of mental processes, and some are slower and more deliberate than others\n - the example of simple shapes moving around: clearly we're proscribing agency and come with all this model baggage even for such simple data\n - system-1 is what happens 95% of the time, until you hit something surprising, not coherent, at which point it triggers system-2.\n - system-2 can be thought of as an editor (filter) for system-1.\n - distinction between *doubt* and *surprise*:\n   - instead of continuously predicting what is happening next, you see what happens and then make sense of it\n   - the idea here is that you have your system-1 that just keeps running and continuously checking the data in a very straightforward manner. but then once something weird happens, your attention is drawn.\n   - much more economical (?)"},{"fields":{"slug":"/knowledge-distillation/","title":"Knowledge Distillation"},"frontmatter":{"draft":false},"rawBody":"# Knowledge Distillation\n\nAs a constructive example, think of two similar classifications (two types of breeds of dogs). With one-hot encoding, these are orthogonal, and each class is essentially equivalent/exchangeable. However there's actually more structure to these classifications (you can imagine them being in some embedding space, and the dog classes should be closer together). I guess the phrase is *label similarity*.\n\nThe softmax probabilities is (hopefully) a proxy for this. So, in part, the student gets the shortcut of learning these class representations, but also conceivably it does not have the capacity to *learn* these subtleties, though it is able to mimic it.\n\nDoes Knowledge Distillation really work? [arXiv](https://arxiv.org/pdf/2106.05945.pdf)\n\n - \n\nKD as Semi-parametric Inference [Youtube](https://www.youtube.com/watch?v=dEE3-g_8dWo)\n\n - \n\nMany weird and curious results in this field:\n\n1. It turns out that if you do self-distillation (i.e. the student is the teacher, there's a Buddhist joke here somewhere), then the student will oftentimes outperform the teacher. [arXiv](https://arxiv.org/abs/1805.04770)."},{"fields":{"slug":"/multidimensional-mental-representations-of-natural-objects/","title":"Multidimensional Mental Representations of Natural Objects"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - paper\n - psychology\n---\n\n# Multidimensional Mental Representations of Natural Objects\n\n - src: (@Hebart:2020uz)\n - [arXiv](https://psyarxiv.com/7wrgh/)\n - [ICLR](https://arxiv.org/abs/1901.02915) (earlier version)\n\n## Contemplations\n\n### Distributed Representation\n\n![General Workflow](img/41562_2020_951_Fig1_HTML.png)\n\nThe idea of \"distributed representation\" (`word2vec`) is that a) words are represented as high-dimensional vectors, and b) words that cooccur should be closer together. What this means is that you can simply ingest a large corpus of unstructured text, collecting co-occurrence data, and learning a distributed representation along the way. The beautiful thing about this approach is that the only thing it relies on is some notion of co-occurrence; in the case of language, this is simply a window function.\n\nWhat happens when you don't have such information? Well then, you have to get a little creative. For those familiar with `word2vec`, you'll know that a lesser-known but powerful trick they proposed is this notion of *negative sampling*: they found that it wasn't enough to, during the traversal of your corpus and training your model, to push co-occurring words together, but it was also important to push words apart (i.e. randomly pick a word, and then do the opposite maneuver). Note the similarity to [[triplet-loss]]. In essence, we can move from pairs to triplets. But this seems like we've made the problem even more difficult.\n\nAs it turns out, the triplet structure allows us do something interesting, which is to turn the problem into an \"odd-one-out\" task. Cute!^[Dwelling on this a little longer, the paired version would be more like, show pairs to people and ask if they should be paired, which feels like an arbitrary task. The alternative, which is to give a rating of the similarity, naturally inherits all the ailments that come with analyzing ratings.] In other words, given a corpus of images of objects, we can show a random triplet of them to humans, and ask which one is the odd one out. All that remains is to train our models with this triplet data. Here, they adapt `softmax` to this problem, applying it only to the chosen pair:\n\n$$\n\\sum_{(i,j,k)} \\log \\frac{ \\exp\\left\\{ x_i^T x_j \\right\\} }{ \\exp\\left\\{ x_i^T x_j \\right\\} + \\exp\\left\\{ x_i^T x_k \\right\\} + \\exp\\left\\{ x_j^T x_k \\right\\}} + \\lambda \\sum_{i = 1}^{m} \\|x_{i}\\|_{1},\n$$\n\nwhere the first summation is over all triplets, but, crucially, we designate $(i,j)$ to be the chosen pair (i.e. $k$ is the odd one out). The $l_1$ penalty induces sparsity on the vectors, which improves interpretability. Finally, and I'm not sure exactly how they implement this, but they also enforce the weights of the vector to be positive, to provide an additional modicum of interpretability.^[I feel like they overemphasize these two points, sparsity and positivity, as a way to differentiate themselves from other methods, when it's really quite the trivial change from a technical perspective.]\n\n![Interpretable dimensions](img/41562_2020_951_Fig3_HTML.png)\n\nWhat is the output of this model? Ostensibly, it is a \"distributed representation\" of how we as humans organize our understanding of objects, albeit also conflating the particulars of the image used to represent this object.^[Does the prompt ask the subject to consider the image as an archetype, or simply ask to compare the three images on face value, whatever they see fit? My guess is probably the latter.] Note that even though we're dealing with the visual domain, the open-endedness of the task means that humans aren't necessarily just using visual cues for comparison, but might also be utilizing higher order notions.\n\nA crude approximation is that this model outputs two types of *features*: visual properties, like size, texture, etc.; and then the more holistic, higher-order properties, such as usage, value/price, etc. The key is that the latter property should not be able to be inferred from visuals alone.^[So really what I'm saying is that there are two groups, $V$ and $V^C$. Not groundbreaking.] Thus, this model is picking something intangible that you cannot possibly learn from simply analyzing image classification models. However, this then begs the question: is it possible to learn a similar model by considering a combination of images (visual features) as well as text (holistic features)? What would be the nature of such a multi-modal learning model?\n\nI see a few immediate challenges. The first is that, while one could take the particular image chosen in this experiment to represent an object, it would be too specific. What we would want is to be able to learn a single feature across all images of a single object. I feel like this should be something people have thought about, right? The second problem is, are word embedding of the objects sufficient to get the higher-order features? \nSupposing we solve all those problems, we're still left with the question of how to combine these two things meaningfully. It feels to me like we should be training these things in unison?\n\n### Typicality\n\nThe idea here is that the magnitude of a particular dimension should import some meaning, the most likely being the *typicality* of this object to this particular feature (e.g. food-related). What they do is pick 17 broad categories (the graphs below) that have a corresponding dimension in the vector representation, then for each image/object in this category (e.g. there are 19 images in the Drink category, corresponding to 19 points on the graph), they get humans to rate the typicality of that image for that category.\n\n![Typicality scores](img/41562_2020_951_Fig7_HTML.png)\n\nThis result is somewhat surprising to me. For one, if you give me a list of images for, say, the category of animals, I would have no idea how to rate them based on *typicality*. Like, I think it would involve something cultural, regarding perhaps the stereotypes of what an animal is. I can imagine there being some very crude gradation whereby there are the clear examples of atypical, and then clear examples of typical, and then the rest is just a jumble. It doesn't really appear that way from the data – I would have to look at these images to get a better sense.^[As an aside, I wish they also included the values of all the other images too, not just those in this category. Perhaps they are all at close to zero, which would be great, but they don't mention it, so I assume the worst.]\n\nAlso, how is it able to get at typicality through this model? I think what's illuminating to note is that, out of the 27 broad categories of images in this dataset, 17 can be found in the 49-dimensional vector representation. Here's what I think is probably happening (in particular for these 17 categories):\n\n - if all three images are from different categories, then probably we're learning something about one of those other dimensions (hopefully);^[Another thing is that this step helps to situate the categories amongst each other.]\n - if two images are from the same category, and the third is different, then the same pair is most often picked (helping to form this particular dimension);\n - if all three images are picked, then I guess the odd one out is the one that's the least *typical*.\n\nHaving laid it out like so, I'm starting to get a little skeptical about the results: it almost feels like this is a little too crude, and the data is not sufficiently expressive for a model to be able to learn something that's not just trivial. Put another way, this almost feels like a glorified way of learning the categories – though, there's nothing necessarily wrong with that, since the (high-level) categories are obviously important.\n\nPerhaps it helps to consider the following generative model for images: suppose each image was represented by a sparse vector of weights, with many of the coordinates corresponding to broad categories. Set it up in a such a way that if you're in a broad category, then that dominates all other considerations (so it follows the pattern above). Then, simply run this odd-one-out task directly on these vectors, and see if you're able to recover these vectors.^[It almost feels like a weird autoencoder architecture...]\n\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[triplet-loss]: triplet-loss.md \"Triplet Loss\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/non-deep-networks/","title":"Non-deep Networks"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - paper\n---\n\n# Non-deep Networks\n\n - [arXiv](http://arxiv.org/abs/2110.07641)\n - [OR](https://openreview.net/forum?id=Xg47v73CDaj)\n\nOne common wisdom in the era of deep learning is that \"depth trumps width\". After all, we are not in the era of wide learning. Deeper networks seem to allow for more *expressivity* given the compositional nature of layers, holding the number of parameters fixed. From a theoretical perspective, we know that depth-2 networks are universal approximators, but the size of the network is exponential in the input dimension. I'm pretty sure that there has been work showing that dependence for deeper networks is no longer exponential in $n$, but who knows under what stringent conditions does this hold.\n\nMore recently, we've entered the era of **deep** learning, whereby the number of layers are in the hundreds, if not thousands. This feels like an altogether different regime, which I would call really-deep learning. This also gets closer to the question of scaling.\n\nAlso, note that a lot of the theoretical work talks about ReLU networks that are just MLPs. This is so far removed from what the models are like now – existing architectures are much more like compositions of *blocks* of computation, with only a handful of block types. It's very much like a circuit, except instead of electricity you have a flow of data/information, and the components are these blocks.\n\nContinuing with the circuit analogy, you can differentiate between parallel and series. However, I think this is where the usefulness of the analogy ends. It's more helpful to think about computer programs, where we have sequential and parallel computation. It's clear that almost all programming is done sequentially, as that's how you get composition and the power of compounding. Parallelisation is something that can help speed things up, but for the most part it's something that you do to take advantage of distributed resources (once you hit the limitations of the single core).\n\nWhere does this paper fit in? It takes aim at this very-deep learning regime, and asks the question: is it really necessary to have hundreds of layers? Can we achieve the same sort of performance with only a handful of layers instead?\n\n![](https://i.imgur.com/1ybpnql.png)\n\nTo be honest, I'm not *that* familiar with the ImageNet benchmark. I think what they're trying to argue is that they're replacing depth with width. But to me, seeing the performance of the single-stream already shows that you can already do quite a lot with 12 layers.\n\nActually, as one reviewer notes:\n\n> My major concern is that the authors argues contribution of parallel subnetworks, while I think the traditional RepVGG-wise blocks adopted in ParNet contributes significantly more to prediction accuracy. Even if ParNet adopts 1 branch only, ParNet's accuracy can still achieve 75% more, as \"ParNet-M-OneStream\" shown in Table 8. Comparison of Table 7 and Table 10 shows that the RepVGG blocks is significantly more important than adopted number of branches on improving accuracy.\n\n\n## Paper Specifics\n\n - A key architectural choice is that the *parallel* streams use different resolutions (I guess this is also why you get diminishing returns with more streams).\n - They argue that moving calculations to width will take advantage of multiple cores. Unconvinced by this.\n\n### Parallel Performance\n\n![](https://i.imgur.com/yhfImSa.png)\n\nTable 10: comparing \\# of branches against performance. What they find is that for a fixed number of parameters, the optimal number of branches is 3. This shows that all this \"embarrassingly parallel\" talk is only *embarrassing*. For one thing, the performance improvements are, in my eyes, incredibly marginal. \n\nSomething that they keep doing is to fix the number of parameters and then vary across various configuration parameters (depth, width, channels, streams). This is clearly the simplest way to keep the *capacity* of the model constant, but in this era of complicated architectures, it's unclear how fair this metric really is.\n\n### Ensembles\n\n![](https://i.imgur.com/UpRHFxS.png)\n\nTable 9: comparing ensembles against the same number of streams. Their conclusion for this ablation study is that their *parallel* configuration outperforms a simple ensemble. From my perspective, this makes me think that ensembles, which are the actual \"embarrassingly parallel\" operation, aren't all that bad. Granted, it's not a particularly efficient usage of parameters (and my intuition says that part of why networks do so well is that in some sense they are performing some kind of bagging or boosting).\n\nIt's not clear to me how they train the ensemble (multiple networks). I assume it's not the classic statistical way, which is to do *bagging*. Perhaps there's a canonical choice of ensemble method, but it's really annoying that this isn't specified.\n\n## Final Thoughts / Open Questions\n\n - My feeling is that moving to width is the new *feature engineering*. Yes it's probably possible to *architect* a complex neural network with only a few layers. But that's not really that interesting, because you've essentially *feature engineered* that architecture for that particular problem.\n   - What would make a compelling case would be if the application of extending width was some very simple, extensible procedure, i.e. actually \"embarrassingly parallel\".\n - Anyone thought of adding closed loops to a network? In some sense, this explosion of layers (and scaling), the point of it is to be able to let the neural network go do its thing. I guess the problem is that backprop no longer works. This feels a little like [[can-you-learn-an-algorithm]], where the idea is to the give a RNN more *runway* to be able to do more processing.\n   - My feeling is that loops might be possible as some sort of extension. In other words, you have the normal training with a huge dump of data, which requires backprop, and there are no loops there. But then you move into this later stage where it's much more single-data-at-a-time, and here it's no longer about backprop (what is the replacement then?), and you can now add some loops. This is encroaching on Hofstadter's \"I am a Strange Loop\" territory.\n   - Maybe (reflecting how we learn), what you want is something like unsupervised learning in the beginning (huge unstructured data), which allows you to train these generic processing units, to capture very high level things. And then, at the same time, you have the more systemic learning engine that contains loops and whatnot, and these use these generic units, and that is \"trained\" in a much more deliberate manner, without backprop, to allow for things like loops to work.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[can-you-learn-an-algorithm]: can-you-learn-an-algorithm.md \"Can You Learn an Algorithm\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/objection-detection-for-guis/","title":"Object Detection for GUI"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - gui\n  - machine learning\n---\n\n# Object Detection for GUI\n\n## Problems\n\nLarge in-class variance and similarity between classes: basically, a very difficult problem!\n\n - Large in-class variance:\n   - style is at the whim of the designer/application.\n   - in general (interestingly), each data point has the same style, but across samples you have nothing\n - High cross-class similarity:\n   - key here is that sometimes the difference between two GUIs is something very small. that being said, this is more about classifying different types of GUIs, which we don't really do right now.\n - Packed scenes:\n   - close quarters, dense – in stark contrast to natural object detection problems\n\nI think it's also important to highlight the difference between GUIs and natural images, because that's exactly where we can improve (or even cross-pollinate).\n\n1. sharp, pixel based (plus some shadows, but all computer generated)\n2. close quarter, dense\n3. no common style per class\n\n![examples](https://i.imgur.com/dPDbZRP.png)\n\n\n\n![table](https://i.imgur.com/N1pFCLT.png)\n\n"},{"fields":{"slug":"/","title":"Artifactual Neural Network"},"frontmatter":{"draft":false},"rawBody":"# Artifactual Neural Network\n\nHere be gradients."},{"fields":{"slug":"/stand-alone-self-attention-in-vision-models/","title":"Stand-Alone Self-Attention in Vision Models"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - paper\n---\n\n# Stand-Alone Self-Attention in Vision Models\n\n - [arXiv](https://arxiv.org/abs/1906.05909)\n\n"},{"fields":{"slug":"/triplet-loss/","title":"Triplet Loss"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n---\n\n# Triplet Loss"},{"fields":{"slug":"/journal/2021-12-24/","title":"2021-12-24"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - journal\n---\n"}]}}}