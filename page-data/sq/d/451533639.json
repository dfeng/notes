{"data":{"allMdx":{"nodes":[{"fields":{"slug":"/placeholder/","title":"This Is a Placeholder File for Mdx"},"frontmatter":{"draft":true},"rawBody":"---\ntitle: This Is a Placeholder File for Mdx\ndraft: true\ntags:\n  - gatsby-theme-primer-wiki-placeholder\n---\n"},{"fields":{"slug":"/ai-for-health/","title":"AI for Health"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - xAI\n - medicine\n - artificial_intelligence\n---\n\n# AI for Health\n\nsrc: [WHO](https://www.who.int/publications/i/item/9789240029200) guidance\n\n## Summary\n\nSix key (ethical) principles:\n\n1. *Human autonomy*: think of the humans! (also right to privacy)\n2. *Safety/well-being/public interest*: this is like the Asimov robot laws\n3. *Transparency/explainability/intelligibility*: self-explanatory\n4. *Responsibility/accountability*: I think this follows naturally from the previous point, but means there's \"points of human supervision\" (human-in-the-loop?)\n5. *Inclusiveness/equity*: fairness in a nutshell\n6. *Responsive/sustainable*: on-line, adaptive learning + sustainability w.r.t. the environment\n\nTo someone versed in the societal import of ML, I don't think there's too much in the way of surprises in this guidance document, though it does highlight a few things (worth repeating):\n\n - the differentiation between high and low-income countries, and the potentially widening gap in healthcare outcomes brought about by AI. while there's nothing inherently problematic about that, it does bring up the potential problem of a mismatch in the focus of problems (i.e. cardiovascular diseases and other lifestyle-based, chronic illnesses for high-income versus the more straightforward, brutal problems faced by low-income).\n - biased learning from data collected in the west is a key problem: we know very well that racial groups often have very different health outcomes for the same treatment\n - healthcare in other parts of the world are oftentimes much more holistic (i.e. Chinese Medicine?). how do we reconcile such traditions?\n - AI requires big data, which runs counter to fundamental privacy rights. this is where privacy-preserving measures will be key. on the other hand, the acquisition of such data in less scrupulous countries might be disastrous."},{"fields":{"slug":"/alone/","title":"Alone"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - society\n---\n\n# Alone\n\nvia [OpenCulture](http://www.openculture.com/2015/03/andrei-tarkovskys-message-to-young-people.html)\n\nThis is Tarkovsky:^[Of Stalker, Solaris fame.]\n\n> I don’t know… I think I’d like to say only that they should learn to be alone and try to spend as much time as possible by themselves. I think one of the faults of young people today is that they try to come together around events that are noisy, almost aggressive at times. This desire to be together in order to not feel alone is an unfortunate symptom, in my opinion. Every person needs to learn from childhood how to spend time with oneself. That doesn’t mean he should be lonely, but that he shouldn’t grow bored with himself because people who grow bored in their own company seem to me in danger, from a self-esteem point of view.\n\nAnd here are some thoughts:\n\n> ...our rapid-fire, pressure-cooker public discourse may grant us instant access to information—or misinformation—but it also encourages, nay urges, us to form hasty opinions, ignore nuance and subtleties, and participate in groupthink rather than digesting things slowly and coming to our own conclusions. It’s an environment particularly hostile to mediums like poetry, or the kinds of poetic films Tarkovsky made, which teach us the value of judgment withheld, and immerse us in the kinds of aesthetic experiences the internet and television, with their nonstop chatter, push to the margins.\n\nWhile reflecting on the [[misinformation-project]], I can't help but think about how here I am focusing on the technological solution, when there are much more insidious and difficult, existential questions of tectonic shifts in society causing us to be easier targets of disinformation."},{"fields":{"slug":"/benign-overfitting-in-linear-regression/","title":"Benign Overfitting in Linear Regression"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_paper\n - lit_review\n - interpolation\n---\n\n# Benign Overfitting in Linear Regression\n\nsrc: [@Bartlett:2020ck].\n\n- references:\n    - [slides](https://www.stat.berkeley.edu/~bartlett/talks/201908Microsoft.pdf), better exposition\n    - new paper that generalises this: \"Benign overfitting in the large deviation regime\", via [arXiv](https://arxiv.org/abs/2003.05838.pdf)\n- setting:\n    - linear regression (quadratic loss), linear prediction rule\n        - <mark>important</mark>: we are actually thinking about the ML regime, whereby there isn't just a *true* parameter, but that there is a *best* one by considering the expectation (which, under general settings, corresponds to what would be the \"truth\").\n        - <mark>important</mark>: they are considering random covariates in their analysis! I guess this is pretty standard practice, when you're calculating things like *risk*\n        - that is, you can think of linear regression as a linear approximation to $\\mathbb{E} ( Y_i \\,\\mid\\, X_i)$\n        - [blog](https://www.timlrx.com/2018/02/26/notes-on-regression-approximation-of-the-conditional-expectation-function/) with a good explanation: very standard classical exposition, showing that the conditional expectation is the optimal choice for $L_2$ loss, and the linear regression solutions is the best linear approximation to the conditional expectation (something like that)\n    - infinite dimensional data: $p = \\infty$ (separable Hilbert space)\n        - though results apply to finite-dimensional\n    - thus, there exists $\\theta^*$ that minimises the **expected** quadratic loss\n        - in order to be able to calculate expected value, they must be assuming some model (normal error)\n    - but they want to deal with the interpolating regime, so at the very least $p > n$, and probably >>\n    - \"We ask when it is possible to fit the data exactly and still compete with the prediction accuracy of $\\theta^*$.\"\n        - ~~Unless I'm being stupid, this line doesn't make any sense. If you fit the data exactly, then by definition you're an optimal solution~~\n    - the solution that minimises is underdetermined, so there isn't a unique solution to the convex program\n    - okay, so I think what they're trying to say is that, if your choice of $\\hat{\\theta}$ is the one that minimises the quadratic loss (= 0), and then pick the one with minimum norm (what norm), then let's see when do you get good generalizability\n    - \"We ask when it is possible to overfit in this way – and embed all of the noise of the labels into the parameter estimate $\\hat{\\theta}$ – without harming prediction accuracy\"\n- results:\n    - covariance matrix $\\Sigma$ plays a crucial role\n    - prediction error has following decomposition (two terms):\n        - provided nuclear norm of $\\Sigma$ is small compared to $n$, then $\\hat{\\theta}$ can find $\\theta^*$\n        - impact of noise in the labels on prediction accuracy <mark>(important)</mark>\n            - \"this is small iff the effective rank of $\\Sigma$ in the subspace corresponding to the low variance directions is large compared to $n$\"\n    - \"fitting the training data exactly but with near-optimal prediction accuracy occurs if and only if there are many low variance (and hence unimportant) directions in parameter space where the label noise can be hidden\"\n- intuition: I don't think they've provided much in the way of any intuition about what is going on, so let's see if we can come up with some\n    - this is weird because we're thinking of linear regression, and so basically it's a linear combination of the $X$'s to get $Y$.\n    - if we were to think about this geometrically, is that we have an infinite dimensional $C(X)$, with sufficient \"range\"/span that our $Y$ lands in $C(X)$.\n    - <mark>I feel like what they're trying to do is basically have the main true linear model, and then what you have are these small vectors in all sorts of directions, so that you can just add those to your solution, in order to interpolate (??)</mark>\n        - and since we're dealing with a model that is linear in truth, then\n        - I'm confused, because since both $x,y$ are random, then calculating the risk is over both of these things.\n- takeaway:\n    - basically, the eigenvalues of the covariance matrix should satisfy:\n        - many non-zero entries, large compared to $n$\n        - small sum compared to $n$ (smallest eigenvalues decays slowly)\n        - i.e. lots of small ones, probably some big ones (?)\n- summary: this is what I think is going on\n    - let's just start with $X$ and it's covariance matrix $\\Sigma$: what we have here is something, at the population level, that can be described by a few large eigenvalues, and then many, many small eigenvalues. The idea is that the $\\Sigma$ describes the distribution of the sampled $x$ vectors, and so the eigenvalues determine the variability in the direction of the respective eigenvector. However, there is no guarantee that there needs to be a rotation, so it is entirely possible that the variability in the $x$'s are concentrated on individual coordinates.\n        - ~~I incorrectly thought that what we have are random vectors, and so this spans all the coordinates, which makes it easier to extract the coefficients~~\n    - So what we need are very many small directions of $x$ (again, you can just align them to a coordinate)...\n    - The optimal thing would be this nice sort of decomposition, where the first few elements of $\\beta$ correspond to the true coefficients. Then, the point is that the error can be entirely captured by a linear combination of the rest of the covariates.\n    - ![](img/benign_1.png)\n    - This is a more explicit decomposition of the two terms\n    - ![](img/benign_2.png)\n    - The idea here is that, in some simplified sense, the error is just a random Gaussian vector, and we should be able to represent it exactly as a linear combination of a lot of small gaussian vectors."},{"fields":{"slug":"/artificial-generalised-intelligence/","title":"Artificial Generalised Intelligence"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - artificial_intelligence\n---\n\n# Artificial Generalised Intelligence"},{"fields":{"slug":"/calculus-for-brain-computation/","title":"Calculus For Brain Computation"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - neuroscience\n - biologically_inspired\n---\n\n# Calculus For Brain Computation\n\nsrc: [video](https://www.youtube.com/watch?v=_sOgIwyjrOA&feature=youtu.be), [pdf](https://simons.berkeley.edu/sites/default/files/docs/14061/rpassemblies.pdf)\n\n![How fruit flies remember smells](img/flies_smell.png)\n\n - olfactory *intelligence*: centring -> random projection (50 to 2000) -> sparsification\n - $\\mathbb{R}^{50} \\to \\mathbb{R}^{2000} \\to \\{0,1\\}^{2000}$ (sparsity at the 10% level, thresholding the top).\n\n![Random projection preserves similarity.](img/flies_project.png)\n\n - similarity is preserved by this (random-projection+threshold) procedure; similarity here defined by overlap\n   + not really sure what you're gaining though? ^[I guess, the idea is that you have a sparse representation (binary vector that can be captured by binary-firing neurons). perhaps storage, like with computers, just has to be in binary, so there's nothing particularly profound here.]\n\n - Calculus of the Brain\n   + interesting experiment: have a neuron only fire when you see Eiffel tower (vs house or Obama)\n     * then super-impose Obama onto Eiffel tower, see below\n     * now show Obama, and the neuron will fire (most of the time)\n   + what's going on?\n     * one way you can think of this is that there's the set of neurons that fire for Eiffel (memory of Eiffel), and similarly for other objects\n     * when you see two things together (learning relationships, causality, hierarchy), then what happens is that these two sets of neurons are now connected/merged\n     * but in order for this to make sense, the merge operation needs to be a little bit elaborate. basically you have to create the merged version (so like Eiffel+Obama), and perhaps that becomes the channel that connects the two things?\n   + this basically gives you something like a calculus on the brain, basically involving set operations on neurons\n\n![Ison et al. 2016 Experiment](img/obamatowerexperiment.png)"},{"fields":{"slug":"/bitter-lesson/","title":"Bitter Lesson"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_article\n---\n\n# Bitter Lesson\n\nsrc: [blog](http://incompleteideas.net/IncIdeas/BitterLesson.html)\n\n## Summary\n\nIn AI research, leveraging computation > incorporating domain knowledge. Given finite resources, it always pays to improve computation than to incorporate domain knowledge.\n\n> the only thing that matters in the long run is the leveraging of computation\n\nThis *bitter* lesson arises in part because of our anthropocentric view (that's not how we as humans solve *insert_difficult_task*!), biasing us towards more elaborate systems.\n\n> And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation.\n\nAlso, there's a trade-off, especially as the injection of human knowledge gets more elaborate, then there's usually a computation cost.\n\n### History\n\n - Chess: search-based approaches beat out leveraging human understanding of structure of chess\n - Go: key insight was *learning by self-play*, which enabled massive computation to be brought to bear\n - Speech Recognition: linguistics, we have the Unreasonable Effectiveness paper of Norvig\n   + More recently, the massive scale language models (like GPT-3) show that larger models/datasets outperform fancier architectures (and history repeats itself)\n\n> We want AI agents that can discover like we can, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done.\n\nEven though a lot of our knowledge is *guided*, for most of our general-purpose intelligence, it just happens naturally, so it should be similarly for building AI.\n\n## Thoughts\n\nI'm not as familiar with the AI game literature, but at least in the context of NLP/image classification and DL, there seems to be an ideal sweet-spot in terms of finding the right kind of architecture that's powerful enough to learn, but is simple enough that you can run it on an extreme scale. I think that's partly why we still continue to innovate on the model side. If we hadn't done so, then we wouldn't have gotten [[transformers]], which has been a boon for NLP.\n\nWhat is pretty clear is that domain-knowledge injection is not *scalable* (think expert systems back in the day). What's better are general-purpose methodologies, and the more general-purpose, the better (update: no longer think this relationship is linear: [[generalised-neural-networks]]). However, this would suggest that something like CNNs are actually *suboptimal*, since convolutions are definitely highly specific to image classification. But I think that's part of the allure of [[graph-neural-networks]].\n\nGiven the successes of CNN in image recognition, I suspect that for specific domain tasks, a little bit of domain knowledge can go a very long way (it's not like it's hard-coding edge detection, so it's still pretty general purpose).\n\nFinally, it feels like we just need to be better at *learning-to-learn* (\n[[meta-learning]]). I don't think it's necessarily a bad thing to take inspiration from human intelligence (since we're the only successful example). So, ultimately, I think the *bitter lesson* is really just a stop-gap until we can sufficiently narrow the emulation gap.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[transformers]: transformers.md \"Transformers\"\n[generalised-neural-networks]: generalised-neural-networks.md \"Generalized Neural Networks\"\n[meta-learning]: meta-learning.md \"Meta Learning\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/can-you-learn-an-algorithm/","title":"Can You Learn an Algorithm"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - from_paper\n---\n\n# Can You Learn an Algorithm\n\n - [arXiv](https://arxiv.org/abs/2106.04537)"},{"fields":{"slug":"/classification-vs-regression/","title":"Classification vs Regression"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - interpolation\n - statistics\n - machine_learning\n---\n\n# Classification vs Regression\n\n"},{"fields":{"slug":"/computer-vision-tasks/","title":"Computer Vision Tasks"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - computer_vision\n - biologically_inspired\n---\n\n# Computer Vision Tasks\n\nQuestion: what are the key components of a computer vision task?^[From thinking about convolutional layers and what makes them *specialized*.]\n\nThis question is inextricably linked to how our vision system works (evolved)."},{"fields":{"slug":"/dataset-meta-learning-from-kernel-regression/","title":"Dataset Distillation"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - from_paper\n  - machine_learning\n  - neural_tangent_kernel\n---\n\n# Dataset Distillation\n\n - src: [@nguyen_dataset_2021]\n\t - [slides](https://rosanneliu.com/dlctfs/dlct_210730.pdf)\n\nYou have [[knowledge-distillation]], the goal of which is to compress models down while retaining the same performance. A similar but orthogonal problem is to compress data down while retaining the same \"performance\" (an algorithm trained on this compressed dataset would reach the same performance as if it were trained on the original dataset).\n\nOne (classic) approach is [[coresets]], which seek to find a subset of the full dataset such that queries have approximately the same answer on both datasets. These kinds of summarisation techniques can also be framed in terms of optimising a submodular function (submodularity being the *discrete* analog of convexity), and there is a whole field of applying this topic to machine learning. Finally, one can use (very classic) unsupervised methods like $k$-means clustering to form cluster centers that can be used as representative samples. A problem with the first two methods is that the distilled dataset must be a subset, which might be restrictive (though sometimes being restrictive is a good thing?). The general problem with these aforementioned methods is that they are usually heuristic-based; as such, there are no guarantees on downstream tasks (validation loss) (again, however, this might actually be a good thing, in that we aren't too focused on a particular task).\n\nDataset distillation solves these problems by turning this into a learning problem, where we actually learn a new dataset so that it does well in the downstream task (i.e. these are the \"parameters\" that we're going to optimise over). This new dataset is not a subset of the original dataset (in theory it could be, but as we'll see they end up being very different).\n\nI feel like there's something interesting here for the psychologists. They're always looking to find the *archetypal* image for a class.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[knowledge-distillation]: knowledge-distillation.md \"Knowledge Distillation\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/debiasing-word-embeddings/","title":"Debiasing Word Embeddings"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - nlp\n---\n\n# Debiasing Word Embeddings\n\n## Resources\n\n - Paper: [arXiv](https://arxiv.org/abs/1903.03862)\n\n> Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between \"gender-neutralized\" words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.\n\n - Paper (causal inference?): [arXiv](https://arxiv.org/abs/1911.10787)\n - Paper: [arXiv](https://arxiv.org/abs/1909.06092)"},{"fields":{"slug":"/discretization-of-gradient-flow/","title":"Discretization of Gradient Flow"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - neural_networks\n - optimisation\n---\n\n# Discretization of Gradient Flow\n\nVia [[michael-jordan-plenary-talk]], to this paper on [arXiv](https://arxiv.org/pdf/1905.07436.pdf).\n\nYou have gradient descent, which you can show under convex problems to have a convergence rate of $\\frac{1}{t}$, whereas if you use Nesterov's accelerated gradient method gets $\\frac{1}{t^2}$.^[And this rate is entirely independent of the ambient dimension of the space.]\n\nIf you take the limit of the step sizes to zero, then you're going to get some kind of differential equation. This is *gradient flow*. It turns out you can basically construct a class of [Bregman Lagrangians](https://arxiv.org/abs/1603.04245), which essentially encapsulate all the gradient methods.\n\nYou can solve the Lagrangians for a particular rate, and then out pops an differential equation that obtains that rate in continuous time. What's curious is that the *path* is identical across all these ODEs. Essentially you're getting path-independence from the rate, which suggests that this method has found an *optimal* path, and you can essentially tweak how fast you want to go along that path.\n\nThis would suggest that you could then get arbritrary rates for your gradient method. But it turns out that the *discretization* step is where things break. In fact, Nesterov already has a lower bound on his rate of $\\frac{1}{t^2}$,^[The class of gradient methods are those that have access to all past gradients, I think.] so we know it can't do arbitrarily well. And it turns out that it does match the lower bound. The intuition is that the discretization suffers with curvature. If you go too quickly, then you're not going to be able to read the curvature well enough.\n\nIn other words, discretization is non-trivially different to continuous time. Which sort of makes sense, since in continuous time you have basically all the information.\n\nFinally, in relation to the [[penalty-project]], this doesn't actually work for things like Adam, which we know to be amazing in practice. So, it seems like there's still work left to understand why on earth the adaptive learning rates work so well.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[michael-jordan-plenary-talk]: michael-jordan-plenary-talk.md \"Michael Jordan Plenary Talk\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/effectiveness-of-normalised-quantities/","title":"Effectiveness of Normalised Quantities"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - matrix_completion\n---\n\n# Effectiveness of Normalised Quantities\n\nIn the context of matrix completion, it is known that the *nuclear norm*, being a convex relaxation of the rank,^[You can think of these as the matrix version of the $l_1$ to $l_0$ duality.] has very nice low-rank recovery properties. However, in sparser regimes (not enough observed entries), the nuclear norm starts to falter. This is because there is sufficient ambiguity (equivalently, not enough constraints on the convex program).^[Recall that the convex program is something like $\\min \\|W\\|_{\\star} \\text{ s.t. } P_{\\Omega}(W^\\star) = P_{\\Omega}(W)$, where the $P_{\\Omega}$ is the convenient operator to extract the observed entries.]\nThus, minimising the nuclear norm doesn't actually recover the true matrix.\n\nHowever, it turns out that simply normalising it works wonder.^[It's non-convex, though; here I am talking about running a gradient method to optimise this (and for some reason ADAM is far superior).] Consider the penalty, $L_{\\star/F}$, which is given by\n$$\n\\frac{L_1}{L_2} = \\frac{ \\sum \\sigma_i }{ \\sqrt{ \\sum \\sigma_i^2 } },\n$$\nwhere $\\sigma_i$ are the singular values. The idea here is that we're interested in the relative size of our singular values, not the absolute sizes. For instance, one way to decrease the nuclear norm penalty is to apply a dampening constant uniformly over the matrix entries. This doesn't change the rank (or effective rank, which usually care about relative sizes), and so such a direction shouldn't be promoted. This normalised penalty is invariant to this.\n\nAt the same time, it's still differentiable, so it's better than using the actual rank of the matrix. And in fact, in real-world settings, we don't care that a matrix is *effectively* low rank, even if it is not exactly low-rank. So in practice we would probably want to anyway use the effective rank as our actual measure.^[Here lies one difference between this problem and sparse linear regression: the low-rank object there is a parameter, whereas here it is the data. Thus, there's nothing particularly bad about assuming that the beta vector is exactly sparse.]\n\nCompare this to the gradient accelerator ADAM, which, in a nutshell considers a normalised gradient sequence. That is, the update step is given by^[Granted, I am hiding some details, like how the summation is actually weighted to prioritise the most recent gradient, the division is element-wise, plus some computation tricks to ensure no division by zero.]\n\n$$\n\\frac{ \\sum g_t }{\\sqrt{\\sum g_t^2}},\n$$\n\nwhere $g_t$ is the gradients of the loss. The reason I bring up these two quantities is that it turns out that only by using these two methods do we get something interesting coming out. It could very well be coincidence that they have a similar form, but I suspect that perhaps normalised quantities are somehow superior in the deep learning regime (and batch norm, see [[exponential-learning-rates]]).\n\n## Parallels?\n\nOriginally I thought this felt a lot like our scores in statistics, but upon further inspection I realised that this would only hold if you have assume zero mean, since the entries aren't actually centered (and no $\\sqrt{n}$). The problem there is that all the singular values are positive, so it seems sort of weird to say that this is essentially a $Z$-score of the null hypothesis that these were drawn from a zero-mean normal distribution.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[exponential-learning-rates]: exponential-learning-rates.md \"Exponential Learning Rates\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/embeddings-cant-possibly-be-right/","title":"Embeddings Can't Possibly Be Right"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_paper\n---\n\n# Embeddings Can't Possibly Be Right\n\nI was randomly reading a paper from a journal whose name sounded legitimate but actually is one of those predatory journals (and supposedly has an acceptance rate of 90+%, hence why I'm not linking to them). The authors raised something provocative which I thought was worth debunking:\n\nOne of core insights in (statistical) NLP is the idea of using embeddings (vectors in Euclidean space) to represent words (concepts). However, this seems to raise some *philosophical* quandaries; namely, now that we're in Euclidean space, which is equipped with a natural metric, and allows us to \n\n<Note>\nAside: distributional hypothesis vs distributed representation? I'm pretty sure I use these terms interchangeably.\n</Note>"},{"fields":{"slug":"/estimating-the-mean/","title":"Estimating the Mean"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - statistics\n  - theoretical_statistics\n---\n\n# Estimating the Mean\n\nsrc: [NeurIPS 2021](https://neurips.cc/virtual/2021/invited-talk/22279) Breiman Lecture by Gabor Lugosi\n\n"},{"fields":{"slug":"/exponential-learning-rates/","title":"Exponential Learning Rates"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_paper\n - lit_review\n - gradient_descent\n - optimisation\n - regularization\n---\n\n# Exponential Learning Rates\n\nvia [blog](http://www.offconvex.org/2020/04/24/ExpLR1/) and [@Li:2019tn]\n\nTwo key properties of SOTA nets: normalisation of parameters within layers (Batch Norm); and weight decay (i.e $l_2$ regulariser, [[explicit-regularization]]). For some reason I never thought of [[batch-norm]] as falling in the category of normalisations (see [[effectiveness-of-normalised-quantities]]).\n\nIt has been noted that BN + WD can be viewed as increasing the learning rate (LR). \n\n> When combined with BN, this implies strange dynamics in parameter space, and the experimental papers ([van Laarhoven, 2017](https://arxiv.org/abs/1706.05350), [Hoffer et al., 2018a](https://arxiv.org/abs/1803.01814) and [Zhang et al., 2019](https://openreview.net/forum?id=B1lz-3Rct7)), noticed that combining BN and weight decay can be viewed as increasing the LR.\n\nWhat they show is the following:\n\n> (Informal Theorem) Weight Decay + Constant LR + BN + Momentum is equivalent (in function space) to ExpLR + BN + Momentum\n\nThe proof holds for any loss function satisfying *scale invariance*:\n$$\nL(c \\cdot \\theta) = L(\\theta)\n$$\nHere's an important Lemma:\n\nLemma: A scale-invariant loss $L$ satisfies:\n\n$$\n\\begin{align}\n  \\langle \\nabla_{\\theta} L, \\theta \\rangle &= 0 \\\\\n  \\nabla_{\\theta} L \\mid_{\\theta = \\theta_0} &= c \\nabla_{\\theta} L \\mid_{\\theta = c \\theta_0}\n\\end{align}\n$$\n\nProof: Taking derivatives of $L(c \\cdot \\theta) = L(\\theta)$ wrt $c$, and then setting $c=1$ gives the first result. Taking derivatives wrt $\\theta$ gives the second result.\n\n![Illustration of Lemma](https://www.offconvex.org/assets/inv_lemma.png)\n\nThe first result, if you think of it geometrically, ensures that $|\\theta|$ is increasing.\nThe second result shows that while the loss is scale-invariant, the gradients  have a sort of corrective factor such that larger parameters have smaller gradients.\n\n## Thoughts\n\nThe paper itself is more interested in learning rates. What I think is interesting here is the preoccupation with *scale-invariance*. There seems to be something self-correcting about it that makes it ideal for neural network training. Also, I wonder if there is any way to use the above scale-invariance facts in our proofs.\n\nThey also deal with learning rates, except that the rates themselves are uniform across all parameters, making it much easier to analyze – unlike Adam where you have adaptivity.\n\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[effectiveness-of-normalised-quantities]: effectiveness-of-normalised-quantities.md \"Effectiveness of Normalised Quantities\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/explaining-word-embeddings/","title":"Explaining Word Embeddings"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_article\n - nlp\n---\n\n# Explaining Word Embeddings\n\nsrc: [blog](https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html)\n\nDefine $P(w|a)$ to be the conditional probability that given word $a$, the word $w$ is nearby to it (precise: 2 hops). Then, two words are *similar* if $P(w \\mid a) = P(w \\mid b)$ for every word $w$. That is, their neighbourhood is similar.\n\n$$\nPMI(a, b) = \\log \\left[ \\frac{P(a,b)}{P(a)P(b)} \\right] = \\log \\left[ \\frac{P(a|b)}{P(a)} \\right].\n$$\n\nWords close in the embedding space are often either synonyms (e.g. *happy* and *delighted*), antonyms (e.g. *good* and *evil*) or other easily interchangeable words (e.g. *yellow* and *blue*) (see [[signed-word-embeddings]]). An important distinction is that closeness here is about getting \"interchangeability\" not closeness in meaning, though they are often proxies.\n\n To get to analogies, let's define them through ratios, as below:\n\n - *a is to b is as A is to B* gives $\\frac{P(w|a)}{P(w|b)} = \\frac{P(w|A)}{P(w|B)}$\n - *dog is to puppy as cat is to kitten* gives $\\frac{P(w|dog)}{P(w|puppy)} = \\frac{f(w\\vert age=adult)}{f(w\\vert age=cub)} = \\frac{P(w|cat)}{P(w|kitten)}$\n\nThis suggests to the following *decomposition*:\n$$\n\\begin{aligned}\nP(w\\vert dog) &= f(w\\vert species=dog) \\times f(w\\vert age=adult) \\times P(w\\vert is\\_a\\_pet) \\\\\nP(w\\vert puppy) &= f(w\\vert species=dog) \\times f(w\\vert age=cub) \\times P(w\\vert is\\_a\\_pet) \\\\\nP(w\\vert cat) &= f(w\\vert species=cat) \\times f(w\\vert age=adult) \\times P(w\\vert is\\_a\\_pet) \\\\\nP(w\\vert kitten) &= f(w\\vert species=cat) \\times f(w\\vert age=cub) \\times P(w\\vert is\\_a\\_pet)\n\\end{aligned}\n$$\nThat's how the ratios end up being the same, because you're basically cancelling the shared \"hidden variables\".\n\n - [ ] recent paper exploring this further: [blog](https://carl-allen.github.io/nlp/2019/07/01/explaining-analogies-explained.html)\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[signed-word-embeddings]: signed-word-embeddings.md \"Signed Word Embeddings\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/fast-weights/","title":"Fast Weights"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - neural_networks\n - biologically_inspired\n---\n\n# Fast Weights\n\nsrc: ?\n\nOrdinary RNN/LSTM neural networks have two kinds of memory. The weights of the ordinary neural network can be thought of as the long-term memory,^[For some reason, I never thought of the weights as that. Actually, upon further inspection, it seems weird to me that we do this. When I think of a neural network and it's weights, I'm thinking usually about it as a compute engine, and these are just the current configuration that enables it to do things. Things are again getting a little philosophical/neuroscience, in that what exactly is being stored in our brains? Do we know how memory is stored?] while the *memory* hidden vector can be thought of as the short-term memory.\n\nRNNs have a hidden vector^[which in class I relate to HMMs, basically as latent variables that can also be thought of as some form of short-term memory.] which gets updated at each recurrent step, so it's pretty *flaky*. LSTMs on the other hand try to make this memory more sticky by computing *increments* to the hidden vector, thereby allowing memory to persist for longer.\n\nOne would expect that something in between these two kinds of memory might be useful (more storage capacity than short-term, but also faster than long-term). \n\nHere it feels like there should be some kind of recursive argument that gives some optimal number of *layers*, since you can always push the short term stuff onto the one-higher-level slot. I wonder if human brains have already adapted to have the optimal number of intermediate memory steps. Anyway, some kind of adaptive type of memory slot would be ideal.\n\nThe argument is made based on a temporal-scaling issues: working memory/attention operate on the timescale of 100ms to minutes. The brain, meanwhile, implements these intermediate short-term plasticity mechanisms via some weird *biology*"},{"fields":{"slug":"/from-nerf-to-kernel-regression/","title":"From NERF to Kernel Regression"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - neural_tangent_kernel \n - machine_learning\n---\n\n# From NERF to Kernel Regression\n\nWe can think of [[neural-representations]] as the composition of a set of some random fourier features/basis and an MLP (non-linear function). On the other hand, we have kernel methods, the gist of which is that you project your input space via a kernel into some more complicated, non-linear space, in the hopes that the resulting function to learn is linear. Meanwhile, there's theoretical work that shows that training infinitely wide neural networks corresponds to kernel regression with a particular kernel (NTK), I don't know where this fits in with everything just yet, but it's another piece of the puzzle.\n\nWhat we have is some sort of progression. Starting from MLPs (or generic neural networks), we have the most general architecture that doesn't use any sort of basis functions. Methods like NERF are the next step, starting with some random and small basis functions, not fully expressive enough to result in a linear function, but good enough for a small-sized MLP to resolve. At the other extreme, you have kernel methods, that basically do all the hard (non-linear) work up front with the kernels, leaving the last bit a trivial linear exercise. NTKs are a wrinkle in this description, as it seems to show equivalence of the two extremes, thereby closing the loop?\n\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[neural-representations]: neural-representations.md \"Neural Representations\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/generalised-neural-networks/","title":"Generalized Neural Networks"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - neural_networks\n - machine_learning\n---\n\n# Generalized Neural Networks\n\nRecently, there has been a flurry of work that seeks to build architectures that are domain-agnostic. Perhaps unsurprisingly, most of the work in this direction uses [[transformers]] at its core, given its (apparent) flexibility. The thesis is that domain-specific architectures work great in their particular niche, but the moment something changes, even slightly, then you're back to square one in terms of training, and square three in terms of architecture.^[A simple example is going from single images to stereo pairs, akin to how our own visual system works.] Wouldn't it be great, then, to have some generic architecture that just works across all these domains?^[In the words of LeCun, let the data speak for itself.]\n\nAt first blush, this seems like a no-brainer: you no longer have to build specialized architectures depending on the problem at hand, and there doesn't seem to be much downside. However, I think it's actually worthwhile to spend some time questioning this assumption.\n\n## The Bad\n\n### Visual Tasks\n\nLet's consider the domain of visual tasks, and in particular, the use of convolutions. While convolutions are clearly a form of inductive bias, I don't see any problem with using them as just one of several fundamental building blocks (something that cares mostly about local 2D statistics). I think stereo-pairs is a good working example here: with the current architectures, it is indeed the case that moving from single to stereo is a non-trivial architectural task that requires a choice (about how to fuse the two inputs).\n\nI also can't help but compare to how our visual system works, noting that despite our ability to handle generic visual tasks, part of the system utilizes convolutions. Granted, the fruits of evolution are not a justification in itself, but at the very least it's an existence proof that we needn't throw the baby out with the bath water on our pursuit of generalisability. And if we're going with biologically inspiration, I think stereo inputs should just be the de-facto, as that gives us the depth perception that we need to better understand the world. In terms of inputs, why not just subscribe to the inputs that we as humans have: stereo visual inputs, stereo audial inputs, and some complex input that is our nervous system.\n\nHaving said all this, I realise the problem is that I'm coming at this from the perspective of [[artificial-generalised-intelligence]] (hence taking inspiration from how we do things). The space of inputs that machine learning covers is much wider than what we (as humans) get as inputs. That doesn't negate my argument that it makes more sense to work with a few fundamental units (instead of just one), but at least I can see why it might not be as straightforward as just following evolution.\n\n### Efficiency\n\nConvolutions are a biologically inspired, incredibly efficient means of processing image data. My guess is that the upper bound on what these kinds of generalised neural networks can achieve is to essentially reinvent the wheel and reproduce convolutions. And perhaps future models are flexible enough to do that. For now, though, the best we can hope for is to achieve some sort of parity in performance, though at the (terrible) expense of efficiency and compute. Granted, I do think visual tasks are an easy target, since there already is a simple, obvious architecture which gets you very far. I'm less familiar with the audio world.^[Though in my brief stint, I remember that convolutions (just 1D) also played a role.]\n\n### Great Power, Great Responsibility\n\nLetting the data speak for itself comes with its own problems (and we're not even talking about the [[fairness-project]]). At a high level, this is similar to the problem of [[reinforcement-learning]] (paperclip): given a sufficiently powerful generic algorithm, it will find ways to achieve the goal assigned to it that are possibly counter to what we would like it to learn. That is, it will pick up on correlations and use those, operating under the assumption that correlation implies causation ([[causal-inference]]). A simple example is, when classifying a boat, a generic algorithm will use the whole image (e.g. presence of the sea) to help it to classify, maybe to the point where it doesn't even care about the specifics of the target object (see also [[computer-vision-tasks]]).\n\nIn some sense, with these kinds of generalised architectures, the inductive bias step is now hidden in the regularisation techniques. Now, it's probable that regularisation is actually where you want to inject these kinds of biases (as opposed to in the architecture). Again with relating it to humans, our generalised intelligence doesn't have the same kinds of problem, a simple reason being that our learnings are not the result of some simple loss function. That is, it's the fact that we're doing a \"simple\" non-convex optimization that leads to these kinds of quirky solutions.^[And if you think about it, no amount of injecting common-sense could really sidestep this fundamental flaw.]\n\n## The Good?\n\nWhen such generic architectures actually succeed, it provides fairly strong evidence that this particular architecture is a powerful fundamental building block, and perhaps should be used in the later layers of a generalised neural network. On the other hand, it's *almost* akin to saying that `NOT` gates are a powerful building block for computers – how much do we learn from such a statement in terms of understanding how computers work?\n\nBeing domain-agnostic seems like it would be \nMeanwhile, being input/output format agnostic lets you experiment with combining different input sources \n\nIt also speaks in part to the [[bitter-lesson]]. However, I think the regime here is a little different: there are \n\n## Literature\n\n### Perceiver\n\nsrc: [@jaegle2021perceiver]\n\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[transformers]: transformers.md \"Transformers\"\n[artificial-generalised-intelligence]: artificial-generalised-intelligence.md \"Artificial Generalised Intelligence\"\n[computer-vision-tasks]: computer-vision-tasks.md \"Computer Vision Tasks\"\n[bitter-lesson]: bitter-lesson.md \"Bitter Lesson\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/gpt3/","title":"GPT-3"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - nlp\n - deep_learning\n---\n\n# GPT-3\n\npaper: [arXiv](https://arxiv.org/abs/2005.14165)\n\nThis is a 175b-parameter language model (117x larger than GPT-2).\nIt doesn't use SOTA architecture (the architecture is from 2018: 96 transformer layers with 96 attention heads (see [[transformers]])^[The self-attention somehow encodes the relative position of the words (using some trigonometric functions?).]), is trained on a few different text corpora, including CommonCrawl (i.e. random internet html text dump), wikipedia, and books,^[The way they determine the pertinence/weight of a source is by upvotes on Reddit. Not sure how I feel about that.] and follows the principle of language modelling (unidirectional prediction of next text token).^[So, not bi-directional as is the case for the likes of BERT. A nice way to think of BERT is a denoising auto-encoder: you feed it sentences but hide some fraction of the words, and BERT has to guess those words.]\nAnd yet, it's able to be surprisingly general purpose and is able to perform surprisingly well on a wide range of tasks (examples curated below).\n\nIt turns out that GPT-2 required things to be fine-tuned in order to be able to be applied to various tasks. In the spirit of [[transfer-learning]], the GPT-2 can be thought of the base model, and then for a given task (with training data), you then train on that specific dataset.\n\nGPT-3 no longer requires this fine-tuning step, which is a **big** deal. The way it works now is that you give it some samples/examples, and then start *interacting* with it. In fact, that's it. The idea is that you're basically populating the *context window* (which is 2048 tokens), and then letting the model do its thing. That's basically the extent of the *fine-tuning*.\n\nThey use this window to differentiate between different levels of $n$-shot learning. So, zero-shot learning is when you don't provide any examples of question/answer pairs, one-shot is one example, and few-shot is ~10 examples (that enable you to fill up the context window). They show improved performance as you go from zero to one to few.\n\nSo what's going on here?\n\nLanguage models are an incredibly simple paradigm: guess the next word in a sequence. Its simplicity, however, belies the level of complexity required to achieve such a task.\nIt's also a little different from our typical supervised paradigm, as there is no truth (i.e. there is no such thing as a prefect predictor of text/speech).\nWhat you want is to create a language model that is indistinguishable from humans (i.e. the Turing test).\n\nWhat's sort of curious is that you can frame many problems as text (and many people do so online), and so in order for the language model to be able to predict the next word in a wide array of contexts, it sort of has to learn all these various contexts, so indirectly having sub-specialties. Case in point: GPT-3 is able to do pretty simple arithmetics, even without being explicitly told to do so.\n\nI think what is **key** is this notion of *emergent properties* that might be happening with large enough networks. It's like it's self-organising, almost.\nOne might be tempted to think that you're going to get *diminishing returns* as you increase the size of your model, as is typical of most models in statistics/ML (you get to saturation). ^[That being said, there are supposedly a non-trivial collection of papers that investigate this *scaling* question: [link](https://www.gwern.net/newsletter/2020/05#fn8).]\nAnd yet, it seems like the opposite is true. With a general enough architecture (self-attention or convolutions), these things are able efficiently distribute their *knowledge* to better predict the output.\n\n - Disentanglement supposedly emerges in [StyleGAN](https://arxiv.org/abs/1812.04948) with sufficiently deep $w$ embeddings.\n - there is something about [[fast-weights]] that I don't really understand\n\n## Examples\n\nAll of these examples basically start by giving example of question/answer pairs.\n\n - English to Command Line arguments ([video](https://cdn.openai.com/API/English_Bash_Python.mp4))\n   + what I find most interesting about this example, and others, is the way in which it is able to adapt to the requests that you make of it. there's nothing particularly difficult about regurgitating what the command line code for print working directory is. what's surprising is that it understands what you mean when you try to correct it, or update it\n - Dialogues ([gwern](https://www.gwern.net/GPT-3#miscellaneous-dialogues))\n   + this is more in the realms of the Turing Test, but it's pretty shocking how it is able to maintain a pretty high-level/meta conversation about the nature of intelligence.\n - Gary Marcus Issues ([gwern](https://www.gwern.net/GPT-3#marcus-2020)):\n   + ML dissident Marcus has an [article](https://thegradient.pub/gpt2-and-the-nature-of-intelligence/) arguing that GPT-2, and language models in general, are not capable of any form of intelligence. and he proposes some questions to ask the language model, which gwern finds GPT-3 is able to answer in a satisfactory manner.\n\n## Fairness\n\nThis is a research direction on their waitlist:\n\n> Fairness and Representation: How should performance criteria be established for fairness and representation? How can responsive systems be established to effectively support the goals of fairness and representation in specific, deployed contexts?\n\nSo far we've seen people explore the biases inherent in word embeddings and language models; GPT-3 will inevitably fall into the purview of such scrutiny. Is there something manifestly different now, with GPT-3, that makes it a much more interesting question, since it feels as if GPT-3 is able to perform some low-level kinds of reasoning.\n\nIn the more basic models, you could easily attribute all the inherent biases to a reflection of the state of the world learnable through text.\n\nMy feeling is that this is slowly encroaching into the problem of how to equip artificial intelligence with notions of morality (I wonder if anyone has asked ethical questions---probably), though more importantly, might require you to actively force it to take a stand on certain kinds of topics.\n\n## Ideas\n\n - it seems to me that the missing link is the #reinforcement_learning type self-learning aspect. like, right now, the weights of GPT-3 are basically set. but it would be 1000x more worrying if it was talking to itself, or learning from all the interactions we are doing with it. that feedback mechanism feels like it would be the missing picture.\n\n## Resources\n\n - https://www.gwern.net/GPT-3\n - https://www.gwern.net/newsletter/2020/05#gpt-3\n - https://www.lesswrong.com/posts/6Hee7w2paEzHsD6mn/collection-of-gpt-3-results\n - https://www.lesswrong.com/posts/L5JSMZQvkBAx9MD5A/to-what-extent-is-gpt-3-capable-of-reasoning\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[transformers]: transformers.md \"Transformers\"\n[fast-weights]: fast-weights.md \"Fast Weights\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/graph-network-as-arbitrary-inductive-bias/","title":"Graph Network as Arbitrary Inductive Bias"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_article\n - neural_networks\n - graph_neural_networks\n---\n\n# Graph Network as Arbitrary Inductive Bias\n\n[src](https://blog.paperspace.com/introduction-to-geometric-deep-learning/).\n\nThe architecture of a neural network imposes some kind of structure that lends itself to particular types of problem (CNN, RNN). Thus, you can think of this as some form of *inductive bias*. An interesting view of [[graph-neural-networks]] is that essentially these provide arbitrary inductive bias, since the goal is to learn the architecture?\n\n| Component | Entities      | Relations  | Inductive Bias | Invariance      |\n|:----------|:--------------|:-----------|:---------------|:----------------|\n| FC        | Units         | All-to-all | Weak           | -               |\n| Conv.     | Grid elements | Local      | Locality       | Spatial transl. |\n| Recurrent | Time          | Sequential | Sequentially   | Time transl.    |\n| Graph     | Nodes         | Edges      | *Arbitrary*    | V,E permute     |\n\nFrom [@Battaglia:2018vi]"},{"fields":{"slug":"/ideas-around-interpolation/","title":"Ideas Around Interpolation"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - interpolation\n - idea\n---\n\n# Ideas Around Interpolation\n\nDump of thoughts from dream:\n\n - if my theory about interpolation is correct, it would suggest that even if you interpolate your training points, you should be able to distinguish those points that are somehow *outliers*, mostly from the curvature around that point. that seems to give some notion of *certainty*?\n - it almost feels like there should be a way to decompose neural networks in such a way as to start with the low-order functions and move towards higher-order functions. however, that just gets us back to the more classic ML style algorithms that depend on fitting basis functions to the data. what is nice about those algorithms is that you get this decomposition into signal and noise, sort of.\n - my feeling is that most of how I think about things is from a *regression* perspective, even though it's probably the case that the best wins for neural networks is still in classification (actually, [[neural-representations]] is technically regression).\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[neural-representations]: neural-representations.md \"Neural Representations\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/implicit-regularisation/","title":"Implicit-Regularisation"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - implicit_regularisation\n---\n\nIn the spirit of [[envisioning-the-future]], let's think about the key questions in the area of implicit regularisation.\n\nThe whole impetus for implicit regularisation is an attempt to explain the phenomenon of how neural networks rarely (if ever) overfit, even without explicit penalties. Or at least that's my interpretation of the goal. Having understood the exact mechanisms, one would hope that the insights should lead to more efficient algorithms.\n\nIn fact, that's exactly the hope that [[penalty-project]] provides: in a very simple example, we see that we can replace implicit by explicit, to great effect.\n\nIn some sense, it's all about trying to understand why deep learning is so effective, but there's this *implicit* idea that somehow there are these biases, particularly in the choice of the gradient algorithm as well as the loss function itself, that are leading to choosing better generalising minima.\nIn other words, this is basically one approach to explaining the generalisation conundrum.\n\nLooking at the literature, people have been focused on trying to show that gradient descent is favouring solutions with smaller norms (and thankfully, solutions with smaller norms have better generalisation performance).^[In fact, this was the impetus for looking at some other form of low-rank-ness, as I felt that it just so happens in many cases that good generalisation equates to low norm/minimum rank, say.]\n\n - One conclusion might be that the effects of implicit regularisation are a *black-box* that cannot be replicated by explicit regularisers. But I think that would be a sad conclusion, as it would essentially say that there is basically no use to all this research.\n - I hold the more hopeful view that this analysis should lead to better training and understanding generalisation, by replacing such black boxes with explicit renditions.\n - Another possibility is that a lot of the things that we've concluded about implicit regularisation are only applicable in the narrow problem of matrix completion. Though I think you see similar things in linear regression right? So I guess that shouldn't be true.\n - An important question is how things translate to more general settings, when you're dealing with non-linear.\n\n - I really think that one of the lessons of deep learning is the [[unreasonable-effectiveness-of-adam]] (or [[effectiveness-of-normalised-quantities]]). That is, you should always pick normalised terms over convex terms, because convexity is overrated.\n\n## Story So Far\n\nBy focusing on linear neural networks, we can disentangle the over-parameterisation effect from the expressivity of the model. Thus, what can we say about over-parameterisation? It appears that, for any sort of problem where the weights form a full-product matrix (so matrix completion, matrix sensing), then running GD will favour low-rank matrices.\n\nIt is very tempting to say that there is some penalty being penalised.\n[@Arora:2019ug] show that the nuclear norm is not what's going on, since they cleverly find a situation where the nuclear norm minimiser does not agree with the minimum rank minimiser.\nWe show that actually the normalised nuclear norm does the trick.\n\nThe key thing here is that, we don't believe that over-parameterisation is equivalent to this explicit regulariser. In fact, there seems to be some accelerative effect going on, or some kind of smart normalising thing, so clearly that too cannot be captured by a simple norm alone.\nWhat we want to show is that this is the most *reasonable* proxy (and it has nice gradient properties).\n\nRecent paper [@Razin:2020up] suggests that the implicit regularisation is actually the *rank* itself. This doesn't really make much sense: the rank is discrete, and as such is not particularly useful. If on the other hand you decide to look at its continuous version (like effective rank), then it's just a question of which proxy do you pick, and at that point you might as well just pick ours, since you can actually take gradients.\n\n## Next Steps\n\nThe whole purpose of this, as we recall, is to say something about generalisation for deep learning, which is complicated by various architectural features and non-linear components. In such cases, it is no longer possible (I don't think?) to disentangle the over-parameterisation from the expressivity. But my feeling is that this is just a technical nuisance. The spirit should still remain.\n\nIn our very special setting, we can show exactly what effect over-parameterisation is doing, and that such a method perfectly solves the problem (so there's basically no tuning/initialisation worries).\nHowever, once you move into more complicated models, my feeling is that this *surprise* that deep learning does so well in terms of generalisation is no longer such an exact/perfect situation. What I mean is that there's a lot of tuning going on in the background to make these things generalise, but the surprise is still there: it's still surprising that with just a little bit of tweaking, you're able to find the solution that does generalise.\n\nThe point of this is that as you get into more complicated models, this generalisation performance is more of an *art*, and, as such, we should expect that the resolution to the conundrum should also be more *artful* than the precise, simple, explicit way that was laid out in the cited papers.\n\n```{remark}\nWhat would be great would be some kind of *information* or *sparsity* measure that holds for general models, but when specialised down to linear neural networks, collapses to the singular values of the product matrix.\n```\n\nOne of the worries that I have is that all of this just doesn't really generalise past linear neural networks, for the simple reason that a lot of the success of deep learning comes from these carefully constructed architectures for a specific data type, which suggests that there is something inherent in the data itself (some inductive bias in the distribution of pixels, for instance), and which this kind of theory has no way of capturing. ^[This was inspired by the last paragraph of this blog [article](http://www.offconvex.org/2018/02/17/generalisation2/): \"I don’t see how to use the above ideas to demonstrate that the effective number of parameters in VGG19 is as low as 50k, as seems to be the case. I suspect doing so will force us to understand the structure of the data (in this case, real-life images) which the above analysis mostly ignores.\"]\n\nOr, another way to put this is that, these previous results suggest that any old over-parameterised neural network should have all these nice sparsity-inducing, acceleration-type properties. Though, maybe the key ingredient is: some sort of alignment with the kinds of sparsity that the data possesses.^[This must somehow relate to the whole sparsity of image data in the Fourier domain.] In other words, this notion of convolution (which I imagine was biologically inspired) might already have been optimised such that gradient methods^[hmmm, but biology doesn't work via gradients, so not sure if this makes sense] produce sparsity of the right kind.\n\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[effectiveness-of-normalised-quantities]: effectiveness-of-normalised-quantities.md \"Effectiveness of Normalised Quantities\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/invariant-risk-minimisation/","title":"Invariant Risk Minimisation"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - machine_learning\n---\n\n# Invariant Risk Minimisation\n\nsrc: [@arjovsky2019invariant]."},{"fields":{"slug":"/kahneman-neurips-perception/","title":"Perception"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - neurips\n  - psychology\n---\n\n# Perception\n\n - top down vs bottom up (optical illusions)\n - same data, different representations given context\n - AlphaGo's structure has parallels with system-1,2 thinking\n   - the Monte Carlo tree search is system-2: has all the logic, slow, deliberate\n   - however, you can't just exhaustively search the tree, so you need a neural network to have gain some intuition/heuristics about where to go in the tree, and that's system-1\n - \n - GPT-3 ≠ system-1: heuristics are borne out of forming representations/models that are internally coherent\n   - i.e. they find patterns, but they don't actually have \n   - *absurd mistakes*: 2-dimensional data, object-permanence, basically \n - system-1,2:\n   - framed mainly for the benefit of the populace, as people understand *agents*.\n   - in fact, it's more categorizations of mental processes, and some are slower and more deliberate than others\n - the example of simple shapes moving around: clearly we're proscribing agency and come with all this model baggage even for such simple data\n - system-1 is what happens 95% of the time, until you hit something surprising, not coherent, at which point it triggers system-2.\n - system-2 can be thought of as an editor (filter) for system-1.\n - distinction between *doubt* and *surprise*:\n   - instead of continuously predicting what is happening next, you see what happens and then make sense of it\n   - the idea here is that you have your system-1 that just keeps running and continuously checking the data in a very straightforward manner. but then once something weird happens, your attention is drawn.\n   - much more economical (?)"},{"fields":{"slug":"/knowledge-distillation/","title":"Knowledge Distillation"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - machine_learning\n---\n\n\n# Knowledge Distillation\n\nAs a constructive example, think of two similar classifications (two types of breeds of dogs). With one-hot encoding, these are orthogonal, and each class is essentially equivalent/exchangeable. However there's actually more structure to these classifications (you can imagine them being in some embedding space, and the dog classes should be closer together). I guess the phrase is *label similarity*.\n\nThe softmax probabilities is (hopefully) a proxy for this. So, in part, the student gets the shortcut of learning these class representations, but also conceivably it does not have the capacity to *learn* these subtleties, though it is able to mimic it.\n\nDoes Knowledge Distillation really work? [arXiv](https://arxiv.org/pdf/2106.05945.pdf)\n\n - \n\nKD as Semi-parametric Inference [Youtube](https://www.youtube.com/watch?v=dEE3-g_8dWo)\n\n - \n\nMany weird and curious results in this field:\n\n1. It turns out that if you do self-distillation (i.e. the student is the teacher, there's a Buddhist joke here somewhere), then the student will oftentimes outperform the teacher. [arXiv](https://arxiv.org/abs/1805.04770)."},{"fields":{"slug":"/learning-dags/","title":"Learning DAGs"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_paper\n - causal_inference\n---\n\n# Learning DAGs\n\nsrc: [blog](https://blog.ml.cmu.edu/2020/04/10/learning-dags-with-continuous-optimization/) version of paper.\n\nLearning a DAG is essentially a causal_inference problem, with the key being that the graph has to be *acyclic*. But this type of problem is generally NP-hard. These people came up with a continuous version of the constraint, taking advantage of matrix exponentials."},{"fields":{"slug":"/machine-learning-apis/","title":"Machine Learning APIs"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - machine_learning\n - MLOps\n---\n\n# Machine Learning APIs\n\nsrc: [@chen2021did], [OR](https://openreview.net/forum?id=gFDFKC4gHL4)\n\nMachine Learning APIs are weird. Usually we deal with APIs because they are well-defined and predictable (and oftentimes deterministic, i.e. a [pure function](https://en.wikipedia.org/wiki/Pure_function)). Ignoring the actual API scaffolding here (the request/response schema), which I assume would follow API convention, the key output of the API, the predictions from the ML model, are incredibly"},{"fields":{"slug":"/market-for-lemons/","title":"Market for Lemons"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - economics\n - from_article\n---\n\n# Market for Lemons \n\nvia FT's [Free Lunch](https://app.ft.com/cms/s/db917987-63af-42b9-b4a7-9f91ff648ce1.html).\n\nNobel Laureate George Arkerlof's paper on \"The Market for Lemons\"\n\n> It was a pioneering analysis of how markets fail to achieve efficient transactions when consumers have worse information about the product than the sellers. In the used-car example, because some unscrupulous used-car traders try to pass off “lemons” as “peaches”, consumers are aware that not all cars are what they seem to be, and discount the value they are willing to put even on the cars of perfectly respectable sellers. Because it is hard to distinguish lemons and peaches in the dealership yard, all used cars are tarnished by this risk.\n\nI talked about this in my Introduction to Economics class, on a simple example of inefficient markets. Interestingly, you can think of #misinformation along the same lines, as a *marketplace of ideas*, as well as recent riots:\n\n> These provocations are, therefore, of a piece with social media propaganda campaigns that spread misinformation. Again, trust is the casualty: the effect of misinformation is not so much to spread false beliefs as to eliminate confidence in truthful ones. As the title of a book by Peter Pomerantsev captures so well, the outcome is that “nothing is true and everything is possible”.\n\n> We should understand the effect of agents provocateurs in America’s equal justice protests in the same way. The effect of seeding vandalism and rioting amid the protests is to make it hard to distinguish legitimately angry but peaceful protesters from violent mobs. As a result all are tarred with the same brush."},{"fields":{"slug":"/meta-analysis-vs-preregistration/","title":"Meta Analysis vs Preregistration"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_podcast\n - meta_analysis\n - statistics\n---\n\n# Meta Analysis vs Preregistration\n\nsrc: [Expertise of Death](http://www.theblackgoatpodcast.com/posts/the-expertise-of-death/), Black Goat Podcast\n\n>  How important is expertise in conducting replications? Many Labs 4 was a large, multi-lab effort that brought together 21 labs, running 2,220 subjects, to study that question. The goal was to compare replications with and without the involvement of the original authors to see if that made a difference. But things got complicated when the effect they chose to study – the mortality salience effect that is a cornerstone of terror management theory – could not be replicated by anyone.\n\nThe somewhat crazy thing is that a comprehensive meta-analysis was conducted for terror management theory, via [@Burke:2010ij]. In particular, it was \"164 articles with 277 experiments\". That's a lot of experiments. Things like funnel plots don't help either.\n\n![](img/terror_funnel.png)\n\nThe important question is: are there any meta-analysis techniques that would be able to flag something wrong with these experiments? I'm pretty sure the answer here is no. Though, I would loved to be proved wrong. As one of the podcasters remarked, doing a meta-analysis cannot beat doing a large preregistered study.\n\nThe problem with meta-analyses is that they assume that the $p$-values are drawn from the true distribution, without any bias or problems with the experimental design. But that's an unreasonable assumption. What is the point of a meta-analysis if not to help combat this replication crisis? I thought the point was that you're able to detect cases of $p$-hacking."},{"fields":{"slug":"/meta-learning/","title":"Meta Learning"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - machine_learning\n - meta_learning\n---\n\n# Meta Learning\n\nsrc: [thesis](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-105.pdf)\n\n- relates to [[transfer-learning]], except you're trying to do an even better kind of pre-training, that generalises across all tasks\n- intuition:\n    - you want to train a NN so that it does well on a whole class of tasks, and not specific to one dataset\n    - you want to \"pre-train\" a NN such that, regardless of the task you eventually train it on, it'll be very quick to \"train\"\n        - speed of training is a proxy for \"close-ness\" of the solution\n        - if you're learning a useful representation, then this pre-trained model should speed up training\n        - <mark>it seems to me that this doesn't necessarily hold: if you think of training a NN as \"learning\", then this is like starting with knowledge of \"fundamentals\". but I don't think the analogy works very well, since it's really just the \"initialisation\", and it quickly gets \"replaced\".</mark>\n            - I guess what I'd like is some kind of **augmentation**\n            - a little more like how LSTMs have a memory slot\n- then they're going to use gradient updates, somehow\n- my initial guess:\n    - suppose you have a set of datasets. in some sense, what you want is to find the mid-point of all the trained models. you could just take one step per dataset (or sampled)\n    - but will this *converge*?\n        - it might be the case that the steps you take are in the opposite direction?\n            - though, if that's the case then that just means you've reached an \"impasse\" (or that there's nothing shared between them)\n- *The process of training a model’s parameters such that a few gradient steps, or even a single gradient step, can produce good results on a new task can be viewed from a feature learning standpoint as building an internal representation that is broadly suitable for many tasks.*\n- *From a dynamical systems standpoint, our learning process can be viewed as maximising the sensitivity of the loss functions of new tasks with respect to the parameters*\n- they have some theory that shows that a single gradient-step can get you to any function (??) – this seems impossible"},{"fields":{"slug":"/michael-jordan-plenary-talk/","title":"Michael Jordan Plenary Talk"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_talk\n - statistics\n - economics\n - data_science\n---\n\n# Michael Jordan Plenary Talk\n\nOur MJ gave a plenary talk at the SIAM conference on the mathematics of DS ([link](https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=69238)).\n\n## Summary\n\nThe next set of (interesting) questions from machine learning will be \"decision-based\". Consider recommendation systems: what if you recommend everyone the same restaurant (relevant [article](https://hdsr.mitpress.mit.edu/pub/2imtstfu/release/6))?^[Similar problem to route suggestions in Maps.] This is untenable, and so maybe we should take a leaf from Engineering and do some type of *load-balancing*?\n\nOr, even better, why not just create a market. Here we have the intersection between machine learning and microeconomics. The essential issue here is *scarcity*, which therefore requires a market. The question is then how do algorithms fit into this? One could take inspiration from matching algorithms (e.g. Match day for medical students). However, these require known preferences – usually in big data settings preferences are inferred. One could imagine adopting a bandit framework to do so.\n\nIn the second half, something more theoretical and unrelated: [[discretization-of-gradient-flow]]\n\n## Thoughts\n\nThis take is somewhat similar to what I'm going for with my [[fairness-project]], except that I don't think of markets as the solution. The general idea is that we usually think of algorithms as doing something on a single data point – in some sense the actions performed as a result of the machine learning model exist in parallel universes, independently. However, the reality is that we have scarcity, whether it be college placements in the fairness problem, or restaurant seats in the recommendation problem. And so we have to start incorporating this detail into our implementations.\n\nIn both cases we're also thinking about *feedback mechanisms* (a little like [[reflexivity]]).\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[discretization-of-gradient-flow]: discretization-of-gradient-flow.md \"Discretization of Gradient Flow\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/monopoly-in-tech/","title":"Monopoly in Tech"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - money_stuff\n - finance\n---\n\n# Monopoly in Tech\n\nThis is quintessential Levine: taking a concept that seems pretty straightforward at first blush (the worry that tech companies are becoming monopolies – duh, right?), and using very simple words and ideas to make you slowly realise that you haven't thought this through, clearly.\n\nI aspire to create content like this.\n\n> **Bigness**\n> \n> Some banks are very big. Some people think this is bad. There is a traditional way to say “it is bad that this company is so big,” and that way uses the word “monopoly.” “This company is so big that it is a monopoly, which is bad.” It is useful to be able to say this, because the government has a lot of power to limit monopolies, to regulate their behavior and break them up. \n> \n> It is not, however, particularly true of the big banks. A monopoly is a specific thing, a company that is so big that it dominates its market and can force out competitors and raise prices. The markets in which banks compete are, for the most part, extremely competitive.[5] If you want a mortgage, you pretty much pay the market rate for mortgages; JPMorgan Chase & Co. can’t charge you whatever it wants.\n> \n> Still, people think it is bad that the banks are so big, for other reasons. They worry about risk concentration, about banks that are “too big to fail” taking too many risks and the taxpayers bearing those risks, about too much centralization of banking making it more fragile, about banks that are “too big to manage” doing dumb things and crashing the financial system. Those worries are controversial, but never mind that. Assume for now that they are correct. What should the government do about them?\n> \n> One possibility is that the government’s antitrust regulators — at the Justice Department and the Federal Trade Commission — should go after the biggest banks for antitrust violations. The regulators could say “you are too big, you are a monopoly, we need to break you up into smaller pieces.” And then the banks would say “no,” and they would go to court, and the regulators could try to prove that the big banks are monopolists. And this would be hard to do, because they basically aren’t. It wouldn’t be impossible, though, I guess, because they are in lots of businesses and some of them are less competitive than others and there are probably some bad emails somewhere and so forth. The regulators’ odds of breaking up the big banks on antitrust grounds wouldn’t be zero. But they would be low.\n> \n> The other possibility is that other government regulators should, in setting other regulations, take bigness into account and try to regulate and discourage it. Conveniently banking is a very regulated business, and there are regulators and prudential supervisors who can do all sorts of meddling in a bank’s business. So for instance if you worried that giant banks could be “too big to fail” and pose a systemic risk to the financial system, the banks’ capital regulators could put out a rule saying “very big banks need to have more capital to offset the higher risk they pose to the financial system.” That would both reduce the risk of big-bank failure and also create an incentive for banks to stay smaller or break themselves up. And in fact there is such a rule, for exactly those sorts of reasons; it is called the “G-SIB surcharge.”\n> \n> Or if you worried that giant banks could be “too big to manage” and do dumb things, then the banks’ supervisors could tell a big bank that did a dumb thing “you can’t get any bigger until we’re satisfied you won’t do more dumb things.” They can just do that! The supervisors can just tell a bank not to get bigger, and it has to listen! They actually did it to Wells Fargo & Co., it’s kind of amazing. The theory wasn’t “Wells Fargo is a monopoly”; it was just “we don’t like what Wells Fargo has been up to so it can’t get any bigger.”\n> \n> I should emphasize that banking is a very regulated business, and the government doesn’t have quite as many levers to pull with most other businesses. Still lots of businesses are regulated in lots of ways, and the same general principles apply:\n> \n> If you think it is bad that a business is big, because it has a monopoly, sure, have the antitrust regulators go after it for antitrust violations.\n> If you think it is bad that a business is big, for other reasons, have other regulators try to limit its bigness in ways that directly address those other reasons.\n> In a pinch, if you think it is bad that a business is big, you could always have the other regulators try to limit its bigness in ways that don’t relate in any particularly logical way to those reasons. If you think that the bigness of social media companies is bad because they spread misinformation and undermine democracy, that is not really an antitrust problem,[6] and there is not exactly a Federal Truth Regulator that can promulgate misinformation rules. But maybe you can find some regulatory regime to shoehorn into that purpose. Maybe you’ve got a regulator in charge of, I don’t know, internet bandwidth or wireless spectrum or electricity usage or truth in advertising or whatever,[7] and you tell that regulator to turn up the heat on big social media companies. Not because you care about their electricity usage or whatever, but just to deter them from being big, because you think their bigness is bad.\n> If all else fails, you can have the antitrust regulators try to break up the business because it is too big, even though it isn’t a monopoly. That may not work though.\n> We talked yesterday about Facebook Inc. Lots of people think it is bad that Facebook is so big, but it is a little hard to put that in traditional monopoly terms. Is the problem of Facebook’s bigness that it can charge users monopolistically high prices for posting on Facebook? No; posting on Facebook is free. Is the problem that it can charge advertisers monopolistically high prices for advertising on Facebook? No. I don’t know if it can; I just know that I have never heard anyone make that complaint: If you don’t like Facebook, it’s not because you worry about advertisers overpaying.\n> \n> Is the problem of Facebook’s bigness some other form of anticompetitive behavior that makes consumers (Facebook users) worse off? Sure, maybe; intuitively I suspect Instagram would be a nicer place if it was still independent than it is under Facebook’s ownership. But this stuff is a little hard to articulate, which is why the FTC failed to articulate it: It sued Facebook for antitrust violations, and this week a judge dismissed that lawsuit for failing to even say why Facebook might have a monopoly. “It is almost as if the agency expects the Court to simply nod to the conventional wisdom that Facebook is a monopolist,” wrote the judge. The conventional wisdom is that it is bad that Facebook is so big! But that does not make it a monopoly in the technical, legal sense of the term.\n> \n> I suspect the main problem most people have with Facebook’s bigness is not about consumer choice but rather about Facebook’s political and social influence. You could imagine addressing that more directly than with antitrust law. And there have been suggestions for doing so — repealing Section 230, using election law to regulate Facebook, etc. — though I can’t say any of them strike me as great. Still it’s the right basic idea. Figure out what you don’t like about Facebook’s dominance and then regulate that, rather than just equating bigness with antitrust.\n> \n> Anyway here’s this:\n> \n> > The Biden administration is developing an executive order directing agencies to strengthen oversight of industries that they perceive to be dominated by a small number of companies, a wide-ranging attempt to rein in big business power across the economy, according to people familiar with the plans.\n> > \n> > The executive order, which President Biden could sign as soon as next week, would direct regulators of industries from airlines to agriculture to rethink their rule-making process to inject more competition and to give consumers, workers and suppliers more rights to challenge large producers.\n> > \n> > The goal is to broaden the way policy makers approach business concentration in the U.S., going beyond conventional antitrust enforcement focused on blocking big mergers. For example, companies in industries controlled by a small number of big firms might face new rules for disclosing fees to consumers or for their relationships with suppliers, the people familiar with the effort said.\n> \n> Seems right! Or not, I mean; I guess it depends on how you feel about big business generally. But if you feel bad about some big business specifically, addressing that in a specific way — rather than assuming that big business is exclusively an antitrust problem — seems like the way to go."},{"fields":{"slug":"/multidimensional-mental-representations-of-natural-objects/","title":"Multidimensional Mental Representations of Natural Objects"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_paper\n - psychology\n---\n\n# Multidimensional Mental Representations of Natural Objects\n\n - src: [@Hebart:2020uz]\n - [arXiv](https://psyarxiv.com/7wrgh/)\n - [ICLR](https://arxiv.org/abs/1901.02915) (earlier version)\n\n## Contemplations\n\n### Distributed Representation\n\n![General Workflow](img/41562_2020_951_Fig1_HTML.png)\n\nThe idea of \"distributed representation\" (`word2vec`) is that a) words are represented as high-dimensional vectors, and b) words that cooccur should be closer together. What this means is that you can simply ingest a large corpus of unstructured text, collecting co-occurrence data, and learning a distributed representation along the way. The beautiful thing about this approach is that the only thing it relies on is some notion of co-occurrence; in the case of language, this is simply a window function.\n\nWhat happens when you don't have such information? Well then, you have to get a little creative. For those familiar with `word2vec`, you'll know that a lesser-known but powerful trick they proposed is this notion of *negative sampling*: they found that it wasn't enough to, during the traversal of your corpus and training your model, to push co-occurring words together, but it was also important to push words apart (i.e. randomly pick a word, and then do the opposite maneuver). Note the similarity to [[triplet-loss]]. In essence, we can move from pairs to triplets. But this seems like we've made the problem even more difficult.\n\nAs it turns out, the triplet structure allows us do something interesting, which is to turn the problem into an \"odd-one-out\" task. Cute!^[Dwelling on this a little longer, the paired version would be more like, show pairs to people and ask if they should be paired, which feels like an arbitrary task. The alternative, which is to give a rating of the similarity, naturally inherits all the ailments that come with analyzing ratings.] In other words, given a corpus of images of objects, we can show a random triplet of them to humans, and ask which one is the odd one out. All that remains is to train our models with this triplet data. Here, they adapt `softmax` to this problem, applying it only to the chosen pair:\n\n$$\n\\sum_{(i,j,k)} \\log \\frac{ \\exp\\left\\{ x_i^T x_j \\right\\} }{ \\exp\\left\\{ x_i^T x_j \\right\\} + \\exp\\left\\{ x_i^T x_k \\right\\} + \\exp\\left\\{ x_j^T x_k \\right\\}} + \\lambda \\sum_{i = 1}^{m} \\|x_{i}\\|_{1},\n$$\n\nwhere the first summation is over all triplets, but, crucially, we designate $(i,j)$ to be the chosen pair (i.e. $k$ is the odd one out). The $l_1$ penalty induces sparsity on the vectors, which improves interpretability. Finally, and I'm not sure exactly how they implement this, but they also enforce the weights of the vector to be positive, to provide an additional modicum of interpretability.^[I feel like they overemphasize these two points, sparsity and positivity, as a way to differentiate themselves from other methods, when it's really quite the trivial change from a technical perspective.]\n\n![Interpretable dimensions](img/41562_2020_951_Fig3_HTML.png)\n\nWhat is the output of this model? Ostensibly, it is a \"distributed representation\" of how we as humans organize our understanding of objects, albeit also conflating the particulars of the image used to represent this object.^[Does the prompt ask the subject to consider the image as an archetype, or simply ask to compare the three images on face value, whatever they see fit? My guess is probably the latter.] Note that even though we're dealing with the visual domain, the open-endedness of the task means that humans aren't necessarily just using visual cues for comparison, but might also be utilizing higher order notions.\n\nA crude approximation is that this model outputs two types of *features*: visual properties, like size, texture, etc.; and then the more holistic, higher-order properties, such as usage, value/price, etc. The key is that the latter property should not be able to be inferred from visuals alone.^[So really what I'm saying is that there are two groups, $V$ and $V^C$. Not groundbreaking.] Thus, this model is picking something intangible that you cannot possibly learn from simply analyzing image classification models. However, this then begs the question: is it possible to learn a similar model by considering a combination of images (visual features) as well as text (holistic features)? What would be the nature of such a multi-modal learning model?\n\nI see a few immediate challenges. The first is that, while one could take the particular image chosen in this experiment to represent an object, it would be too specific. What we would want is to be able to learn a single feature across all images of a single object. I feel like this should be something people have thought about, right? The second problem is, are word embedding of the objects sufficient to get the higher-order features? \nSupposing we solve all those problems, we're still left with the question of how to combine these two things meaningfully. It feels to me like we should be training these things in unison?\n\n### Typicality\n\nThe idea here is that the magnitude of a particular dimension should import some meaning, the most likely being the *typicality* of this object to this particular feature (e.g. food-related). What they do is pick 17 broad categories (the graphs below) that have a corresponding dimension in the vector representation, then for each image/object in this category (e.g. there are 19 images in the Drink category, corresponding to 19 points on the graph), they get humans to rate the typicality of that image for that category.\n\n![Typicality scores](img/41562_2020_951_Fig7_HTML.png)\n\nThis result is somewhat surprising to me. For one, if you give me a list of images for, say, the category of animals, I would have no idea how to rate them based on *typicality*. Like, I think it would involve something cultural, regarding perhaps the stereotypes of what an animal is. I can imagine there being some very crude gradation whereby there are the clear examples of atypical, and then clear examples of typical, and then the rest is just a jumble. It doesn't really appear that way from the data – I would have to look at these images to get a better sense.^[As an aside, I wish they also included the values of all the other images too, not just those in this category. Perhaps they are all at close to zero, which would be great, but they don't mention it, so I assume the worst.]\n\nAlso, how is it able to get at typicality through this model? I think what's illuminating to note is that, out of the 27 broad categories of images in this dataset, 17 can be found in the 49-dimensional vector representation. Here's what I think is probably happening (in particular for these 17 categories):\n\n - if all three images are from different categories, then probably we're learning something about one of those other dimensions (hopefully);^[Another thing is that this step helps to situate the categories amongst each other.]\n - if two images are from the same category, and the third is different, then the same pair is most often picked (helping to form this particular dimension);\n - if all three images are picked, then I guess the odd one out is the one that's the least *typical*.\n\nHaving laid it out like so, I'm starting to get a little skeptical about the results: it almost feels like this is a little too crude, and the data is not sufficiently expressive for a model to be able to learn something that's not just trivial. Put another way, this almost feels like a glorified way of learning the categories – though, there's nothing necessarily wrong with that, since the (high-level) categories are obviously important.\n\nPerhaps it helps to consider the following generative model for images: suppose each image was represented by a sparse vector of weights, with many of the coordinates corresponding to broad categories. Set it up in a such a way that if you're in a broad category, then that dominates all other considerations (so it follows the pattern above). Then, simply run this odd-one-out task directly on these vectors, and see if you're able to recover these vectors.^[It almost feels like a weird autoencoder architecture...]\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[triplet-loss]: triplet-loss.md \"Triplet Loss\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/neural-representations/","title":"Neural Representations"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - neural_networks\n - from_paper\n---\n\n# Neural Representations\n\nsrc: [@sitzmann2020implicit]\n\nConsider the following (seemingly contrived) problem: learn the function $f:\\mathbb{R}^2\\mapsto \\mathbb{R}^3$ that takes $(x,y)$ coordinates and outputs the $(r,g,b)$ values of a (fixed) image. In other words, the idea here is to learn a *neural representation* of an image.^[It almost feels like we're talking about compression, but also not quite?]\n\nTo begin with, let's use an MLP to learn the function. All we're doing is learning a function from $[0,1]^2$ (first squeeze the image into the unit square) to $[0,1]^3$. Of course, the problem here is that this function is very much not smooth. However, it's also not the extreme case of learning an image of random noise, as there are patterns in the data (and possibly stretches of similar color in the background, for instance). Unsurprisingly, it is difficult for the MLP to learn this \"function\" – it simply learns a smooth function, which is basically a blurry version of the image.^[One might argue that MLPs are non-linear and so very much not smooth, in a proper functional analysis sense, and they would be right. Here I am using smoothness loosely to mean how bendy the function is (how able it is to interpolate random noise, say).]\n\nHere is where thinking in terms of compression might be helpful. The fact that we can compress most natural images (via representations in different bases) suggests that there should be a way to functionally represent an image in an *efficient* manner. The question is, how can we transform the space so that the function $f$ we're learning is **smooth(er)**? Perhaps unsurprisingly, it turns out that thinking in terms of fourier bases gets you pretty far.\n\nRecall that the idea behind image compression with FFT (great youtube video [here](https://www.youtube.com/watch?v=gGEBUdM0PVc)) is that you can represent an image as a sum of a sparse set of 2D-fourier bases (think 2D periodic waves). That is, most coefficients are negligible, and so you only need a sparse set of coefficients to reproduce the image. One way you can think about this is that you can start with the (deterministic) function $g: \\mathbb{R}^2 \\mapsto \\mathbb{R}^{2k}$ that essentially stacks $k$ 2D-fourier bases, followed by a function $h: \\mathbb{R}^{2k} \\mapsto \\mathbb{R}^3$. that takes a weighted sum of these bases. Putting this together, we have $f = g \\circ h$, which is a way of expressing an image through its fourier representation.\n\nNow, suppose we wanted to learn $f$ in a data-driven manner using a MLP. It's somewhat unsurprising from the above analysis that we might want to take $g$ as given, and just learn $h$. "},{"fields":{"slug":"/next-steps-for-deep-learning/","title":"Next Steps for Deep Learning"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_article\n---\n\n#  Next Steps for Deep Learning\n\nsrc: [Quora](https://www.quora.com/q/handsonnlpmodelreview/Deep-Learning-beyond-2019)\n\n## Deep Learning 1.0\n\nLimitations:\n\n - training time/labeled data: this is a common refrain about DL/RL, in that it takes an insurmountable amount of data. Contrast this with the [[bitter-lesson]], which claims that we *should* be utilising computational power.\n\n![](https://qph.fs.quoracdn.net/main-qimg-5ff7808245cd0e86c45037f47f6244d2)\n\n - adversarial attacks/fragility/robustness: due to the nature of DL (functional estimation), there's always going to be fault-lines.\n   + selection bias in training data\n   + rare events/models are not able to *generalise*, out of distribution\n - i.i.d assumption on DGP\n   + temporal changes/feedback mechanisms/causal structure?\n\n## Deep Learning 2.0\n\n### Self-supervised learning---learning by predicting input\n\n - Supervised learning : this is like when a kid points to a tree and says \"dog!\", and you're like, \"no\".\n - Unsupervised learning : when there isn't an answer, and you're just trying to understand the data better.\n   + classically: clustering/pattern recognition.\n   + auto-encoders: pass through a bottleneck to reconstruct the input (the main question is how to develop the architecture)\n     * i.e. learning efficient *representations*\n - Semi-supervised learning : no longer just about reconstructing the input, but self-learning\n   + e.g. shuffle-and-learn: shuffle video frames to figure out if it's in the correct temporal order\n   + more about choosing clever objectives to optimise (to *learn* something intrinsic about the data)\n\n> The brain has about 10^4 synapses and we only live for about 10^9 seconds. So we have a lot more parameters than data. This motivates the idea that we must do a lot of unsupervised learning since the perceptual input (including proprioception) is the only place we can get 10^5 dimensions of constraint per second.\n\nHinton via [/r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/2lmo0l/ama_geoffrey_hinton/clyjogf/). ^[I'm not sure I follow the logic here. My guess is that you have a dichotomy between straight up perceptual input and actually learning by interacting with the world. But that latter feedback mechanism also contains the perceptual input, so it should be at the right scale.]\n\n - much more data than *supervised* or #reinforcement_learning\n - if coded properly (like the shuffle-and-learn paradigm), then it can learn temporal/causal notions\n - utilisation of the agent/learner to direct things, via #attention, for instance.\n\n![Newborns learn about gravity at 9m. Pre: experiment push car off and keep it suspended in air (with string) won't surprise. Post: surprise.](https://qph.fs.quoracdn.net/main-qimg-e8047005bf69845ff7b2285fcd71336c)\n\n - case study: BERT. self-supervised learning, predicting the next word, or missing word in sentence (reconstruction).\n   + not multi-sensory/no explicit agents. this prevents it from picking up physical notions.\n     * sure, we need multi-sensory for general-purpose intelligence, but it's still surprising how far you can go with just statistics\n - less successful in image/video (i.e. something special about words/language). ^[Could it be the fact that we deal with already highly *representative* objects that are already *classifications* of a sort, whereas everything else is at the bit/data level, and we know that's not really how we process such things. This suggests that we should find an equivalent type of *language*/*vocabulary* for images.]\n   + we operate at the *pixel*-level, which is suboptimal\n - prediction/learning in (raw) input space vs representation space\n   + high-level learning should beat out raw-inputs^[My feeling though is that you eventually have to go down to the raw-input space, because that is where the comparisons are made/the training is done. In other words, you can transform/encode everything into some better representation, but you'll need to decode it at some point.]\n\n```{remark}\nPattern recognition $\\iff$ self-supervised learning?\n```\n\n### Leverage power of compositionality in distributed representations\n\n - combinatorial explosion should be exploited\n - composition: basis for the intuition as to why *depth* in neural networks is better than *width*\n\n```{remark}\nDon't think this is a particularly interesting insight.\n```\n\n### Moving away from stationarity\n\n - stationarity: train/test distribution is the same distribution\n   + they talk about IID, which is not quite the same thing (IID is on the individual sample level---the moment you have correlations then you lose IID)\n   + not just following econometrics, in that the underlying distributions are time-varying though\n - feedback mechanisms (via agents/society) require dropping IID (and relate to [[fairness-project]])\n - example: classifying cows vs camels reduces to classifying desert vs grass (yellow vs green)\n   + not sure how this example relates, probably need to read [[invariant-risk-minimisation]]\n\n### Causality\n\n - causal distributed representations (causal graphs)\n   + allows for causal reasoning\n - it's a little like reasoning in the representation space?\n\n### Lifelong Learning\n\n - learning-to-learn\n - cumulative learning\n\n### Inspiration from Nature\n\nflies: [video](https://www.youtube.com/watch?v=_sOgIwyjrOA&feature=youtu.be); [paper](https://ccneuro.org/2019/proceedings/0000998.pdf)\n\nSee [[calculus-for-brain-computation]].\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[bitter-lesson]: bitter-lesson.md \"Bitter Lesson\"\n[invariant-risk-minimisation]: invariant-risk-minimisation.md \"Invariant Risk Minimisation\"\n[calculus-for-brain-computation]: calculus-for-brain-computation.md \"Calculus For Brain Computation\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/no-free-lunch/","title":"No Free Lunch"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - machine_learning\n---\n\n# No Free Lunch\n\nLet's start by thinking about *search* algorithms \n\nNotation:\n\n - $X$ is countable search space (e.g. set of parameters)\n - objective function $f: X \\mapsto Y$ that evaluates a particular choice\n - dataset $d^m = \\{ d_X^m, d_Y^m \\}$  is a set of $m$ pairs of $(x, f(x))$ (i.e. what you've tried so far)\n - search algorithm $\\mathcal{A}$ simply appends to this dataset (i.e. continues the search, ideally finding $x$'s that lead to smaller $f(x)$)\n - arbitrary performance measure $\\Phi: d_Y^m \\to \\mathbb{R}$ (i.e. evaluate how good a search algorithm is)\n\nSuppose we want to find the minimum of $f$. Then, the search algorithm is exactly gradient descent, as in a proposal for the next step in the search.\n\nKey Points:\n\n - "},{"fields":{"slug":"/non-deep-networks/","title":"Non-deep Networks"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - from_paper\n---\n\n# Non-deep Networks\n\n - [arXiv](http://arxiv.org/abs/2110.07641)\n - [OR](https://openreview.net/forum?id=Xg47v73CDaj)\n\nOne common wisdom in the era of deep learning is that \"depth trumps width\". After all, we are not in the era of wide learning. Deeper networks seem to allow for more *expressivity* given the compositional nature of layers, holding the number of parameters fixed. From a theoretical perspective, we know that depth-2 networks are universal approximators, but the size of the network is exponential in the input dimension. I'm pretty sure that there has been work showing that dependence for deeper networks is no longer exponential in $n$, but who knows under what stringent conditions does this hold.\n\nMore recently, we've entered the era of **deep** learning, whereby the number of layers are in the hundreds, if not thousands. This feels like an altogether different regime, which I would call really-deep learning. This also gets closer to the question of scaling.\n\nAlso, note that a lot of the theoretical work talks about ReLU networks that are just MLPs. This is so far removed from what the models are like now – existing architectures are much more like compositions of *blocks* of computation, with only a handful of block types. It's very much like a circuit, except instead of electricity you have a flow of data/information, and the components are these blocks.\n\nContinuing with the circuit analogy, you can differentiate between parallel and series. However, I think this is where the usefulness of the analogy ends. It's more helpful to think about computer programs, where we have sequential and parallel computation. It's clear that almost all programming is done sequentially, as that's how you get composition and the power of compounding. Parallelisation is something that can help speed things up, but for the most part it's something that you do to take advantage of distributed resources (once you hit the limitations of the single core).\n\nWhere does this paper fit in? It takes aim at this very-deep learning regime, and asks the question: is it really necessary to have hundreds of layers? Can we achieve the same sort of performance with only a handful of layers instead?\n\n![](https://i.imgur.com/1ybpnql.png)\n\nTo be honest, I'm not *that* familiar with the ImageNet benchmark. I think what they're trying to argue is that they're replacing depth with width. But to me, seeing the performance of the single-stream already shows that you can already do quite a lot with 12 layers.\n\nActually, as one reviewer notes:\n\n> My major concern is that the authors argues contribution of parallel subnetworks, while I think the traditional RepVGG-wise blocks adopted in ParNet contributes significantly more to prediction accuracy. Even if ParNet adopts 1 branch only, ParNet's accuracy can still achieve 75% more, as \"ParNet-M-OneStream\" shown in Table 8. Comparison of Table 7 and Table 10 shows that the RepVGG blocks is significantly more important than adopted number of branches on improving accuracy.\n\n\n## Paper Specifics\n\n - A key architectural choice is that the *parallel* streams use different resolutions (I guess this is also why you get diminishing returns with more streams).\n - They argue that moving calculations to width will take advantage of multiple cores. Unconvinced by this.\n\n### Parallel Performance\n\n![](https://i.imgur.com/yhfImSa.png)\n\nTable 10: comparing \\# of branches against performance. What they find is that for a fixed number of parameters, the optimal number of branches is 3. This shows that all this \"embarrassingly parallel\" talk is only *embarrassing*. For one thing, the performance improvements are, in my eyes, incredibly marginal. \n\nSomething that they keep doing is to fix the number of parameters and then vary across various configuration parameters (depth, width, channels, streams). This is clearly the simplest way to keep the *capacity* of the model constant, but in this era of complicated architectures, it's unclear how fair this metric really is.\n\n### Ensembles\n\n![](https://i.imgur.com/UpRHFxS.png)\n\nTable 9: comparing ensembles against the same number of streams. Their conclusion for this ablation study is that their *parallel* configuration outperforms a simple ensemble. From my perspective, this makes me think that ensembles, which are the actual \"embarrassingly parallel\" operation, aren't all that bad. Granted, it's not a particularly efficient usage of parameters (and my intuition says that part of why networks do so well is that in some sense they are performing some kind of bagging or boosting).\n\nIt's not clear to me how they train the ensemble (multiple networks). I assume it's not the classic statistical way, which is to do *bagging*. Perhaps there's a canonical choice of ensemble method, but it's really annoying that this isn't specified.\n\n## Final Thoughts / Open Questions\n\n - My feeling is that moving to width is the new *feature engineering*. Yes it's probably possible to *architect* a complex neural network with only a few layers. But that's not really that interesting, because you've essentially *feature engineered* that architecture for that particular problem.\n   - What would make a compelling case would be if the application of extending width was some very simple, extensible procedure, i.e. actually \"embarrassingly parallel\".\n - Anyone thought of adding closed loops to a network? In some sense, this explosion of layers (and scaling), the point of it is to be able to let the neural network go do its thing. I guess the problem is that backprop no longer works. This feels a little like [[can-you-learn-an-algorithm]], where the idea is to the give a RNN more *runway* to be able to do more processing.\n   - My feeling is that loops might be possible as some sort of extension. In other words, you have the normal training with a huge dump of data, which requires backprop, and there are no loops there. But then you move into this later stage where it's much more single-data-at-a-time, and here it's no longer about backprop (what is the replacement then?), and you can now add some loops. This is encroaching on Hofstadter's \"I am a Strange Loop\" territory.\n   - Maybe (reflecting how we learn), what you want is something like unsupervised learning in the beginning (huge unstructured data), which allows you to train these generic processing units, to capture very high level things. And then, at the same time, you have the more systemic learning engine that contains loops and whatnot, and these use these generic units, and that is \"trained\" in a much more deliberate manner, without backprop, to allow for things like loops to work.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[can-you-learn-an-algorithm]: can-you-learn-an-algorithm.md \"Can You Learn an Algorithm\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/objection-detection-for-guis/","title":"Object Detection for GUI"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - gui\n  - machine_learning\n---\n\n# Object Detection for GUI\n\n## Problems\n\nLarge in-class variance and similarity between classes: basically, a very difficult problem!\n\n - Large in-class variance:\n   - style is at the whim of the designer/application.\n   - in general (interestingly), each data point has the same style, but across samples you have nothing\n - High cross-class similarity:\n   - key here is that sometimes the difference between two GUIs is something very small. that being said, this is more about classifying different types of GUIs, which we don't really do right now.\n - Packed scenes:\n   - close quarters, dense – in stark contrast to natural object detection problems\n\nI think it's also important to highlight the difference between GUIs and natural images, because that's exactly where we can improve (or even cross-pollinate).\n\n1. sharp, pixel based (plus some shadows, but all computer generated)\n2. close quarter, dense\n3. no common style per class\n\n![examples](https://i.imgur.com/dPDbZRP.png)\n\n\n\n![table](https://i.imgur.com/N1pFCLT.png)\n\n"},{"fields":{"slug":"/on-optimisation-in-matrix-completion/","title":"On Optimisation in Matrix Completion"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - matrix_completion\n - optimisation\n---\n\n# On Optimisation in Matrix Completion\n\nStatisticians get their first taste of optimisation when they learn about penalised linear regression:\n$$\n\\min_{\\beta} \\|Y - X \\beta\\|_2^2 + \\lambda\\|\\beta\\|_p.\n$$\nIt turns out that there's an equivalent formulation (as a constrained minimisation) that provides a little more intuition:\n$$\n\\min \\|Y - X \\beta\\|_2^2 \\text{ s.t. } \\|\\beta\\|_p \\leq s,\n$$\n\n![The OG image.](img/lasso.png)\n\nwhere there is a one-to-one relation between $\\lambda$ and $s$. This different view provides a geometric interpretation: depending on the geometry induced by the $l_p$ ball that is the constraint set, you're going to get different kinds of solutions.\n\nNote that the above equivalence is exact only when the loss and the penalty are both convex. In the matrix completion setting ([[penalty-project]]), this is no longer the case.\n\nThe (classical) convex program [@Candes:2009kj] is given by:\n$$\n\\min \\|W\\|_\\star \\text{ s.t. } \\|\\mathcal{A}(W) - y \\|_2^2 \\leq \\epsilon,\n$$\nwhere setting $\\epsilon = 0$ reduces to the noise-less setting. The more recent development using a penalised linear model is given by:\n$$\n\\min_{W} \\| \\mathcal{A}(W) - y \\|_2^2 + \\lambda \\|W\\|_{\\star}\n$$\nSince they're both convex, you have equivalence just like for the Lasso.^[I'm pretty sure that people have profitably used this equivalence in this context.]\n\n## Non-Convex\n\nWhat happens when we move to non-convex penalties though? Then, I think what you have is a *duality gap* (I could be wrong). If we use our penalty $\\frac{\\|{W}_{\\star}}{\\|{W}_{F}}$, then these two *views* are no longer equivalent. And here I think actually going back to the penalised loss function gives better results. For one thing, it's no longer a convex program, amenable to simple solvers."},{"fields":{"slug":"/one-stock-to-rule-them-all/","title":"One Stock to Rule Them All"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - finance\n - idea\n---\n\n# One Stock to Rule Them All\n\nA small, stupid, simple idea (that's been bouncing around my head): the price of a stock is a function of all the information about that particular stock (which should dictate its movement). If you believe the efficient market hypothesis (EMH) (which, granted, most people do not), then by extension this suggests that a stock's price must also *capture* all the information in the world. Of course, most other information takes up an infinitely small part of that stock's price (the less relevant, the smaller the part), but in some sense, the whole world is captured in every single stock.\n\nNow, even if you don't believe in the EMH, such a view might be instructive when considering the real world: essentially each stock is like a *view* of the infinite dimensional representation of the world. Under the EMH, a view (a projection) does not reduce the dimensions (i.e. no zero'd coordinates), whereas in real life, you're most likely getting a projection into a pretty compact finite space.\n\nWhat's the end result of all this? It sort of feels like the word embedding stuff, but not really.^[The key difference there is that you assume words/stocks have some latent representation in some shared space, whereas here we assume all stocks derive from the same latent representation, but simply take on different views.] The conjecture that comes out of this is that, essentially, there's a very high dimensional vector that corresponds to the world, and each stock is simply one view into that world. The goal is then to learn this vector, as well as the projections associated with each stock. The question is then: is this a solvable problem?\n\nMore formally, let $Z_t$ be this latent representation of the world (at time $t$). For simplicity, let's assume that $Z_t$ is finite-dimensional, $\\in \\mathbb{R}^{n}$. Then, each stock $X^{i}_{t}$ corresponds to a view of $Z_{t}$, namely $X^{i}_{t} = P^{i}(Z_{t})$, where $P^{i}$ is some projection matrix (possibly varies with $t$, but if it does, we probably need to assume that it is slowly-varying). We are given the $\\left\\{ X^{i}_{t} \\right\\}_{i, t}$, and our goal is to estimate $\\left\\{ Z_t \\right\\}_{t}$ and $\\left\\{ P^{i} \\right\\}_{i}$.\n\nThe above formulation still subscribes to the linear world, whereas nowadays we want to express things as non-linear functions, allowing us to take full advantage of deep learning. However, it's unclear to me whether or not it's even possible to frame this in a way that's conducive for deep learning. Of course, every time there's talk of a latent representation, it feels like you should be able to solve it with neural networks."},{"fields":{"slug":"/precision-recall/","title":"Precision/Recall"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - interview\n - computer_science\n---\n\n# Precision/Recall\n\nSomething that we statisticians don't really think about is the notion of precision/recall ([wiki](https://en.wikipedia.org/wiki/Precision_and_recall)) in classification. The definition actually comes from *information retrieval* (e.g. web searches). You have the *retrieved* documents (the pages returned by the search engine), as well as the *relevant* documents (those pages that are actually relevant to the query). Then *precision* is the fraction of those documents that were retrieved that are actually relevant. Similarly, *recall* is the fraction of relevant documents that were retrieved.\n\nTo translate this to something more relevant to classification, if you're trying to classify dogs, then precision is the fraction of those that you categorised as dogs (retrieved) that were actually dogs, while recall is the fraction of actual dogs (relevant) that were correctly categorised. So recall is more about coverage, whereas precision is more like accuracy, sort of.\n\nEven though it's an incredibly straightforward concept, it actually lends itself to non-trivial queries.\n\nA very similar in statistics is the notion of false/true positives/negatives. Though in that case what we're mostly interested in is hypothesis testing, though you can easily coerce it into the framework of classification."},{"fields":{"slug":"/overparameterized-regression/","title":"Overparameterized Regression"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_paper\n - overparameterization\n---\n\n# Overparameterized Regression\n\nsrc: [@Arora:2018vn], section 3 on $l_p$ regression.\n\n## Summary\n\nThey show that in the simple scalar linear regression problem, i.e. loss function\n$$\n  \\mathbb{E}[(x,y)\\sim S]{\\frac{1}{p}(y - \\mathbf{x}^\\top \\mathbf{w})^p}\n$$\nif you overparameterize ever so slightly to\n$$\n  \\mathbb{E}[(x,y)\\sim S]{\\frac{1}{p}(y - \\mathbf{x}^\\top \\mathbf{w}_1 w_2)^p},\n$$\nthen gradient descent turns into an accelerated version.\n\n## Thoughts\n\nI remember Arora presenting on this particular result a few years back (or something like this), and finding it very intriguing. I think this goes nicely with the theme of [[statistics-vs-ml]], though it's a slightly different angle. Essentially: we statisticians are **afraid** of overparameterization, because it removes specificity/leads to ambiguity. But actually, when it comes to these gradient methods, it actually helps to *overparameterize*!"},{"fields":{"slug":"/","title":"Artifactual Neural Network"},"frontmatter":{"draft":false},"rawBody":"# Artifactual Neural Network\n\nHere be gradients."},{"fields":{"slug":"/signed-word-embeddings/","title":"Signed Word Embeddings"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - idea\n - nlp\n---\n\n# Signed Word Embeddings\n\nThe basis for word embeddings lies in the *distributional hypothesis*, which states that \"a word is characterised by the company it keeps\".\n\n- I've thought about this before, but for some reason I can't quite find where I wrote about this. basically it follows from two pieces of intuition:\n  - that word embeddings cannot distinguish between synonyms and antonyms, because all they care about is co-occurrence\n    - as a result, oftentimes you'll see that antonyms will appear close to each other in the embedding space, which doesn't really make much sense if we're think of the embedding space as a proxy for \"meaning\"\n  - that I've worked on signed graphs, and so have some intuition about dealing with positive/negative \"ties\"\n- I think that an interesting place to start will be thinking in terms of the SVD decomposition of the PMI matrix (even though that's not exactly the same thing as SGNS/Word2Vec)\n- How does this relate to Signed Graphs?\n  - The one time I was thinking of embedding positive and negative edges, it was when considering the spectral decomposition (?) of the graph\n  - I vaguely remember a paper that tried to spectral methods to do some sort of visualisation (though it was very unwieldy)\n- the problem is that when you do things geometrically, then you're basically going to be conforming to transitivity indirectly. just like when you're trying to model signed graphs. thus, I think that even though it seems innocuous to have some sort of *geometric* interpretation, it has this unconscious bias."},{"fields":{"slug":"/stand-alone-self-attention-in-vision-models/","title":"Stand-Alone Self-Attention in Vision Models"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - from_paper\n---\n\n# Stand-Alone Self-Attention in Vision Models\n\n - [arXiv](https://arxiv.org/abs/1906.05909)\n\n"},{"fields":{"slug":"/slack-bot/","title":"Slack Bot"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - gui\n---\n\n# Slack Bot\n\nAs a fun little thought experiment, say I want to track the status of all my coworkers on Slack, because I'm curious about people's work habits. This is probably possible via APIs if you were the employer (or some admin), but not for a lowly employee. However, all this information is displayed on the Slack (Electron?) app, ready to be scraped. Doing scraping on the web is much easier, as you have access to all the html (though I wonder if the recent introduction of client-side javascript rendering makes scraping much more difficult). It would be cool to be able to do this all through the actual GUI itself. \n\nLet's think about a hypothetical GUI-as-a-service."},{"fields":{"slug":"/pseudo-inverses-and-sgd/","title":"Pseudo-inverses and SGD"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - mathematics\n - gradient_descent\n---\n\n# Pseudo-inverses and SGD\n\n- The Moore-Penrose pseudo inverse (which is often just called the pseudoinverse) is just a choice of multiple solutions that leads to a minimum norm solution\n  + $\\hat{\\beta} = X^+ Y$\n  + An obvious consequence, is that ridge regression is not the same as the pseudoinverse solution, even though they're obviously very similar\n  + my feeling is that, for a particular choice of penalty, the solution should be same (since you should be able to reframe the optimization into\n    *  $\\min ||\\beta||_2 \\text{ s.t. } ||Y - X \\beta ||_2 \\leq t$\n  + part of me is a little confused: if you can just define this pseudoinverse, then how come we don't use this estimator more often? clearly it must not have good properties?\n\n- thanks to this [tweet](https://twitter.com/beenwrekt/status/1041724607911227392), realize there's a section in [@Zhang:2016ve] that relates to [@Bartlett:2020ck] [[benign-overfitting-in-linear-regression]], in that they show that the SGD solution is the same as the minimum-norm (pseudo-inverse)\n  + the curvature of the minimum (LS) solutions don't actually tell you anything (they're the same)\n  + \"Unfortunately, this notion of minimum norm is not predictive of generalization performance. For example, returning to the MNIST example, the $l_2$-norm of the minimum norm solution with no preprocessing is approximately 220. With wavelet preprocessing, the norm jumps to 390. Yet the test error drops by a factor of 2. So while this minimum-norm intuition may provide some guidance to new algorithm design, it is only a very small piece of the generalization story.\"\n    * but this is changing the data, so I don't think this comparison really matters – it's not saying across all kinds of models, we should be minimizing the norm. it's just saying that we prefer models with minimum norm\n    * interesting that this works well for MNIST/CIFAR10\n  + there must be something in all of this: on page 29 of [slides](https://www.stat.berkeley.edu/~bartlett/talks/201908Microsoft.pdf), they show that SGD converges to min-norm interpolating solution with respect to a certain kernel (so the norm is on the coefficients for each kernel)\n    * as pointed out, this is very different to the benign paper, as this result is data-independent (it's just a feature of SGD)"},{"fields":{"slug":"/stewardship-of-global-collective-beahvior/","title":"Stewardship of Global Collective Behaviour"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_paper\n - society\n - research\n---\n\n# Stewardship of Global Collective Behaviour\n\nsrc: [@BakColeman:2021wt]\n\nA well written call-to-arms for some sort of interdisciplinary effort to understand the possible pernicious effects of social media and more broadly the rapid pace of technological change affecting the way we communicate, form groups, digest information, and hopefully provide guidance on how to solve these problems (e.g. writing pieces specifically for regulators).\n\nThey use a term called *crisis discipline* which I like, the canonical example being climate (change) science: you have this incredibly complicated system that needs urgent research and attention (for catastrophic reasons), but you don't necessarily have the time (or it's just not possible given the complexities of the system) to be entirely systematic and sure about the conclusions. In other words, these kinds of disciplines call for a much more *agile* form of research.\n\nWhat's interesting to me is that they couch all this talk through the lens of [[complexity-theory]]. The idea is that, for instance, once you connect half the world's population together through the internet, or social media, you're going to get unaccounted-for emergent behaviour, very much like those studied in complexity science, except usually the subject is natural processes (like swarms of locusts or school of fish). The difference now is that we're dealing with humans, social interactions.\n\nA good example here is the flow of information. Usually when we think of information flow we think of communication networks, where we're sending bits of data around. However, the real information flow networks, and those that matter the most right now from a catastrophic perspective, are the information flows that we humans create when we read and share news over social media, thereby enabling the incredible propagation of fake news that we see permeate the world today. And this isn't just a simple process: once you incorporate humans (and human judgement) into this network, it becomes infinitely more complicated to model and predict.\n\nI definitely feel like this is something that I've been trying to articulate, and so I'm happy to see it laid out in this clear manner (unlike the way my brain organises its information, if it does that at all). It also has the same sort of flavor as my [[fairness-project]] work."},{"fields":{"slug":"/system-1-and-2/","title":"System 1 and 2"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - artificial_intelligence\n - machine_learning\n - psychology\n---\n\n# System 1 and 2\n\nUpdate: Kahneman actually talks about this in his NeurIPS 2021 invited talk: [[kahneman-neurips-perception]].\n\nI forget where this was mentioned (either in one of the AI podcast episodes, or this numenta [video](https://www.youtube.com/watch?v=0ZVOmBp29E0)), but basically we can think of GPT-3 as being the first almost perfect copy of system-1 human thinking, which is how Kahneman chose to dichotomise how our brains work---essentially, system-1 is the fast, intuitive thinking, while system-2 is the deliberate, rational, logical thinking.\n\nPattern recognition is basically system-1, and it's where all the problems of correlation ≠ causation occur, since it's just focused on predicting things by association. And that's basically what GPT-3 is capable of doing.\n\nThe question is then how do we get to system-2 thinking, which is pretty much our competitive edge---deliberate thought. Here's a random #idea that I had on my run, and I suspect someone has already thought about: what if system-2 = system-1 + simulation? It seems to me that the crucial piece of the puzzle is basically being able to *simulate* the world, or at least some very crude model of it. Once you have the capacity to simulate the world, then you can run your system-1 inferences, and see how things compare to the truths of your simulation, while also making sure to update your model of the world against reality.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[kahneman-neurips-perception]: kahneman-neurips-perception.md \"Perception\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/theoretical-statistics-beauty-or-banality/","title":"Theoretical Statistics: Beauty or Banality"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - statistics\n  - theoretical_statistics\n  - commentary\n---\n\n# Theoretical Statistics: Beauty or Banality\n\nWhile listening to this fairly interesting invited talked by Lugosi, I can't help reflect on the banality that is most of theoretical statistics, and why I have moved swiftly on from it.\n\nWhen I was deeply embedded in that *sub-culture*"},{"fields":{"slug":"/the-unreasonable-effectiveness-of-adam/","title":"The Unreasonable Effectiveness of Adam"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - optimisation\n - gradient_descent\n---\n\n# The Unreasonable Effectiveness of Adam\n\n## Intuition about Adam / History\n\nWe talk about gradient descent (GD), which is a first-order approximation (via Taylor expansion) of minimising some loss, and Newton's method (NM) is the second-order version. The key difference is in the *step-size*, which, in the second-order case, is actually the inverse Hessian (think curvature).\n\nNesterov comes along, wonders if the GD is *optimal*. Turns out that if you make use of past gradients via momentum, then you can get better convergence (for convex problems). This is basically like *memory*. See also [[discretization-of-gradient-flow]].\n\nWhat remains is still the *step-size*, which needs to pre-determined. Wouldn't it be nice to have an adaptive *step-size*? That's where AdaGrad enters the picture, and basically uses the inverse of the mean of the square of the past gradients as a proxy for step-size. The problem is that it treats the first gradient equally to the most recent one, which seems unfair. RMSProp makes the adjustment by having it be exponentially weighted, so that the more recent gradients are preferentially weighted.\n\nAdam [@Kingma:2015us] is basically a combination of these two ideas: *momentum* and *adaptive step-size*, plus a bias correction term.^[I didn't think much of the bias correction, but supposedly it's a pretty big deal in practice.] One of its key properties is that it is scale-invariant (see [[effectiveness-of-normalised-quantities]]), meaning that if you multiply all the gradients by some constant, it won't change the gradient/movement. ^[Note here, in the context of [[penalty-project]] that the invariance of Adam is *over time*, while our penalty's invariance is over the parameters themselves.]\n\nOne interpretation proffered in the paper is that it's like a **signal-to-noise** ratio: you have the first moment against the raw (uncentered) second moment. Essentially, the direction of movement is a normalised average of past gradients, in such a way that the more variability in your gradient estimates, the more uncertain the *algorithm* is, and so the smaller the step size.\n\nIn any case, Adam works incredibly well in practice. The thing is that we just don't really understand too much about why it does so well. You have a convergence result in the original paper, but I'm no longer that interested in convergence results.\n\n## Adam + Ratio\n\nLet's consider a simple test-bed to hopefully elicit some insight. In the context of the [[penalty-project]]: we want to understand why Adam + our penalty is able to recover the matrix. In our paper, we analyse the gradient of the ratio penalty, and give some heuristics for why it might be better than just the nuclear norm. However, all this analysis breaks down when you move to Adam, because the squared gradient term is applied element-wise.\n\nThis is why [@Arora:2019ug] work with gradient descent: things behave nicely and you are able to deal with the singular values directly, and everything becomes a matrix operation. It is only natural to try to extend this to Adam, except my feeling is that you can't really do that. Since we're now perturbing things element-wise, this basically breaks all the nice linear structure. That doesn't mean everything breaks down, but simply we can't resort to a concatenation of simple linear operations.^[Though, as we'll see below, breaking the linear structure might be why it's good.]\n\nIt's almost like there are two invariances going on: one keeps the observed entries invariant, while the other keeps the singular vectors invariant.\n\nOne **conjecture** as to why Adam might be better is: due to the adaptive learning rate, what it's actually doing is also relaxing the column space invariance. But this is really just to explain why GD and even momentum are unable to succeed.\n\nA more concrete **conjecture**: what we're mimicking is some form of alternating minimisation procedure. It would be great if we could show that we're basically moving around, reducing the rank of the matrix slowly, while staying in the vicinity of the constraint set.\n\n## Intuition\n\nLet's start with some intuition before diving into some theoretical justifications. If we take the extreme case of the solution path having to be exactly in our constraint set, then we'd be doomed. But at the same time, there's really not much point in your venturing too far away from this set. So perhaps what's going on is that you've relaxed your set (a little like the noisy case), and you can now travel within this expanded set of matrices. Or, it's more like you're travelling in and out of this set. I think either way is a close enough approximation of what's going on, and so it really depends on which provides a good theoretical testbed.\n\nNow, in both our penalty and the standard nuclear norm penalty, the gradients lie in the span of $W$, which highly restricts the movement direction. One might be able to show that if one were to be constrained as above, and only be able to move within the span, then this does not give enough flexibility. Part of the point is that span of the initial $W$ with the zero entries is clearly far from the ground truth, so you really want to get away from that span.\n\n## Generalisability\n\nOne of the problems here is that matrix completion is a simple but also fairly artificial setting. One might ask how generalisable the things we learn in this context are to the wider setting. For one thing, it's very unusual that you can essentially initialise by essentially fitting the training data exactly, though it turns out that this is okay here. This probably breaks down once you move to *noisy* matrix completion, but it's unclear if there's just a simple fix for that.\n\nSecondly, matrix completion is a *linear* problem, and a lot of the reasons why the standard things might be failing is because they don't break the linearity. But once you move to a *non-linear* setting, then we might be equal footing to everything else. For instance, even when we start overparameterising (DLNN) the linearity also breaks down, freeing gradient descent from the span of $W$.\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[discretization-of-gradient-flow]: discretization-of-gradient-flow.md \"Discretization of Gradient Flow\"\n[effectiveness-of-normalised-quantities]: effectiveness-of-normalised-quantities.md \"Effectiveness of Normalised Quantities\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/transformers/","title":"Transformers"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - machine_learning\n - neural_networks\n---\n\n# Transformers\n\n- references:\n  - [lil-log](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)\n  - [distill](https://distill.pub/2016/augmented-rnns/)\n  - [ebook-chapter](http://d2l.ai/chapter_attention-mechanisms/transformer.html) on transformers (actually this ebook isn't great, as it ends up being more about the implementation)\n- it turns out that #attention is an earlier concept, which itself is motivated by the encoder-decoder sequence-to-sequence architecture\n  - ![](https://karpathy.github.io/assets/rnn/diags.jpeg)\n  - (oops) I didn't really understand this diagram:\n    - sequence to sequence models are essentially the third diagram, where basically the input sequence and output sequence are asynchronous\n    - versus the normal rnn, which takes the output as the input\n    - I really like the example of 1-to-many, image-to-caption, so your input is not a sequence, but your output is\n  - seq2seq example is the translation program, where importantly the input and output sequence don't have to be the same length (due to the way languages differ in their realisations of the same meaning)\n  - the encoder is the rnn on the input sequence, and this culminates into the last hidden layer\n    - this is essentially the *context* vector: the idea here is that this (fixed-length) vector captures all the information about the sentence\n      - key: this acts like a sort of **informational bottleneck**, and actually is the impetus for the attention mechanic\n  - key point (via this [tutorial](https://arxiv.org/pdf/1703.01619.pdf)):\n    - instead of having everything represented as the last hidden layer (fixed-length), why not just look at all the hidden layers (vectors representing each word)\n    - but, that would be *variable* length, so instead just look at a linear combination of those hidden layers. this linear combination is learned, and is basically *attention*\n- Transformers #todo\n  - \"prior art\"\n    - CNN: easy to parallelise, but aren't recurrent (can't capture sequential dependency)\n    - RNN: reverse\n  - goal of transformers/attention is achieve **parallelization** and **recurrence**\n    - by appealing to \"attention\" to get the recurrence (?)\n\n## Transformers\n\n - key is *multi-head self-attention*\n   + encoded representation of input: key-value pairs ($K,V \\in \\mathbb{R}^{n}$)\n     * corresponding to hidden states\n   + previous output is compressed into query $Q \\in \\mathbb{R}^{m}$.\n   + output of the transformer is a weighted sum of the values ($V$).\n\n## Todo\n\n - Visualising and Measuring the Geometry of BERT [arXiv](https://arxiv.org/pdf/1906.02715.pdf)\n - [random blog](http://www.peterbloem.nl/blog/transformers)\n - pretty intuitive description of transformers on [tumblr](https://nostalgebraist.tumblr.com/post/185326092369/the-transformer-explained), via the LessWrong community"},{"fields":{"slug":"/triplet-loss/","title":"Triplet Loss"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n---\n\n# Triplet Loss\n\nsrc: [Git](https://omoindrot.github.io/triplet-loss)"},{"fields":{"slug":"/two-cultures/","title":"Two Cultures"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_paper\n - statistics\n - research\n - classic_papers\n---\n\n# Two Cultures\n\nsrc: [@Breiman:2001wl]\n\nWritten in 2001, I would say that many of the issues raised in this seminal paper have not changed much in the intervening years.\n\n## Chapter by Chapter\n\n### Introduction\n\nWe start with data $(x,y)$, and assume there is a black box that relates inputs to outputs, $y = f(x)$. He then differentiates between two cultures: data modelling, and algorithmic modelling.\n\nData modelling defines a strict stochastic model form of $f$, equipped with noise, parameters (i.e. classical statistics). We learn parameters from the data, and do robustness checks.\n\nAlgorithmic modelling, on the other hand, makes no assumptions on the form of $f$ -- it simply learns some $\\hat{f}$ that is close in prediction space to $f$. There are parameters, and structure, but they are on $\\hat{f}$. Thus, the black box of nature is left intact and untouched.\n\n### Use of Data Models\n\n> This enterprise has at its heart the belief that a statistician, by imagination and by looking at the data, can invent a reasonably good parametric class of models for a complex mechanism devised by nature\n\nHe worries that there is an over-reliance on data models, and that people forget that \"if the model is a poor emulation of nature, the conclusions may be wrong\".^[In my years of consulting experience, I can definitely attest to the over-reliance on data models, but I think this specific point is more a pedagogical issue than a research-focus one.]\n\nWe begin with linear models, the bread-and-butter of a statistician's arsenal. We can probably all recite the model assumptions of this model, which with probability 1 do not hold in practice. That never stopped anyone from claiming significance and writing a paper. More sophisticated practitioners would look at *goodness-of-fit* tests, or even *robustness* checks.\n\n> In a discussion after a presentation of residual analysis in a seminar at Berkeley in 1993, William Cleveland, one of the fathers of residual analysis, admitted that it could not uncover lack of fit in more than four to five dimensions.\n\nWhat you essentially have is the *curse of dimensionality*, which makes it difficult (unless you have exponential sample size) to be able to test for anything in high dimensions.^[Intuitively, you don't get linear increase with dimension, but exponential, since it's more about linear combinations of variables.] Thus, all the classical things we teach in robust statistics just really aren't that applicable once you deal with non-trivial data sizes.\n\nThen there is the separate issue of guided regression (or, the garden of forking paths, as poetically put by Gelman), which, in the past decade, has come under scrutiny as a result of the *replication crisis*. He attributes the laissez-faire attitude that statisticians had to this, back in the day, to the preoccupation with the data model.^[I think this is a little bit of a stretch. I think the takeaway here is that statisticians need to be very careful about the models they create, since goodness-of-fit tests (and the like) make it very easy to hoodwink oneself into thinking that a procedure is valid, when in fact subtle issues of selection bias may be creeping in. On the other hand, the test data never lies (well, rarely).]\n\n## Two Decades On\n\n"},{"fields":{"slug":"/vision-transformers/","title":"Vision Transformers"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n - from_paper\n - machine_learning\n - computer_vision\n - attention\n---\n\n# Vision Transformers\n\nThanks to their computational efficiency (and hence scalability), self-attention based ([[transformers]]) architectures have become the model of choice in NLP. Their success has led people to wonder if such architectures are universal^[For instance, it's been speculated that the self-attention mechanism is comparable to the way attention manifests in our brains – such a thesis would imply the universality of this module.], or, at the very least, if they can be applied to computer vision tasks.\n\nSelf-attention gives a good tradeoff between local context awareness and computational efficiency: rnn/lstm's allow for long-range dependencies, but the sequential nature of training really hampers model size.\n\nCompare this convolution layers (which are highly specialized for [[computer-vision-tasks]])\n\n[[stand-alone-self-attention-in-vision-models]]\n\n\n## An Image Is Worth 16x16 Words\n\nsrc: [@dosovitskiy2021image]\n\n### Summary\n\n\n\n[//begin]: # \"Autogenerated link references for markdown compatibility\"\n[transformers]: transformers.md \"Transformers\"\n[computer-vision-tasks]: computer-vision-tasks.md \"Computer Vision Tasks\"\n[stand-alone-self-attention-in-vision-models]: stand-alone-self-attention-in-vision-models.md \"Stand-Alone Self-Attention in Vision Models\"\n[//end]: # \"Autogenerated link references\""},{"fields":{"slug":"/journal/2021-12-24/","title":"2021-12-24"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - journal\n---\n"}]}}}